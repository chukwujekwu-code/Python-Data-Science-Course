{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 17: Unsupervised Learning - Clustering and Dimensionality Reduction\n",
    "\n",
    "## Topics Covered\n",
    "1. Introduction to Unsupervised Learning\n",
    "2. K-Means Clustering\n",
    "3. Hierarchical Clustering\n",
    "4. Clustering Evaluation (Elbow Method, Silhouette Score)\n",
    "5. Introduction to Dimensionality Reduction\n",
    "6. Principal Component Analysis (PCA)\n",
    "7. Practical Applications\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will be able to:\n",
    "- Understand the difference between supervised and unsupervised learning\n",
    "- Apply K-Means clustering to segment data into groups\n",
    "- Use hierarchical clustering and interpret dendrograms\n",
    "- Evaluate clustering quality using Elbow Method and Silhouette Score\n",
    "- Understand why dimensionality reduction is important\n",
    "- Apply PCA to reduce dimensions while preserving information\n",
    "- Visualize high-dimensional data in 2D or 3D\n",
    "- Apply these techniques to real-world problems like customer segmentation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Clustering algorithms\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "\n",
    "# Dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Datasets\n",
    "from sklearn.datasets import make_blobs, load_iris\n",
    "\n",
    "# Visualization\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Settings\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 1: Introduction to Unsupervised Learning\n",
    "---\n",
    "\n",
    "## What is Unsupervised Learning?\n",
    "\n",
    "Unlike supervised learning where we have labeled data (input-output pairs), unsupervised learning works with **unlabeled data**. The algorithm finds patterns, structures, or relationships in the data without being told what to look for.\n",
    "\n",
    "### Supervised vs Unsupervised Learning\n",
    "\n",
    "| Aspect | Supervised Learning | Unsupervised Learning |\n",
    "|--------|--------------------|-----------------------|\n",
    "| Data | Labeled (X, y) | Unlabeled (X only) |\n",
    "| Goal | Predict y from X | Find patterns in X |\n",
    "| Examples | Classification, Regression | Clustering, Dimensionality Reduction |\n",
    "| Use Cases | Spam detection, Price prediction | Customer segmentation, Data compression |\n",
    "\n",
    "## Two Main Types of Unsupervised Learning\n",
    "\n",
    "### 1. Clustering\n",
    "Group similar data points together\n",
    "- **Customer Segmentation**: Group customers by purchasing behavior\n",
    "- **Image Compression**: Group similar colors together\n",
    "- **Anomaly Detection**: Identify unusual patterns\n",
    "\n",
    "### 2. Dimensionality Reduction\n",
    "Reduce the number of features while preserving information\n",
    "- **Data Visualization**: Project high-dimensional data to 2D/3D\n",
    "- **Noise Reduction**: Remove redundant features\n",
    "- **Speed Up Training**: Fewer features = faster models\n",
    "\n",
    "### Why This Matters in Data Science\n",
    "\n",
    "Most real-world data is unlabeled! Labels are expensive and time-consuming to obtain. Unsupervised learning helps us:\n",
    "- Discover hidden patterns we didn't know existed\n",
    "- Understand data structure before building models\n",
    "- Reduce costs (no manual labeling needed)\n",
    "- Preprocess data for supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize supervised vs unsupervised learning\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Generate sample data\n",
    "X, y_true = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0)\n",
    "\n",
    "# Supervised Learning view (we have labels)\n",
    "axes[0].scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', s=50, alpha=0.6)\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "axes[0].set_title('Supervised Learning\\n(We have labels - colored by class)')\n",
    "\n",
    "# Unsupervised Learning view (no labels)\n",
    "axes[1].scatter(X[:, 0], X[:, 1], c='gray', s=50, alpha=0.6)\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "axes[1].set_title('Unsupervised Learning\\n(No labels - find patterns ourselves)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"In unsupervised learning, we need to discover the 3 groups on our own!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 2: K-Means Clustering\n",
    "---\n",
    "\n",
    "## What is K-Means Clustering?\n",
    "\n",
    "K-Means is the most popular clustering algorithm. It partitions data into **K clusters** by:\n",
    "1. Randomly initializing K cluster centers (centroids)\n",
    "2. Assigning each point to the nearest centroid\n",
    "3. Updating centroids as the mean of assigned points\n",
    "4. Repeating steps 2-3 until convergence\n",
    "\n",
    "### How It Works\n",
    "\n",
    "Imagine you want to group customers into 3 segments:\n",
    "1. Start with 3 random \"representative customers\" (centroids)\n",
    "2. Assign each customer to their most similar representative\n",
    "3. Update each representative to be the average of their group\n",
    "4. Repeat until groups stop changing\n",
    "\n",
    "### When to Use K-Means\n",
    "\n",
    "**Good for:**\n",
    "- Spherical, evenly-sized clusters\n",
    "- Large datasets (very fast)\n",
    "- When you know or can guess K\n",
    "\n",
    "**Not good for:**\n",
    "- Non-spherical shapes (elongated, curved clusters)\n",
    "- Clusters of very different sizes\n",
    "- Clusters of very different densities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntax\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create K-Means model\n",
    "kmeans = KMeans(\n",
    "    n_clusters=3,      # Number of clusters\n",
    "    random_state=42    # For reproducibility\n",
    ")\n",
    "\n",
    "# Fit to data and predict clusters\n",
    "cluster_labels = kmeans.fit_predict(X)\n",
    "\n",
    "# Access cluster centers\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Predict cluster for new data\n",
    "new_clusters = kmeans.predict(X_new)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Customer Segmentation with K-Means\n",
    "print(\"Customer Segmentation Example\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Generate synthetic customer data\n",
    "np.random.seed(42)\n",
    "n_customers = 300\n",
    "\n",
    "# Create 3 distinct customer segments\n",
    "# Segment 1: High income, high spending\n",
    "segment1 = np.random.randn(100, 2) * 10 + [80, 70]\n",
    "# Segment 2: Medium income, medium spending\n",
    "segment2 = np.random.randn(100, 2) * 8 + [50, 40]\n",
    "# Segment 3: Low income, low spending\n",
    "segment3 = np.random.randn(100, 2) * 6 + [25, 20]\n",
    "\n",
    "X_customers = np.vstack([segment1, segment2, segment3])\n",
    "\n",
    "customer_df = pd.DataFrame(X_customers, columns=['Annual_Income_k', 'Spending_Score'])\n",
    "print(f\"\\nDataset shape: {customer_df.shape}\")\n",
    "print(f\"\\nFirst few customers:\")\n",
    "print(customer_df.head())\n",
    "\n",
    "# Visualize unlabeled data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(customer_df['Annual_Income_k'], customer_df['Spending_Score'], \n",
    "           c='gray', alpha=0.6, s=50)\n",
    "plt.xlabel('Annual Income ($1000s)')\n",
    "plt.ylabel('Spending Score (1-100)')\n",
    "plt.title('Customer Data (Unlabeled)')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCan you see 3 groups? Let's use K-Means to find them!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means clustering\n",
    "print(\"Applying K-Means Clustering\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create and fit K-Means model\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(X_customers)\n",
    "\n",
    "# Add cluster labels to dataframe\n",
    "customer_df['Cluster'] = cluster_labels\n",
    "\n",
    "print(f\"\\nClusters found: {kmeans.n_clusters}\")\n",
    "print(f\"\\nCluster distribution:\")\n",
    "print(customer_df['Cluster'].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nCluster Centers (Centroids):\")\n",
    "centroids_df = pd.DataFrame(kmeans.cluster_centers_, \n",
    "                            columns=['Annual_Income_k', 'Spending_Score'],\n",
    "                            index=[f'Cluster {i}' for i in range(3)])\n",
    "print(centroids_df.round(2))\n",
    "\n",
    "# Visualize clusters\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot customers colored by cluster\n",
    "scatter = plt.scatter(customer_df['Annual_Income_k'], \n",
    "                     customer_df['Spending_Score'],\n",
    "                     c=customer_df['Cluster'], \n",
    "                     cmap='viridis', \n",
    "                     alpha=0.6, s=50)\n",
    "\n",
    "# Plot centroids\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], \n",
    "           kmeans.cluster_centers_[:, 1],\n",
    "           c='red', marker='X', s=300, \n",
    "           edgecolors='black', linewidths=2,\n",
    "           label='Centroids')\n",
    "\n",
    "plt.xlabel('Annual Income ($1000s)')\n",
    "plt.ylabel('Spending Score (1-100)')\n",
    "plt.title('Customer Segmentation with K-Means (K=3)')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  Cluster 0: High income, high spending\")\n",
    "print(\"  Cluster 1: Medium income, medium spending\")\n",
    "print(\"  Cluster 2: Low income, low spending\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercise 2.1\n",
    "\n",
    "**Task:** Apply K-Means clustering to the Iris dataset\n",
    "\n",
    "Steps:\n",
    "1. Load the Iris dataset using `load_iris()`\n",
    "2. Use only the first 2 features (sepal length and width) for visualization\n",
    "3. Apply K-Means with K=3\n",
    "4. Visualize the clusters and centroids\n",
    "5. Compare cluster labels with true species labels\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "3 clusters found\n",
    "Visualization showing clusters\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2.1\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X_iris = iris.data[:, :2]  # Use only first 2 features\n",
    "y_iris = iris.target\n",
    "\n",
    "print(\"Iris Dataset Clustering\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Dataset shape: {X_iris.shape}\")\n",
    "print(f\"True species: {iris.target_names}\")\n",
    "\n",
    "# Apply K-Means\n",
    "kmeans_iris = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "clusters_iris = kmeans_iris.fit_predict(X_iris)\n",
    "\n",
    "print(f\"\\nClusters found: {kmeans_iris.n_clusters}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# True labels\n",
    "axes[0].scatter(X_iris[:, 0], X_iris[:, 1], c=y_iris, cmap='viridis', alpha=0.6)\n",
    "axes[0].set_xlabel('Sepal Length')\n",
    "axes[0].set_ylabel('Sepal Width')\n",
    "axes[0].set_title('True Species Labels')\n",
    "\n",
    "# K-Means clusters\n",
    "axes[1].scatter(X_iris[:, 0], X_iris[:, 1], c=clusters_iris, cmap='viridis', alpha=0.6)\n",
    "axes[1].scatter(kmeans_iris.cluster_centers_[:, 0], \n",
    "               kmeans_iris.cluster_centers_[:, 1],\n",
    "               c='red', marker='X', s=300, edgecolors='black', linewidths=2)\n",
    "axes[1].set_xlabel('Sepal Length')\n",
    "axes[1].set_ylabel('Sepal Width')\n",
    "axes[1].set_title('K-Means Clusters')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nK-Means found patterns similar to the true species!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 3: Hierarchical Clustering\n",
    "---\n",
    "\n",
    "## What is Hierarchical Clustering?\n",
    "\n",
    "Hierarchical clustering builds a tree of clusters (dendrogram) by either:\n",
    "- **Agglomerative (bottom-up)**: Start with each point as its own cluster, merge closest pairs\n",
    "- **Divisive (top-down)**: Start with one cluster, split recursively\n",
    "\n",
    "We'll focus on **agglomerative clustering** as it's more common.\n",
    "\n",
    "### How It Works (Agglomerative)\n",
    "\n",
    "1. Start: Each point is its own cluster (N clusters)\n",
    "2. Find the two closest clusters\n",
    "3. Merge them into one cluster\n",
    "4. Repeat steps 2-3 until one cluster remains\n",
    "5. Cut the dendrogram at desired height to get K clusters\n",
    "\n",
    "### Advantages over K-Means\n",
    "\n",
    "- **No need to specify K upfront** (can decide after seeing dendrogram)\n",
    "- Works with **any distance metric**\n",
    "- Can find **non-spherical clusters**\n",
    "- Provides **hierarchy** of clusters\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "- **Slower** than K-Means (O(n\u00b2) vs O(n))\n",
    "- Not suitable for very large datasets\n",
    "- Can't undo merges (greedy algorithm)\n",
    "\n",
    "## Linkage Methods\n",
    "\n",
    "How do we measure distance between clusters?\n",
    "\n",
    "| Method | Description | Use Case |\n",
    "|--------|-------------|----------|\n",
    "| Single | Minimum distance between points | Long, stringy clusters |\n",
    "| Complete | Maximum distance between points | Compact clusters |\n",
    "| Average | Average distance between all points | Balanced approach |\n",
    "| Ward | Minimizes within-cluster variance | Most common, similar to K-Means |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntax\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Create hierarchical clustering model\n",
    "hc = AgglomerativeClustering(\n",
    "    n_clusters=3,       # Number of clusters\n",
    "    linkage='ward'      # Linkage method\n",
    ")\n",
    "\n",
    "# Fit and predict\n",
    "clusters = hc.fit_predict(X)\n",
    "\n",
    "# Create dendrogram (requires scipy)\n",
    "linkage_matrix = linkage(X, method='ward')\n",
    "dendrogram(linkage_matrix)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Hierarchical Clustering with Dendrogram\n",
    "print(\"Hierarchical Clustering Example\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Use smaller customer dataset for clearer dendrogram\n",
    "np.random.seed(42)\n",
    "X_small = np.vstack([\n",
    "    np.random.randn(10, 2) * 5 + [70, 60],\n",
    "    np.random.randn(10, 2) * 5 + [40, 35],\n",
    "    np.random.randn(10, 2) * 5 + [20, 15]\n",
    "])\n",
    "\n",
    "print(f\"Dataset size: {X_small.shape[0]} customers\")\n",
    "\n",
    "# Create dendrogram\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Calculate linkage matrix\n",
    "linkage_matrix = linkage(X_small, method='ward')\n",
    "\n",
    "# Plot dendrogram\n",
    "dendrogram(linkage_matrix)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Customer Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.axhline(y=50, color='r', linestyle='--', label='Cut at distance=50 (3 clusters)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nHow to read the dendrogram:\")\n",
    "print(\"  - Height shows distance between clusters\")\n",
    "print(\"  - Cutting horizontally gives K clusters\")\n",
    "print(\"  - Cut at distance=50 gives 3 clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply hierarchical clustering\n",
    "hc = AgglomerativeClustering(n_clusters=3, linkage='ward')\n",
    "hc_labels = hc.fit_predict(X_small)\n",
    "\n",
    "print(\"Hierarchical Clustering Results\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Clusters found: {len(np.unique(hc_labels))}\")\n",
    "print(f\"\\nCluster distribution:\")\n",
    "print(pd.Series(hc_labels).value_counts().sort_index())\n",
    "\n",
    "# Visualize clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_small[:, 0], X_small[:, 1], c=hc_labels, cmap='viridis', s=100, alpha=0.6)\n",
    "plt.xlabel('Annual Income ($1000s)')\n",
    "plt.ylabel('Spending Score')\n",
    "plt.title('Hierarchical Clustering Results (K=3)')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 4: Clustering Evaluation\n",
    "---\n",
    "\n",
    "## How Do We Know If Clustering Is Good?\n",
    "\n",
    "Unlike supervised learning, we don't have true labels to compare against. We need different evaluation methods.\n",
    "\n",
    "## 1. Elbow Method (Choosing K)\n",
    "\n",
    "The elbow method helps us choose the optimal number of clusters (K) by plotting:\n",
    "- **X-axis**: Number of clusters (K)\n",
    "- **Y-axis**: Within-cluster sum of squares (WCSS) or inertia\n",
    "\n",
    "**Inertia/WCSS**: Sum of squared distances from each point to its cluster center (lower is better)\n",
    "\n",
    "**How to use it:**\n",
    "1. Try K = 1, 2, 3, ..., 10\n",
    "2. Plot inertia for each K\n",
    "3. Look for the \"elbow\" where decrease slows down\n",
    "4. Choose K at the elbow\n",
    "\n",
    "## 2. Silhouette Score\n",
    "\n",
    "Measures how similar a point is to its own cluster compared to other clusters.\n",
    "\n",
    "**Range**: -1 to +1\n",
    "- **+1**: Point is far from neighboring clusters (excellent)\n",
    "- **0**: Point is on the boundary between clusters\n",
    "- **-1**: Point might be in the wrong cluster (poor)\n",
    "\n",
    "**Formula** (for one point):\n",
    "```\n",
    "s = (b - a) / max(a, b)\n",
    "```\n",
    "- a = average distance to points in same cluster\n",
    "- b = average distance to points in nearest different cluster\n",
    "\n",
    "**Average Silhouette Score**: Average across all points\n",
    "- **> 0.7**: Strong structure\n",
    "- **0.5 - 0.7**: Reasonable structure\n",
    "- **0.25 - 0.5**: Weak structure\n",
    "- **< 0.25**: No substantial structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Elbow Method\n",
    "print(\"Elbow Method for Choosing K\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Use customer data from Section 2\n",
    "inertias = []\n",
    "K_range = range(1, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_customers)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    print(f\"K={k}: Inertia = {kmeans.inertia_:.2f}\")\n",
    "\n",
    "# Plot elbow curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Inertia (Within-Cluster Sum of Squares)')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.xticks(K_range)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Mark the elbow at K=3\n",
    "plt.axvline(x=3, color='r', linestyle='--', linewidth=2, label='Elbow at K=3')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  - Inertia always decreases as K increases\")\n",
    "print(\"  - At K=3, we see an 'elbow' where the decrease slows\")\n",
    "print(\"  - K=3 is a good choice (matches our data generation!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Silhouette Score\n",
    "print(\"Silhouette Score Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate silhouette scores for different K\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)  # Silhouette needs at least 2 clusters\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_customers)\n",
    "    score = silhouette_score(X_customers, cluster_labels)\n",
    "    silhouette_scores.append(score)\n",
    "    print(f\"K={k}: Silhouette Score = {score:.3f}\")\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K_range, silhouette_scores, 'go-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score for Different K')\n",
    "plt.xticks(K_range)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.axhline(y=0.5, color='orange', linestyle='--', label='Reasonable threshold (0.5)')\n",
    "best_k = list(K_range)[np.argmax(silhouette_scores)]\n",
    "plt.axvline(x=best_k, color='r', linestyle='--', linewidth=2, label=f'Best K={best_k}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest K based on Silhouette Score: {best_k}\")\n",
    "print(f\"Score: {max(silhouette_scores):.3f} (Reasonable structure)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercise 4.1\n",
    "\n",
    "**Task:** Use Elbow Method and Silhouette Score to find optimal K\n",
    "\n",
    "Generate a dataset with 4 clusters using:\n",
    "```python\n",
    "X, y = make_blobs(n_samples=400, centers=4, random_state=42)\n",
    "```\n",
    "\n",
    "1. Apply Elbow Method for K=1 to 10\n",
    "2. Calculate Silhouette Scores for K=2 to 10\n",
    "3. Plot both metrics\n",
    "4. Determine optimal K\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "Optimal K: 4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 4.1\n",
    "\n",
    "# Generate dataset with 4 clusters\n",
    "X_4clusters, y_4clusters = make_blobs(n_samples=400, centers=4, random_state=42)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Elbow Method\n",
    "inertias = []\n",
    "for k in range(1, 11):\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    km.fit(X_4clusters)\n",
    "    inertias.append(km.inertia_)\n",
    "\n",
    "axes[0].plot(range(1, 11), inertias, 'bo-')\n",
    "axes[0].axvline(x=4, color='r', linestyle='--', label='Elbow at K=4')\n",
    "axes[0].set_xlabel('K')\n",
    "axes[0].set_ylabel('Inertia')\n",
    "axes[0].set_title('Elbow Method')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Silhouette Score\n",
    "sil_scores = []\n",
    "for k in range(2, 11):\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = km.fit_predict(X_4clusters)\n",
    "    score = silhouette_score(X_4clusters, labels)\n",
    "    sil_scores.append(score)\n",
    "\n",
    "axes[1].plot(range(2, 11), sil_scores, 'go-')\n",
    "best_k = range(2, 11)[np.argmax(sil_scores)]\n",
    "axes[1].axvline(x=best_k, color='r', linestyle='--', label=f'Best K={best_k}')\n",
    "axes[1].set_xlabel('K')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].set_title('Silhouette Score')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Optimal K: {best_k} (both methods agree!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 5: Dimensionality Reduction and PCA\n",
    "---\n",
    "\n",
    "## What is Dimensionality Reduction?\n",
    "\n",
    "Dimensionality reduction transforms high-dimensional data (many features) into lower dimensions while preserving important information.\n",
    "\n",
    "### The Curse of Dimensionality\n",
    "\n",
    "As the number of features increases:\n",
    "- Data becomes sparse (points are far apart)\n",
    "- Models need exponentially more data\n",
    "- Training becomes slower\n",
    "- Overfitting risk increases\n",
    "- Visualization becomes impossible (can't plot 100D)\n",
    "\n",
    "**Example**: With 10 features and 10 values per feature, you need 10^10 = 10 billion samples to fill the space!\n",
    "\n",
    "## Why Reduce Dimensions?\n",
    "\n",
    "1. **Visualization**: Plot 100D data in 2D/3D\n",
    "2. **Speed**: Fewer features = faster training\n",
    "3. **Memory**: Store less data\n",
    "4. **Noise Reduction**: Remove irrelevant features\n",
    "5. **Better Performance**: Sometimes less is more!\n",
    "\n",
    "## Two Approaches\n",
    "\n",
    "### 1. Feature Selection\n",
    "Keep a subset of original features\n",
    "- Example: Keep only \"age\" and \"income\", remove \"zip code\"\n",
    "\n",
    "### 2. Feature Extraction (PCA)\n",
    "Create new features as combinations of original features\n",
    "- Example: PC1 = 0.7\u00d7age + 0.3\u00d7income\n",
    "- More powerful but less interpretable\n",
    "\n",
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is the most popular dimensionality reduction technique. It finds new axes (principal components) that capture maximum variance in the data.\n",
    "\n",
    "### How PCA Works\n",
    "\n",
    "1. **Standardize** the data (mean=0, std=1)\n",
    "2. **Compute covariance** matrix (how features relate)\n",
    "3. **Find eigenvectors** (directions of maximum variance)\n",
    "4. **Sort by eigenvalues** (variance explained)\n",
    "5. **Project data** onto top K components\n",
    "\n",
    "Think of it as rotating your data to find the best viewing angle!\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**Principal Components (PCs)**:\n",
    "- PC1: Direction of maximum variance\n",
    "- PC2: Direction of second-most variance (perpendicular to PC1)\n",
    "- PC3: Third-most variance (perpendicular to PC1 and PC2)\n",
    "- ...\n",
    "\n",
    "**Explained Variance**:\n",
    "- How much information each PC captures\n",
    "- Example: PC1=70%, PC2=20%, PC3=8%, PC4=2%\n",
    "- Keep enough PCs to explain 90-95% of variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntax\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Always standardize first!\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Create PCA model\n",
    "pca = PCA(n_components=2)  # Reduce to 2 dimensions\n",
    "\n",
    "# Fit and transform\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Access results\n",
    "explained_var = pca.explained_variance_ratio_  # % variance explained\n",
    "components = pca.components_  # Principal component vectors\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: PCA on Iris Dataset\n",
    "print(\"PCA Example: Iris Dataset\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load full Iris dataset (4 features)\n",
    "iris = load_iris()\n",
    "X_iris_full = iris.data\n",
    "y_iris_full = iris.target\n",
    "\n",
    "print(f\"Original data shape: {X_iris_full.shape}\")\n",
    "print(f\"Features: {iris.feature_names}\")\n",
    "print(f\"\\nCannot visualize 4D data easily!\")\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_iris_scaled = scaler.fit_transform(X_iris_full)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)  # Reduce to 2D for visualization\n",
    "X_iris_pca = pca.fit_transform(X_iris_scaled)\n",
    "\n",
    "print(f\"\\nPCA-transformed data shape: {X_iris_pca.shape}\")\n",
    "print(f\"\\nExplained Variance Ratio:\")\n",
    "for i, var in enumerate(pca.explained_variance_ratio_):\n",
    "    print(f\"  PC{i+1}: {var:.3f} ({var*100:.1f}%)\")\n",
    "\n",
    "total_var = sum(pca.explained_variance_ratio_)\n",
    "print(f\"\\nTotal variance explained by 2 PCs: {total_var:.3f} ({total_var*100:.1f}%)\")\n",
    "print(\"We kept 97.8% of information with just 2 dimensions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize before and after PCA\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Before PCA (using first 2 original features)\n",
    "axes[0].scatter(X_iris_full[:, 0], X_iris_full[:, 1], \n",
    "               c=y_iris_full, cmap='viridis', alpha=0.6, s=50)\n",
    "axes[0].set_xlabel(iris.feature_names[0])\n",
    "axes[0].set_ylabel(iris.feature_names[1])\n",
    "axes[0].set_title('Original Features (2 of 4)\\nNot well separated')\n",
    "\n",
    "# After PCA (2 principal components)\n",
    "axes[1].scatter(X_iris_pca[:, 0], X_iris_pca[:, 1], \n",
    "               c=y_iris_full, cmap='viridis', alpha=0.6, s=50)\n",
    "axes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)')\n",
    "axes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)')\n",
    "axes[1].set_title('Principal Components\\nMuch better separation!')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPCA found the best 2D projection to separate the species!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Explained Variance Analysis\n",
    "print(\"How Many Components Should We Keep?\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Fit PCA with all components\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_iris_scaled)\n",
    "\n",
    "# Calculate cumulative variance\n",
    "cumulative_var = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "print(\"\\nVariance explained by each component:\")\n",
    "for i, (var, cum_var) in enumerate(zip(pca_full.explained_variance_ratio_, cumulative_var)):\n",
    "    print(f\"  PC{i+1}: {var:.3f} (cumulative: {cum_var:.3f})\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Individual variance\n",
    "axes[0].bar(range(1, 5), pca_full.explained_variance_ratio_)\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('Explained Variance Ratio')\n",
    "axes[0].set_title('Variance Explained by Each PC')\n",
    "axes[0].set_xticks(range(1, 5))\n",
    "\n",
    "# Cumulative variance\n",
    "axes[1].plot(range(1, 5), cumulative_var, 'bo-', linewidth=2, markersize=8)\n",
    "axes[1].axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\n",
    "axes[1].set_xlabel('Number of Components')\n",
    "axes[1].set_ylabel('Cumulative Explained Variance')\n",
    "axes[1].set_title('Cumulative Variance Explained')\n",
    "axes[1].set_xticks(range(1, 5))\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRule of thumb: Keep enough PCs to explain 90-95% variance\")\n",
    "print(f\"For Iris: 2 components explain {cumulative_var[1]*100:.1f}% (good enough!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 6: Practical Applications\n",
    "---\n",
    "\n",
    "## Application 1: Customer Segmentation + PCA\n",
    "\n",
    "Combining clustering and PCA for real-world customer analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create realistic customer dataset with many features\n",
    "np.random.seed(42)\n",
    "n = 500\n",
    "\n",
    "# Generate 8 features for customers\n",
    "customer_features = pd.DataFrame({\n",
    "    'age': np.random.randint(18, 70, n),\n",
    "    'income': np.random.normal(50000, 20000, n).clip(20000, 150000),\n",
    "    'spending_score': np.random.randint(1, 100, n),\n",
    "    'years_customer': np.random.randint(0, 20, n),\n",
    "    'num_purchases': np.random.poisson(10, n),\n",
    "    'avg_transaction': np.random.normal(100, 30, n).clip(10, 500),\n",
    "    'loyalty_points': np.random.randint(0, 10000, n),\n",
    "    'satisfaction': np.random.randint(1, 11, n)\n",
    "})\n",
    "\n",
    "print(\"Customer Dataset\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Shape: {customer_features.shape}\")\n",
    "print(f\"\\nFeatures: {list(customer_features.columns)}\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(customer_features.head())\n",
    "\n",
    "print(\"\\nChallenge: 8 features - too many to visualize!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Apply PCA to reduce to 2D\n",
    "scaler = StandardScaler()\n",
    "X_customers_scaled = scaler.fit_transform(customer_features)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_customers_pca = pca.fit_transform(X_customers_scaled)\n",
    "\n",
    "print(\"Step 1: PCA Dimensionality Reduction\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Reduced from {customer_features.shape[1]}D to {X_customers_pca.shape[1]}D\")\n",
    "print(f\"Variance explained: {sum(pca.explained_variance_ratio_)*100:.1f}%\")\n",
    "\n",
    "# Step 2: Apply K-Means clustering on PCA features\n",
    "kmeans_final = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "clusters = kmeans_final.fit_predict(X_customers_pca)\n",
    "\n",
    "print(f\"\\nStep 2: K-Means Clustering\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Number of clusters: 4\")\n",
    "print(f\"\\nCluster sizes:\")\n",
    "print(pd.Series(clusters).value_counts().sort_index())\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "scatter = plt.scatter(X_customers_pca[:, 0], X_customers_pca[:, 1], \n",
    "                     c=clusters, cmap='viridis', alpha=0.6, s=50)\n",
    "plt.scatter(kmeans_final.cluster_centers_[:, 0], \n",
    "           kmeans_final.cluster_centers_[:, 1],\n",
    "           c='red', marker='X', s=300, edgecolors='black', linewidths=2)\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)')\n",
    "plt.title('Customer Segmentation using PCA + K-Means')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze segments\n",
    "customer_features['Cluster'] = clusters\n",
    "print(\"\\nCustomer Segment Profiles:\")\n",
    "print(customer_features.groupby('Cluster').mean().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Module Summary\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### Unsupervised Learning\n",
    "- Works with **unlabeled data** to find patterns\n",
    "- Two main types: **Clustering** and **Dimensionality Reduction**\n",
    "- Most real-world data is unlabeled (cheap, abundant)\n",
    "\n",
    "### K-Means Clustering\n",
    "- Most popular clustering algorithm\n",
    "- Partitions data into K clusters by minimizing within-cluster variance\n",
    "- **Pros**: Fast, scalable, simple\n",
    "- **Cons**: Need to specify K, assumes spherical clusters\n",
    "- **Use cases**: Customer segmentation, image compression, anomaly detection\n",
    "\n",
    "### Hierarchical Clustering\n",
    "- Builds a tree (dendrogram) of clusters\n",
    "- **Agglomerative**: Bottom-up (merge clusters)\n",
    "- Don't need to specify K upfront\n",
    "- Dendrograms help visualize cluster hierarchy\n",
    "- **Slower** than K-Means but more flexible\n",
    "\n",
    "### Clustering Evaluation\n",
    "- **Elbow Method**: Plot inertia vs K, look for elbow\n",
    "- **Silhouette Score**: Measures cluster quality (-1 to +1)\n",
    "  - >0.7: Strong structure\n",
    "  - 0.5-0.7: Reasonable\n",
    "  - <0.5: Weak/no structure\n",
    "\n",
    "### PCA (Principal Component Analysis)\n",
    "- Reduces dimensionality while preserving information\n",
    "- Finds axes of maximum variance (principal components)\n",
    "- **Always standardize** data first!\n",
    "- **Explained variance**: How much info each PC captures\n",
    "- **Rule of thumb**: Keep PCs that explain 90-95% variance\n",
    "- **Use cases**: Visualization, noise reduction, speed up training\n",
    "\n",
    "### When to Use What\n",
    "\n",
    "| Task | Method | Why |\n",
    "|------|--------|-----|\n",
    "| Group similar customers | K-Means | Fast, simple, works well for marketing segments |\n",
    "| Understand cluster hierarchy | Hierarchical | Get dendrogram, flexible K |\n",
    "| Visualize high-D data | PCA | Reduce to 2D/3D for plotting |\n",
    "| Speed up model training | PCA | Fewer features = faster training |\n",
    "| Remove noise | PCA | Keep only high-variance components |\n",
    "| Preprocessing for supervised learning | Both | PCA then classification/regression |\n",
    "\n",
    "## Next Module\n",
    "\n",
    "In **Module 18: Model Optimization and Hyperparameter Tuning**, we'll learn how to:\n",
    "- Systematically tune model parameters\n",
    "- Use Grid Search and Random Search\n",
    "- Prevent overfitting\n",
    "- Build robust ML pipelines\n",
    "\n",
    "## Additional Practice\n",
    "\n",
    "Try these challenges to reinforce your learning:\n",
    "\n",
    "1. **Customer Segmentation**: Load a real customer dataset (e.g., from Kaggle) and:\n",
    "   - Apply K-Means with different K values\n",
    "   - Use Elbow Method and Silhouette Score to choose K\n",
    "   - Profile each segment (what makes them unique?)\n",
    "\n",
    "2. **Image Compression**: Use K-Means to compress an image:\n",
    "   - Load an image as RGB pixel values\n",
    "   - Cluster pixels into K=16 colors\n",
    "   - Replace each pixel with its cluster center\n",
    "   - Compare original vs compressed size\n",
    "\n",
    "3. **PCA Visualization**: Load the Wine or Breast Cancer dataset:\n",
    "   - Apply PCA to reduce to 2D\n",
    "   - Visualize in 2D colored by class\n",
    "   - How many PCs needed for 95% variance?\n",
    "\n",
    "4. **Combined Workflow**: On Iris dataset:\n",
    "   - Apply PCA to reduce from 4D to 2D\n",
    "   - Apply K-Means on PCA features\n",
    "   - Compare with K-Means on original features\n",
    "   - Which works better? Why?\n",
    "\n",
    "5. **Hierarchical Clustering**: Create a dendrogram for:\n",
    "   - A small dataset (20-30 points)\n",
    "   - Interpret the dendrogram\n",
    "   - Try different linkage methods (ward, complete, average)\n",
    "   - How do results differ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}