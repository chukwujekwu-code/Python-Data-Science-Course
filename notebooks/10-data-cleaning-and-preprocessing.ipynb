{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 10: Data Cleaning and Preprocessing\n",
    "\n",
    "## Topics Covered\n",
    "1. Common Data Quality Issues\n",
    "2. Handling Duplicates\n",
    "3. Dealing with Missing Values (Strategies)\n",
    "4. Outlier Detection and Treatment\n",
    "5. Data Normalization and Standardization\n",
    "6. Encoding Categorical Variables\n",
    "7. Feature Engineering Basics\n",
    "8. Data Cleaning Pipeline Project\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will be able to:\n",
    "- Identify and diagnose common data quality issues\n",
    "- Handle duplicates and missing values appropriately\n",
    "- Detect and treat outliers using various methods\n",
    "- Scale and normalize numeric features\n",
    "- Encode categorical variables for machine learning\n",
    "- Create new features from existing data\n",
    "- Build reusable data cleaning pipelines\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 1: Common Data Quality Issues\n",
    "---\n",
    "\n",
    "## Why Data Cleaning Matters\n",
    "\n",
    "Real-world data is messy. Data scientists spend 60-80% of their time on data cleaning and preparation. Common issues include:\n",
    "\n",
    "- **Missing values**: Empty cells, NaN, None\n",
    "- **Duplicates**: Repeated records\n",
    "- **Inconsistent formatting**: \"USA\", \"U.S.A.\", \"United States\"\n",
    "- **Invalid data**: Negative ages, future dates\n",
    "- **Outliers**: Extreme values that may be errors\n",
    "- **Wrong data types**: Numbers stored as strings\n",
    "\n",
    "### The Cost of Dirty Data\n",
    "\n",
    "- Incorrect analysis and conclusions\n",
    "- Poor model performance\n",
    "- Wasted computational resources\n",
    "- Loss of stakeholder trust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Settings\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messy dataset to demonstrate cleaning techniques\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "messy_data = pd.DataFrame({\n",
    "    'customer_id': [1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
    "    'name': ['John Smith', 'jane doe', 'ROBERT BROWN', 'Robert Brown', 'Alice Wilson',\n",
    "             'Bob Johnson', None, 'Carol White', 'David Lee', 'Eva Martinez',\n",
    "             'Frank Miller', 'Grace Kim', 'Henry Chen', 'Ivy Taylor', 'Jack Davis', 'Kate Moore'],\n",
    "    'age': [25, 30, -5, 35, 150, 28, 42, 33, None, 29, 45, 38, 27, 31, 1000, 26],\n",
    "    'email': ['john@email.com', 'jane@email.com', 'robert@email.com', 'robert@email.com',\n",
    "              'invalid-email', 'bob@email.com', 'carol@email.com', 'carol@email.com',\n",
    "              'david@email.com', None, 'frank@email.com', 'grace@email.com',\n",
    "              'henry@email.com', 'ivy@email.com', 'jack@email.com', 'kate@email.com'],\n",
    "    'income': [50000, 60000, 75000, 75000, 45000, None, 80000, 55000, \n",
    "               62000, 58000, 1000000, 72000, 48000, 65000, 52000, 69000],\n",
    "    'city': ['New York', 'new york', 'NYC', 'New York', 'Los Angeles',\n",
    "             'LA', 'Chicago', 'chicago', 'Houston', 'Phoenix',\n",
    "             'Philadelphia', 'San Antonio', 'San Diego', 'Dallas', 'San Jose', 'Austin'],\n",
    "    'signup_date': ['2023-01-15', '2023/02/20', 'March 5, 2023', '2023-03-05',\n",
    "                    '2023-04-10', '2023-05-15', '2023-06-20', '15-07-2023',\n",
    "                    '2023-08-25', '2023-09-30', '2023-10-05', '2023-11-10',\n",
    "                    '2023-12-15', '2024-01-20', '2024-02-25', '2024-03-01']\n",
    "})\n",
    "\n",
    "print(\"Messy Dataset Created:\")\n",
    "print(messy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Diagnosing data quality issues\n",
    "\n",
    "def diagnose_data_quality(df):\n",
    "    \"\"\"Comprehensive data quality diagnosis.\"\"\"\n",
    "    print(\"DATA QUALITY REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"\\n1. BASIC INFO\")\n",
    "    print(f\"   Rows: {len(df)}, Columns: {len(df.columns)}\")\n",
    "    \n",
    "    # Missing values\n",
    "    print(f\"\\n2. MISSING VALUES\")\n",
    "    missing = df.isnull().sum()\n",
    "    if missing.sum() > 0:\n",
    "        for col, count in missing[missing > 0].items():\n",
    "            print(f\"   {col}: {count} ({count/len(df)*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"   None\")\n",
    "    \n",
    "    # Duplicates\n",
    "    print(f\"\\n3. DUPLICATES\")\n",
    "    print(f\"   Full row duplicates: {df.duplicated().sum()}\")\n",
    "    \n",
    "    # Data types\n",
    "    print(f\"\\n4. DATA TYPES\")\n",
    "    for col, dtype in df.dtypes.items():\n",
    "        print(f\"   {col}: {dtype}\")\n",
    "    \n",
    "    # Numeric column stats\n",
    "    print(f\"\\n5. NUMERIC COLUMNS - POTENTIAL ISSUES\")\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "    for col in numeric_cols:\n",
    "        min_val, max_val = df[col].min(), df[col].max()\n",
    "        print(f\"   {col}: min={min_val}, max={max_val}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "diagnose_data_quality(messy_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 2: Handling Duplicates\n",
    "---\n",
    "\n",
    "Duplicates can occur from data entry errors, system glitches, or merging datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Identifying duplicates\n",
    "\n",
    "# Check for exact duplicates (all columns match)\n",
    "exact_duplicates = messy_data[messy_data.duplicated(keep=False)]\n",
    "print(f\"Exact duplicate rows: {len(exact_duplicates) // 2}\")\n",
    "print(exact_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Check duplicates based on specific columns\n",
    "\n",
    "# Duplicates based on customer_id\n",
    "id_duplicates = messy_data[messy_data.duplicated(subset=['customer_id'], keep=False)]\n",
    "print(\"Duplicate customer IDs:\")\n",
    "print(id_duplicates)\n",
    "\n",
    "# Duplicates based on email\n",
    "email_duplicates = messy_data[messy_data.duplicated(subset=['email'], keep=False)].dropna(subset=['email'])\n",
    "print(\"\\nDuplicate emails:\")\n",
    "print(email_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Removing duplicates\n",
    "\n",
    "df = messy_data.copy()\n",
    "print(f\"Before: {len(df)} rows\")\n",
    "\n",
    "# Remove exact duplicates (keep first occurrence)\n",
    "df_no_dups = df.drop_duplicates()\n",
    "print(f\"After removing exact duplicates: {len(df_no_dups)} rows\")\n",
    "\n",
    "# Remove duplicates based on customer_id (keep first)\n",
    "df_unique_id = df.drop_duplicates(subset=['customer_id'], keep='first')\n",
    "print(f\"After removing duplicate IDs: {len(df_unique_id)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Different strategies for handling duplicates\n",
    "\n",
    "df = messy_data.copy()\n",
    "\n",
    "# Keep first occurrence\n",
    "keep_first = df.drop_duplicates(subset=['customer_id'], keep='first')\n",
    "\n",
    "# Keep last occurrence\n",
    "keep_last = df.drop_duplicates(subset=['customer_id'], keep='last')\n",
    "\n",
    "# Remove all duplicates (keep none)\n",
    "keep_none = df.drop_duplicates(subset=['customer_id'], keep=False)\n",
    "\n",
    "print(f\"Original: {len(df)} rows\")\n",
    "print(f\"Keep first: {len(keep_first)} rows\")\n",
    "print(f\"Keep last: {len(keep_last)} rows\")\n",
    "print(f\"Keep none: {len(keep_none)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 3: Dealing with Missing Values\n",
    "---\n",
    "\n",
    "Missing values require careful consideration. The right strategy depends on:\n",
    "- How much data is missing\n",
    "- Why data is missing (random vs. systematic)\n",
    "- The importance of the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real data with missing values\n",
    "sales = pd.read_csv('assets/datasets/sales_data.csv')\n",
    "employees = pd.read_csv('assets/datasets/employees.csv')\n",
    "\n",
    "# Check missing values\n",
    "print(\"Missing values in Sales:\")\n",
    "print(sales.isnull().sum()[sales.isnull().sum() > 0])\n",
    "\n",
    "print(\"\\nMissing values in Employees:\")\n",
    "print(employees.isnull().sum()[employees.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Visualizing missing values\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Missing value counts\n",
    "sales_missing = sales.isnull().sum()\n",
    "sales_missing = sales_missing[sales_missing > 0]\n",
    "axes[0].bar(sales_missing.index, sales_missing.values, color='coral')\n",
    "axes[0].set_title('Missing Values in Sales Data')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Missing value pattern (heatmap)\n",
    "# Sample for visualization\n",
    "sample = sales.sample(100, random_state=42)\n",
    "sns.heatmap(sample.isnull(), cbar=True, yticklabels=False, ax=axes[1])\n",
    "axes[1].set_title('Missing Value Pattern (Sample)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Value Strategies\n",
    "\n",
    "| Strategy | When to Use |\n",
    "|----------|-------------|\n",
    "| **Delete rows** | Small % missing, data is MCAR |\n",
    "| **Delete columns** | >50% missing, not important |\n",
    "| **Fill with mean/median** | Numeric, random missing |\n",
    "| **Fill with mode** | Categorical data |\n",
    "| **Fill with constant** | Known default value |\n",
    "| **Forward/backward fill** | Time series data |\n",
    "| **Interpolation** | Ordered numeric data |\n",
    "| **Model-based imputation** | Complex patterns |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Strategy 1 - Delete rows with missing values\n",
    "\n",
    "df = sales.copy()\n",
    "print(f\"Before: {len(df)} rows\")\n",
    "\n",
    "# Drop rows with ANY missing value\n",
    "df_dropped_any = df.dropna()\n",
    "print(f\"After dropna(): {len(df_dropped_any)} rows\")\n",
    "\n",
    "# Drop rows only if specific columns are missing\n",
    "df_dropped_subset = df.dropna(subset=['unit_price', 'sales_rep'])\n",
    "print(f\"After dropna(subset): {len(df_dropped_subset)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Strategy 2 - Fill with statistics\n",
    "\n",
    "df = sales.copy()\n",
    "\n",
    "# Fill numeric with mean\n",
    "mean_price = df['unit_price'].mean()\n",
    "df['unit_price_mean'] = df['unit_price'].fillna(mean_price)\n",
    "\n",
    "# Fill numeric with median (better for skewed data)\n",
    "median_price = df['unit_price'].median()\n",
    "df['unit_price_median'] = df['unit_price'].fillna(median_price)\n",
    "\n",
    "# Fill categorical with mode\n",
    "mode_rep = df['sales_rep'].mode()[0]\n",
    "df['sales_rep_filled'] = df['sales_rep'].fillna(mode_rep)\n",
    "\n",
    "print(f\"Mean price: ${mean_price:.2f}\")\n",
    "print(f\"Median price: ${median_price:.2f}\")\n",
    "print(f\"Mode sales rep: {mode_rep}\")\n",
    "\n",
    "# Verify\n",
    "print(f\"\\nRemaining missing in unit_price_mean: {df['unit_price_mean'].isnull().sum()}\")\n",
    "print(f\"Remaining missing in sales_rep_filled: {df['sales_rep_filled'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Strategy 3 - Group-based imputation\n",
    "\n",
    "df = sales.copy()\n",
    "\n",
    "# Fill missing unit_price with category median\n",
    "df['unit_price_filled'] = df.groupby('category')['unit_price'].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "\n",
    "# Check result\n",
    "print(\"Category-based median prices:\")\n",
    "print(df.groupby('category')['unit_price'].median())\n",
    "print(f\"\\nRemaining missing: {df['unit_price_filled'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Strategy 4 - Fill with constant/indicator\n",
    "\n",
    "df = sales.copy()\n",
    "\n",
    "# Fill with a constant value\n",
    "df['sales_rep_const'] = df['sales_rep'].fillna('Unknown')\n",
    "\n",
    "# Create a missing indicator (useful for ML)\n",
    "df['rating_missing'] = df['customer_rating'].isnull().astype(int)\n",
    "df['customer_rating_filled'] = df['customer_rating'].fillna(0)\n",
    "\n",
    "print(\"Sales rep values after filling:\")\n",
    "print(df['sales_rep_const'].value_counts())\n",
    "\n",
    "print(f\"\\nMissing rating indicator sum: {df['rating_missing'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Strategy 5 - Forward/backward fill (for time series)\n",
    "\n",
    "# Create time series example\n",
    "ts_data = pd.DataFrame({\n",
    "    'date': pd.date_range('2023-01-01', periods=10),\n",
    "    'value': [100, np.nan, np.nan, 110, 115, np.nan, 120, 125, np.nan, 130]\n",
    "})\n",
    "\n",
    "print(\"Original:\")\n",
    "print(ts_data)\n",
    "\n",
    "# Forward fill\n",
    "ts_data['ffill'] = ts_data['value'].ffill()\n",
    "\n",
    "# Backward fill\n",
    "ts_data['bfill'] = ts_data['value'].bfill()\n",
    "\n",
    "# Interpolation\n",
    "ts_data['interpolate'] = ts_data['value'].interpolate()\n",
    "\n",
    "print(\"\\nAfter filling:\")\n",
    "print(ts_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercise 3.1\n",
    "\n",
    "**Task:** Clean the employees dataset:\n",
    "1. Check for missing values in all columns\n",
    "2. Fill missing performance_rating with the department median\n",
    "3. Fill missing email with a pattern based on name\n",
    "4. Handle missing bonus appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3.1\n",
    "\n",
    "employees = pd.read_csv('assets/datasets/employees.csv')\n",
    "\n",
    "# 1. Check missing values\n",
    "print(\"1. Missing Values:\")\n",
    "print(employees.isnull().sum())\n",
    "\n",
    "# 2. Fill performance_rating with department median\n",
    "employees['performance_rating'] = employees.groupby('department')['performance_rating'].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "\n",
    "# 3. Fill missing email based on name\n",
    "def generate_email(row):\n",
    "    if pd.isnull(row['email']):\n",
    "        first = row['first_name'].lower()\n",
    "        last = row['last_name'].lower()\n",
    "        return f\"{first}.{last}@company.com\"\n",
    "    return row['email']\n",
    "\n",
    "employees['email'] = employees.apply(generate_email, axis=1)\n",
    "\n",
    "# 4. Handle missing bonus (fill with 0, as no bonus)\n",
    "employees['bonus'] = employees['bonus'].fillna(0)\n",
    "\n",
    "print(\"\\nAfter cleaning:\")\n",
    "print(employees.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 4: Outlier Detection and Treatment\n",
    "---\n",
    "\n",
    "Outliers are extreme values that differ significantly from other observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Outlier detection methods\n",
    "\n",
    "employees = pd.read_csv('assets/datasets/employees.csv')\n",
    "\n",
    "def detect_outliers_iqr(series, multiplier=1.5):\n",
    "    \"\"\"Detect outliers using IQR method.\"\"\"\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - multiplier * IQR\n",
    "    upper = Q3 + multiplier * IQR\n",
    "    return (series < lower) | (series > upper), lower, upper\n",
    "\n",
    "def detect_outliers_zscore(series, threshold=3):\n",
    "    \"\"\"Detect outliers using Z-score method.\"\"\"\n",
    "    z_scores = np.abs((series - series.mean()) / series.std())\n",
    "    return z_scores > threshold\n",
    "\n",
    "# Apply to salary\n",
    "is_outlier_iqr, lower, upper = detect_outliers_iqr(employees['salary'])\n",
    "is_outlier_zscore = detect_outliers_zscore(employees['salary'])\n",
    "\n",
    "print(\"Salary Outlier Detection:\")\n",
    "print(f\"IQR Method: {is_outlier_iqr.sum()} outliers (bounds: ${lower:,.0f} - ${upper:,.0f})\")\n",
    "print(f\"Z-Score Method: {is_outlier_zscore.sum()} outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Visualizing outliers\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Box plot\n",
    "sns.boxplot(x=employees['salary'], ax=axes[0])\n",
    "axes[0].set_title('Box Plot - Salary')\n",
    "\n",
    "# Histogram with bounds\n",
    "sns.histplot(employees['salary'], ax=axes[1], bins=30)\n",
    "axes[1].axvline(lower, color='red', linestyle='--', label=f'Lower: ${lower:,.0f}')\n",
    "axes[1].axvline(upper, color='red', linestyle='--', label=f'Upper: ${upper:,.0f}')\n",
    "axes[1].legend()\n",
    "axes[1].set_title('Distribution with IQR Bounds')\n",
    "\n",
    "# Scatter plot highlighting outliers\n",
    "colors = ['red' if x else 'blue' for x in is_outlier_iqr]\n",
    "axes[2].scatter(range(len(employees)), employees['salary'], c=colors, alpha=0.5)\n",
    "axes[2].set_title('Outliers Highlighted (red)')\n",
    "axes[2].set_xlabel('Index')\n",
    "axes[2].set_ylabel('Salary')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Outlier treatment strategies\n",
    "\n",
    "df = employees.copy()\n",
    "\n",
    "# Strategy 1: Remove outliers\n",
    "is_outlier, lower, upper = detect_outliers_iqr(df['salary'])\n",
    "df_removed = df[~is_outlier]\n",
    "print(f\"Strategy 1 - Remove: {len(df)} -> {len(df_removed)} rows\")\n",
    "\n",
    "# Strategy 2: Cap/Winsorize (clip to bounds)\n",
    "df['salary_capped'] = df['salary'].clip(lower=lower, upper=upper)\n",
    "print(f\"\\nStrategy 2 - Cap:\")\n",
    "print(f\"  Original max: ${df['salary'].max():,.0f}\")\n",
    "print(f\"  Capped max: ${df['salary_capped'].max():,.0f}\")\n",
    "\n",
    "# Strategy 3: Replace with median\n",
    "median_salary = df['salary'].median()\n",
    "df['salary_median'] = df['salary'].copy()\n",
    "df.loc[is_outlier, 'salary_median'] = median_salary\n",
    "print(f\"\\nStrategy 3 - Replace with median (${median_salary:,.0f}): {is_outlier.sum()} values replaced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compare distributions after treatment\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "sns.histplot(df['salary'], ax=axes[0], kde=True)\n",
    "axes[0].set_title('Original')\n",
    "\n",
    "sns.histplot(df['salary_capped'], ax=axes[1], kde=True)\n",
    "axes[1].set_title('Capped (Winsorized)')\n",
    "\n",
    "sns.histplot(df['salary_median'], ax=axes[2], kde=True)\n",
    "axes[2].set_title('Outliers Replaced with Median')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 5: Data Normalization and Standardization\n",
    "---\n",
    "\n",
    "Scaling numeric features is essential for many machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Methods\n",
    "\n",
    "| Method | Formula | Range | Use Case |\n",
    "|--------|---------|-------|----------|\n",
    "| **Min-Max** | (x - min) / (max - min) | [0, 1] | Neural networks, bounded data |\n",
    "| **Standard (Z-score)** | (x - mean) / std | ~[-3, 3] | Most ML algorithms |\n",
    "| **Robust** | (x - median) / IQR | Varies | Data with outliers |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Manual scaling\n",
    "\n",
    "employees = pd.read_csv('assets/datasets/employees.csv')\n",
    "\n",
    "# Min-Max scaling\n",
    "salary = employees['salary']\n",
    "salary_minmax = (salary - salary.min()) / (salary.max() - salary.min())\n",
    "\n",
    "# Standard scaling (Z-score)\n",
    "salary_standard = (salary - salary.mean()) / salary.std()\n",
    "\n",
    "# Robust scaling\n",
    "median = salary.median()\n",
    "iqr = salary.quantile(0.75) - salary.quantile(0.25)\n",
    "salary_robust = (salary - median) / iqr\n",
    "\n",
    "# Compare\n",
    "comparison = pd.DataFrame({\n",
    "    'Original': salary,\n",
    "    'MinMax': salary_minmax,\n",
    "    'Standard': salary_standard,\n",
    "    'Robust': salary_robust\n",
    "}).describe()\n",
    "\n",
    "print(\"Scaling Comparison:\")\n",
    "print(comparison.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using scikit-learn scalers\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "# Prepare data\n",
    "salary_values = employees[['salary']].values\n",
    "\n",
    "# MinMax Scaler\n",
    "minmax_scaler = MinMaxScaler()\n",
    "salary_minmax = minmax_scaler.fit_transform(salary_values)\n",
    "\n",
    "# Standard Scaler\n",
    "standard_scaler = StandardScaler()\n",
    "salary_standard = standard_scaler.fit_transform(salary_values)\n",
    "\n",
    "# Robust Scaler\n",
    "robust_scaler = RobustScaler()\n",
    "salary_robust = robust_scaler.fit_transform(salary_values)\n",
    "\n",
    "print(\"Sklearn Scalers:\")\n",
    "print(f\"MinMax - Range: [{salary_minmax.min():.3f}, {salary_minmax.max():.3f}]\")\n",
    "print(f\"Standard - Mean: {salary_standard.mean():.3f}, Std: {salary_standard.std():.3f}\")\n",
    "print(f\"Robust - Median: {np.median(salary_robust):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Visualize scaling effects\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Original\n",
    "axes[0, 0].hist(salary_values, bins=30, edgecolor='white')\n",
    "axes[0, 0].set_title('Original')\n",
    "axes[0, 0].set_xlabel('Salary')\n",
    "\n",
    "# MinMax\n",
    "axes[0, 1].hist(salary_minmax, bins=30, edgecolor='white', color='green')\n",
    "axes[0, 1].set_title('Min-Max Scaled [0, 1]')\n",
    "\n",
    "# Standard\n",
    "axes[1, 0].hist(salary_standard, bins=30, edgecolor='white', color='orange')\n",
    "axes[1, 0].set_title('Standard Scaled (Z-score)')\n",
    "\n",
    "# Robust\n",
    "axes[1, 1].hist(salary_robust, bins=30, edgecolor='white', color='purple')\n",
    "axes[1, 1].set_title('Robust Scaled')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Scaling multiple columns\n",
    "\n",
    "# Prepare numeric data\n",
    "employees = pd.read_csv('assets/datasets/employees.csv')\n",
    "employees['hire_date'] = pd.to_datetime(employees['hire_date'])\n",
    "employees['years_exp'] = (pd.Timestamp.now() - employees['hire_date']).dt.days / 365\n",
    "employees['bonus'] = employees['bonus'].fillna(0)\n",
    "\n",
    "# Select numeric columns to scale\n",
    "numeric_cols = ['salary', 'bonus', 'years_exp']\n",
    "numeric_data = employees[numeric_cols].copy()\n",
    "\n",
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(numeric_data)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=[f\"{col}_scaled\" for col in numeric_cols])\n",
    "\n",
    "print(\"Scaled Data Statistics:\")\n",
    "print(scaled_df.describe().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 6: Encoding Categorical Variables\n",
    "---\n",
    "\n",
    "Machine learning algorithms require numeric input. Categorical variables must be encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Label Encoding (ordinal)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "employees = pd.read_csv('assets/datasets/employees.csv')\n",
    "\n",
    "# Label encode status (has inherent order)\n",
    "le = LabelEncoder()\n",
    "employees['status_encoded'] = le.fit_transform(employees['status'])\n",
    "\n",
    "print(\"Label Encoding - Status:\")\n",
    "print(pd.DataFrame({'Original': le.classes_, 'Encoded': range(len(le.classes_))}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: One-Hot Encoding (nominal)\n",
    "\n",
    "# Using pandas get_dummies\n",
    "dept_encoded = pd.get_dummies(employees['department'], prefix='dept')\n",
    "print(\"One-Hot Encoding - Department:\")\n",
    "print(dept_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: One-Hot Encoding with sklearn\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Prepare data\n",
    "dept_values = employees[['department']]\n",
    "\n",
    "# Fit and transform\n",
    "ohe = OneHotEncoder(sparse_output=False, drop='first')  # drop first to avoid multicollinearity\n",
    "dept_encoded = ohe.fit_transform(dept_values)\n",
    "\n",
    "# Create DataFrame\n",
    "dept_df = pd.DataFrame(\n",
    "    dept_encoded, \n",
    "    columns=ohe.get_feature_names_out(['department'])\n",
    ")\n",
    "\n",
    "print(\"One-Hot Encoding (sklearn, drop first):\")\n",
    "print(dept_df.head())\n",
    "print(f\"\\nShape: {dept_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Ordinal Encoding (for ordered categories)\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Create sample data with ordered categories\n",
    "education_data = pd.DataFrame({\n",
    "    'education': ['High School', 'Bachelor', 'Master', 'PhD', 'Bachelor', \n",
    "                  'High School', 'Master', 'Bachelor', 'PhD', 'High School']\n",
    "})\n",
    "\n",
    "# Define order\n",
    "education_order = [['High School', 'Bachelor', 'Master', 'PhD']]\n",
    "\n",
    "# Encode\n",
    "oe = OrdinalEncoder(categories=education_order)\n",
    "education_data['education_encoded'] = oe.fit_transform(education_data[['education']])\n",
    "\n",
    "print(\"Ordinal Encoding - Education:\")\n",
    "print(education_data.drop_duplicates().sort_values('education_encoded'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Target/Mean Encoding (for high-cardinality)\n",
    "\n",
    "sales = pd.read_csv('assets/datasets/sales_data.csv')\n",
    "\n",
    "# Calculate mean target for each product\n",
    "product_means = sales.groupby('product')['total_amount'].mean()\n",
    "\n",
    "# Map to original data\n",
    "sales['product_encoded'] = sales['product'].map(product_means)\n",
    "\n",
    "print(\"Target/Mean Encoding - Product:\")\n",
    "print(sales[['product', 'product_encoded']].drop_duplicates().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercise 6.1\n",
    "\n",
    "**Task:** Prepare the employees dataset for machine learning:\n",
    "1. One-hot encode the department column\n",
    "2. Ordinal encode the status column (Active=2, On Leave=1, Terminated=0)\n",
    "3. Standard scale the salary and years of experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 6.1\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "\n",
    "employees = pd.read_csv('assets/datasets/employees.csv')\n",
    "employees['hire_date'] = pd.to_datetime(employees['hire_date'])\n",
    "employees['years_exp'] = (pd.Timestamp.now() - employees['hire_date']).dt.days / 365\n",
    "\n",
    "# 1. One-hot encode department\n",
    "dept_dummies = pd.get_dummies(employees['department'], prefix='dept', drop_first=True)\n",
    "print(\"1. Department One-Hot Encoded:\")\n",
    "print(dept_dummies.columns.tolist())\n",
    "\n",
    "# 2. Ordinal encode status\n",
    "status_order = [['Terminated', 'On Leave', 'Active']]\n",
    "oe = OrdinalEncoder(categories=status_order)\n",
    "employees['status_encoded'] = oe.fit_transform(employees[['status']])\n",
    "print(\"\\n2. Status Ordinal Encoded:\")\n",
    "print(employees[['status', 'status_encoded']].drop_duplicates())\n",
    "\n",
    "# 3. Standard scale salary and years_exp\n",
    "scaler = StandardScaler()\n",
    "employees[['salary_scaled', 'years_exp_scaled']] = scaler.fit_transform(\n",
    "    employees[['salary', 'years_exp']]\n",
    ")\n",
    "print(\"\\n3. Scaled Variables:\")\n",
    "print(employees[['salary', 'salary_scaled', 'years_exp', 'years_exp_scaled']].describe().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 7: Feature Engineering Basics\n",
    "---\n",
    "\n",
    "Feature engineering creates new features from existing data to improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: DateTime features\n",
    "\n",
    "sales = pd.read_csv('assets/datasets/sales_data.csv', parse_dates=['date'])\n",
    "\n",
    "# Extract datetime components\n",
    "sales['year'] = sales['date'].dt.year\n",
    "sales['month'] = sales['date'].dt.month\n",
    "sales['day'] = sales['date'].dt.day\n",
    "sales['day_of_week'] = sales['date'].dt.dayofweek\n",
    "sales['day_name'] = sales['date'].dt.day_name()\n",
    "sales['quarter'] = sales['date'].dt.quarter\n",
    "sales['is_weekend'] = sales['day_of_week'].isin([5, 6]).astype(int)\n",
    "sales['is_month_end'] = sales['date'].dt.is_month_end.astype(int)\n",
    "\n",
    "print(\"DateTime Features:\")\n",
    "print(sales[['date', 'year', 'month', 'day_of_week', 'is_weekend', 'quarter']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Mathematical transformations\n",
    "\n",
    "sales = pd.read_csv('assets/datasets/sales_data.csv')\n",
    "\n",
    "# Log transformation (for skewed data)\n",
    "sales['log_amount'] = np.log1p(sales['total_amount'])  # log1p handles zeros\n",
    "\n",
    "# Square root transformation\n",
    "sales['sqrt_amount'] = np.sqrt(sales['total_amount'])\n",
    "\n",
    "# Polynomial features\n",
    "sales['amount_squared'] = sales['total_amount'] ** 2\n",
    "\n",
    "# Compare distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].hist(sales['total_amount'], bins=30)\n",
    "axes[0].set_title('Original (Skewed)')\n",
    "\n",
    "axes[1].hist(sales['log_amount'], bins=30, color='green')\n",
    "axes[1].set_title('Log Transformed')\n",
    "\n",
    "axes[2].hist(sales['sqrt_amount'], bins=30, color='orange')\n",
    "axes[2].set_title('Square Root Transformed')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Binning/Discretization\n",
    "\n",
    "employees = pd.read_csv('assets/datasets/employees.csv')\n",
    "\n",
    "# Equal-width bins\n",
    "employees['salary_bin'] = pd.cut(employees['salary'], bins=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "# Quantile-based bins (equal frequency)\n",
    "employees['salary_quantile'] = pd.qcut(employees['salary'], q=5, labels=['Q1', 'Q2', 'Q3', 'Q4', 'Q5'])\n",
    "\n",
    "# Custom bins\n",
    "bins = [0, 50000, 75000, 100000, 150000, float('inf')]\n",
    "labels = ['Entry', 'Junior', 'Mid', 'Senior', 'Executive']\n",
    "employees['salary_level'] = pd.cut(employees['salary'], bins=bins, labels=labels)\n",
    "\n",
    "print(\"Binned Salary:\")\n",
    "print(employees[['salary', 'salary_bin', 'salary_quantile', 'salary_level']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Aggregation features\n",
    "\n",
    "sales = pd.read_csv('assets/datasets/sales_data.csv')\n",
    "\n",
    "# Product-level statistics\n",
    "product_stats = sales.groupby('product').agg(\n",
    "    avg_amount=('total_amount', 'mean'),\n",
    "    total_sales=('total_amount', 'sum'),\n",
    "    transaction_count=('transaction_id', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# Merge back to original data\n",
    "sales = sales.merge(product_stats, on='product', suffixes=('', '_product'))\n",
    "\n",
    "# Calculate deviation from product average\n",
    "sales['amount_vs_product_avg'] = sales['total_amount'] - sales['avg_amount']\n",
    "\n",
    "print(\"Aggregation Features:\")\n",
    "print(sales[['product', 'total_amount', 'avg_amount', 'amount_vs_product_avg']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Text-based features\n",
    "\n",
    "employees = pd.read_csv('assets/datasets/employees.csv')\n",
    "\n",
    "# Name length\n",
    "employees['name_length'] = (employees['first_name'] + ' ' + employees['last_name']).str.len()\n",
    "\n",
    "# First letter\n",
    "employees['first_letter'] = employees['first_name'].str[0]\n",
    "\n",
    "# Title word count\n",
    "employees['title_words'] = employees['title'].str.split().str.len()\n",
    "\n",
    "# Contains 'Senior' or 'Manager'\n",
    "employees['is_senior'] = employees['title'].str.contains('Senior|Lead', case=False).astype(int)\n",
    "employees['is_manager'] = employees['title'].str.contains('Manager', case=False).astype(int)\n",
    "\n",
    "print(\"Text-based Features:\")\n",
    "print(employees[['first_name', 'title', 'is_senior', 'is_manager', 'title_words']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Interaction features\n",
    "\n",
    "employees = pd.read_csv('assets/datasets/employees.csv')\n",
    "employees['hire_date'] = pd.to_datetime(employees['hire_date'])\n",
    "employees['years_exp'] = (pd.Timestamp.now() - employees['hire_date']).dt.days / 365\n",
    "employees['bonus'] = employees['bonus'].fillna(0)\n",
    "employees['performance_rating'] = employees['performance_rating'].fillna(3)\n",
    "\n",
    "# Salary per year of experience\n",
    "employees['salary_per_year'] = employees['salary'] / (employees['years_exp'] + 1)\n",
    "\n",
    "# Total compensation\n",
    "employees['total_compensation'] = employees['salary'] + employees['bonus']\n",
    "\n",
    "# Bonus ratio\n",
    "employees['bonus_ratio'] = employees['bonus'] / employees['salary']\n",
    "\n",
    "# Performance x Experience interaction\n",
    "employees['perf_exp_interaction'] = employees['performance_rating'] * employees['years_exp']\n",
    "\n",
    "print(\"Interaction Features:\")\n",
    "print(employees[['salary', 'bonus', 'years_exp', 'total_compensation', 'salary_per_year']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 8: Data Cleaning Pipeline Project\n",
    "---\n",
    "\n",
    "Let's build a complete, reusable data cleaning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Data Cleaning Pipeline Class\n",
    "\n",
    "class DataCleaningPipeline:\n",
    "    \"\"\"A reusable data cleaning pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cleaning_log = []\n",
    "        self.original_shape = None\n",
    "        \n",
    "    def log(self, message):\n",
    "        \"\"\"Add message to cleaning log.\"\"\"\n",
    "        self.cleaning_log.append(message)\n",
    "        print(f\"  -> {message}\")\n",
    "    \n",
    "    def fit_transform(self, df):\n",
    "        \"\"\"Apply all cleaning steps.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"DATA CLEANING PIPELINE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        self.original_shape = df.shape\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Step 1: Remove duplicates\n",
    "        print(\"\\nStep 1: Removing Duplicates\")\n",
    "        initial_rows = len(df)\n",
    "        df = df.drop_duplicates()\n",
    "        self.log(f\"Removed {initial_rows - len(df)} duplicate rows\")\n",
    "        \n",
    "        # Step 2: Handle missing values\n",
    "        print(\"\\nStep 2: Handling Missing Values\")\n",
    "        for col in df.columns:\n",
    "            missing = df[col].isnull().sum()\n",
    "            if missing > 0:\n",
    "                if df[col].dtype in ['float64', 'int64']:\n",
    "                    median_val = df[col].median()\n",
    "                    df[col] = df[col].fillna(median_val)\n",
    "                    self.log(f\"{col}: filled {missing} missing with median ({median_val:.2f})\")\n",
    "                else:\n",
    "                    mode_val = df[col].mode()[0] if len(df[col].mode()) > 0 else 'Unknown'\n",
    "                    df[col] = df[col].fillna(mode_val)\n",
    "                    self.log(f\"{col}: filled {missing} missing with mode ({mode_val})\")\n",
    "        \n",
    "        # Step 3: Fix data types\n",
    "        print(\"\\nStep 3: Fixing Data Types\")\n",
    "        for col in df.columns:\n",
    "            if 'date' in col.lower():\n",
    "                try:\n",
    "                    df[col] = pd.to_datetime(df[col])\n",
    "                    self.log(f\"{col}: converted to datetime\")\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # Step 4: Standardize text\n",
    "        print(\"\\nStep 4: Standardizing Text\")\n",
    "        for col in df.select_dtypes(include='object').columns:\n",
    "            df[col] = df[col].str.strip()\n",
    "            self.log(f\"{col}: stripped whitespace\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"COMPLETE: {self.original_shape} -> {df.shape}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_report(self):\n",
    "        \"\"\"Return cleaning report.\"\"\"\n",
    "        return \"\\n\".join(self.cleaning_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using the pipeline\n",
    "\n",
    "# Load data\n",
    "sales = pd.read_csv('assets/datasets/sales_data.csv')\n",
    "\n",
    "# Create and run pipeline\n",
    "pipeline = DataCleaningPipeline()\n",
    "clean_sales = pipeline.fit_transform(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Complete preprocessing function\n",
    "\n",
    "def preprocess_employees(df):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for employees dataset.\n",
    "    Returns cleaned and feature-engineered DataFrame.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Handle dates\n",
    "    df['hire_date'] = pd.to_datetime(df['hire_date'])\n",
    "    df['years_exp'] = (pd.Timestamp.now() - df['hire_date']).dt.days / 365\n",
    "    \n",
    "    # 2. Handle missing values\n",
    "    df['performance_rating'] = df.groupby('department')['performance_rating'].transform(\n",
    "        lambda x: x.fillna(x.median())\n",
    "    )\n",
    "    df['bonus'] = df['bonus'].fillna(0)\n",
    "    df['email'] = df.apply(\n",
    "        lambda row: f\"{row['first_name'].lower()}.{row['last_name'].lower()}@company.com\" \n",
    "        if pd.isnull(row['email']) else row['email'], axis=1\n",
    "    )\n",
    "    \n",
    "    # 3. Feature engineering\n",
    "    df['total_compensation'] = df['salary'] + df['bonus']\n",
    "    df['salary_per_year'] = df['salary'] / (df['years_exp'] + 1)\n",
    "    df['is_manager'] = df['title'].str.contains('Manager', case=False).astype(int)\n",
    "    df['is_senior'] = df['title'].str.contains('Senior|Lead', case=False).astype(int)\n",
    "    \n",
    "    # 4. Encode categoricals\n",
    "    df['status_encoded'] = df['status'].map({'Terminated': 0, 'On Leave': 1, 'Active': 2})\n",
    "    \n",
    "    # 5. Remove unnecessary columns\n",
    "    df = df.drop(['hire_date'], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply preprocessing\n",
    "employees = pd.read_csv('assets/datasets/employees.csv')\n",
    "employees_clean = preprocess_employees(employees)\n",
    "\n",
    "print(\"Preprocessed Employees:\")\n",
    "print(employees_clean.head())\n",
    "print(f\"\\nShape: {employees_clean.shape}\")\n",
    "print(f\"\\nColumns: {employees_clean.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercise 8.1\n",
    "\n",
    "**Task:** Create a complete preprocessing pipeline for the sales data that:\n",
    "1. Handles missing values\n",
    "2. Creates datetime features (year, month, day_of_week, is_weekend)\n",
    "3. Creates aggregation features (product average, region average)\n",
    "4. One-hot encodes category and region\n",
    "5. Returns a clean DataFrame ready for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 8.1\n",
    "\n",
    "def preprocess_sales(df):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for sales dataset.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Handle missing values\n",
    "    df['unit_price'] = df.groupby('category')['unit_price'].transform(\n",
    "        lambda x: x.fillna(x.median())\n",
    "    )\n",
    "    df['sales_rep'] = df['sales_rep'].fillna('Unknown')\n",
    "    df['customer_rating'] = df['customer_rating'].fillna(df['customer_rating'].median())\n",
    "    \n",
    "    # 2. DateTime features\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    \n",
    "    # 3. Aggregation features\n",
    "    product_avg = df.groupby('product')['total_amount'].transform('mean')\n",
    "    df['product_avg_amount'] = product_avg\n",
    "    df['vs_product_avg'] = df['total_amount'] - product_avg\n",
    "    \n",
    "    region_avg = df.groupby('region')['total_amount'].transform('mean')\n",
    "    df['region_avg_amount'] = region_avg\n",
    "    \n",
    "    # 4. One-hot encode\n",
    "    category_dummies = pd.get_dummies(df['category'], prefix='cat', drop_first=True)\n",
    "    region_dummies = pd.get_dummies(df['region'], prefix='reg', drop_first=True)\n",
    "    df = pd.concat([df, category_dummies, region_dummies], axis=1)\n",
    "    \n",
    "    # 5. Clean up\n",
    "    df = df.drop(['date'], axis=1)\n",
    "    \n",
    "    print(f\"Preprocessing complete: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    print(f\"Missing values remaining: {df.isnull().sum().sum()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply\n",
    "sales = pd.read_csv('assets/datasets/sales_data.csv')\n",
    "sales_clean = preprocess_sales(sales)\n",
    "\n",
    "print(\"\\nNew columns:\")\n",
    "new_cols = [c for c in sales_clean.columns if c not in sales.columns]\n",
    "print(new_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Module Summary\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Data quality is critical**: Garbage in, garbage out\n",
    "2. **Duplicates**: Identify and remove based on business logic\n",
    "3. **Missing values**: Choose strategy based on pattern and importance\n",
    "4. **Outliers**: Detect with IQR/Z-score, treat appropriately\n",
    "5. **Scaling**: Essential for many ML algorithms\n",
    "6. **Encoding**: Convert categories to numbers for ML\n",
    "7. **Feature engineering**: Create informative features from raw data\n",
    "8. **Pipelines**: Make preprocessing reproducible and reusable\n",
    "\n",
    "## Common Preprocessing Steps\n",
    "\n",
    "```python\n",
    "# Typical workflow\n",
    "1. df.drop_duplicates()           # Remove duplicates\n",
    "2. df.fillna() / df.dropna()      # Handle missing\n",
    "3. Outlier detection/treatment     # Handle outliers\n",
    "4. pd.to_datetime()               # Fix date types\n",
    "5. StandardScaler / MinMaxScaler  # Scale numeric\n",
    "6. pd.get_dummies() / LabelEncoder # Encode categorical\n",
    "7. Feature engineering             # Create new features\n",
    "```\n",
    "\n",
    "## Next Module\n",
    "\n",
    "In the next module, we'll cover **Statistics for Data Science** - the statistical foundations needed for data analysis and machine learning.\n",
    "\n",
    "## Additional Practice\n",
    "\n",
    "1. **End-to-End Pipeline**: Create a complete preprocessing pipeline that can be saved and loaded for new data.\n",
    "\n",
    "2. **Missing Value Analysis**: Compare different imputation strategies on a dataset and evaluate their impact on model performance.\n",
    "\n",
    "3. **Feature Engineering Challenge**: Create at least 10 new features from the sales data that might be predictive of transaction amount."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
