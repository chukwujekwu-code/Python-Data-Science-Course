{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 9: Exploratory Data Analysis\n",
    "\n",
    "## Topics Covered\n",
    "1. The EDA Process and Workflow\n",
    "2. Understanding Your Data\n",
    "3. Univariate Analysis\n",
    "4. Bivariate Analysis\n",
    "5. Multivariate Analysis\n",
    "6. Identifying Patterns and Anomalies\n",
    "7. EDA Case Study: Real Dataset\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will be able to:\n",
    "- Follow a systematic approach to exploring new datasets\n",
    "- Summarize data using appropriate statistics and visualizations\n",
    "- Identify relationships between variables\n",
    "- Detect outliers, anomalies, and data quality issues\n",
    "- Generate hypotheses from data exploration\n",
    "- Document findings for stakeholders\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 1: The EDA Process and Workflow\n",
    "---\n",
    "\n",
    "## What is Exploratory Data Analysis?\n",
    "\n",
    "Exploratory Data Analysis (EDA) is the critical first step in any data science project. It involves:\n",
    "\n",
    "- **Understanding** the structure and content of your data\n",
    "- **Summarizing** main characteristics using statistics and visualizations\n",
    "- **Discovering** patterns, relationships, and anomalies\n",
    "- **Formulating** hypotheses for further analysis\n",
    "\n",
    "### Why This Matters in Data Science\n",
    "\n",
    "EDA helps you:\n",
    "- Avoid costly mistakes from misunderstanding your data\n",
    "- Make informed decisions about data cleaning and preprocessing\n",
    "- Choose appropriate modeling techniques\n",
    "- Communicate findings to stakeholders effectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The EDA Workflow\n",
    "\n",
    "A systematic approach to EDA:\n",
    "\n",
    "1. **Data Collection & Loading**\n",
    "   - Load data from various sources\n",
    "   - Initial inspection of structure\n",
    "\n",
    "2. **Data Quality Assessment**\n",
    "   - Check for missing values\n",
    "   - Identify data types\n",
    "   - Detect duplicates\n",
    "\n",
    "3. **Univariate Analysis**\n",
    "   - Analyze each variable individually\n",
    "   - Distribution, central tendency, spread\n",
    "\n",
    "4. **Bivariate Analysis**\n",
    "   - Relationships between pairs of variables\n",
    "   - Correlations, comparisons\n",
    "\n",
    "5. **Multivariate Analysis**\n",
    "   - Complex relationships among multiple variables\n",
    "   - Patterns and clusters\n",
    "\n",
    "6. **Findings & Insights**\n",
    "   - Summarize key discoveries\n",
    "   - Document for next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for EDA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Settings\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 2: Understanding Your Data\n",
    "---\n",
    "\n",
    "The first step is to understand what you're working with: structure, size, data types, and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our datasets\n",
    "sales = pd.read_csv('assets/datasets/sales_data.csv', parse_dates=['date'])\n",
    "employees = pd.read_csv('assets/datasets/employees.csv')\n",
    "\n",
    "print(f\"Sales dataset: {sales.shape[0]} rows, {sales.shape[1]} columns\")\n",
    "print(f\"Employees dataset: {employees.shape[0]} rows, {employees.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: First look at the data\n",
    "\n",
    "print(\"Sales Data - First 5 Rows:\")\n",
    "print(sales.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Data types and memory usage\n",
    "\n",
    "print(\"Data Types and Info:\")\n",
    "print(sales.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Quick statistics\n",
    "\n",
    "print(\"Numeric Summary:\")\n",
    "print(sales.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Categorical summary\n",
    "\n",
    "print(\"Categorical Summary:\")\n",
    "print(sales.describe(include='object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Missing values analysis\n",
    "\n",
    "def missing_values_summary(df):\n",
    "    \"\"\"Create a summary of missing values.\"\"\"\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    \n",
    "    summary = pd.DataFrame({\n",
    "        'Missing Count': missing,\n",
    "        'Missing %': missing_pct,\n",
    "        'Data Type': df.dtypes\n",
    "    })\n",
    "    \n",
    "    return summary[summary['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "\n",
    "print(\"Missing Values in Sales Data:\")\n",
    "print(missing_values_summary(sales))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Check for duplicates\n",
    "\n",
    "print(f\"Duplicate rows in sales: {sales.duplicated().sum()}\")\n",
    "print(f\"Duplicate transaction IDs: {sales['transaction_id'].duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Unique values in categorical columns\n",
    "\n",
    "categorical_cols = sales.select_dtypes(include='object').columns\n",
    "\n",
    "print(\"Unique Values per Categorical Column:\")\n",
    "print(\"-\" * 40)\n",
    "for col in categorical_cols:\n",
    "    unique_count = sales[col].nunique()\n",
    "    print(f\"{col}: {unique_count} unique values\")\n",
    "    if unique_count <= 10:\n",
    "        print(f\"  Values: {sales[col].unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Date range analysis\n",
    "\n",
    "print(f\"Date range: {sales['date'].min()} to {sales['date'].max()}\")\n",
    "print(f\"Time span: {(sales['date'].max() - sales['date'].min()).days} days\")\n",
    "print(f\"Unique dates: {sales['date'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA Helper Function\n",
    "\n",
    "Let's create a reusable function for initial data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_eda(df, name=\"Dataset\"):\n",
    "    \"\"\"Perform initial exploratory data analysis on a DataFrame.\"\"\"\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"EDA Report: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"\\n1. SHAPE: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    \n",
    "    # Data types\n",
    "    print(f\"\\n2. DATA TYPES:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "    # Missing values\n",
    "    missing = df.isnull().sum()\n",
    "    if missing.sum() > 0:\n",
    "        print(f\"\\n3. MISSING VALUES:\")\n",
    "        print(missing[missing > 0])\n",
    "    else:\n",
    "        print(f\"\\n3. MISSING VALUES: None\")\n",
    "    \n",
    "    # Duplicates\n",
    "    dups = df.duplicated().sum()\n",
    "    print(f\"\\n4. DUPLICATE ROWS: {dups}\")\n",
    "    \n",
    "    # Numeric summary\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(f\"\\n5. NUMERIC COLUMNS SUMMARY:\")\n",
    "        print(df[numeric_cols].describe().round(2))\n",
    "    \n",
    "    # Categorical summary\n",
    "    cat_cols = df.select_dtypes(include='object').columns\n",
    "    if len(cat_cols) > 0:\n",
    "        print(f\"\\n6. CATEGORICAL COLUMNS:\")\n",
    "        for col in cat_cols:\n",
    "            print(f\"   {col}: {df[col].nunique()} unique values\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "\n",
    "# Test the function\n",
    "initial_eda(sales, \"Sales Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 3: Univariate Analysis\n",
    "---\n",
    "\n",
    "Univariate analysis examines each variable individually to understand its distribution and characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric Variables\n",
    "\n",
    "For numeric variables, we examine:\n",
    "- Central tendency (mean, median, mode)\n",
    "- Spread (standard deviation, range, IQR)\n",
    "- Distribution shape (skewness, kurtosis)\n",
    "- Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Detailed analysis of a numeric variable\n",
    "\n",
    "def analyze_numeric(series, name):\n",
    "    \"\"\"Detailed analysis of a numeric variable.\"\"\"\n",
    "    print(f\"Analysis of: {name}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Central tendency\n",
    "    print(f\"Mean: {series.mean():.2f}\")\n",
    "    print(f\"Median: {series.median():.2f}\")\n",
    "    print(f\"Mode: {series.mode().iloc[0]:.2f}\")\n",
    "    \n",
    "    # Spread\n",
    "    print(f\"\\nStd Dev: {series.std():.2f}\")\n",
    "    print(f\"Variance: {series.var():.2f}\")\n",
    "    print(f\"Range: {series.min():.2f} - {series.max():.2f}\")\n",
    "    print(f\"IQR: {series.quantile(0.75) - series.quantile(0.25):.2f}\")\n",
    "    \n",
    "    # Shape\n",
    "    print(f\"\\nSkewness: {series.skew():.2f}\")\n",
    "    print(f\"Kurtosis: {series.kurtosis():.2f}\")\n",
    "    \n",
    "    # Percentiles\n",
    "    print(f\"\\nPercentiles:\")\n",
    "    for p in [5, 25, 50, 75, 95]:\n",
    "        print(f\"  {p}th: {series.quantile(p/100):.2f}\")\n",
    "\n",
    "analyze_numeric(sales['total_amount'], 'Total Amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Visualizing numeric distribution\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Histogram with KDE\n",
    "sns.histplot(sales['total_amount'], kde=True, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Distribution of Transaction Amount')\n",
    "axes[0, 0].axvline(sales['total_amount'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[0, 0].axvline(sales['total_amount'].median(), color='green', linestyle='--', label='Median')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Box plot\n",
    "sns.boxplot(x=sales['total_amount'], ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Box Plot of Transaction Amount')\n",
    "\n",
    "# Violin plot\n",
    "sns.violinplot(x=sales['total_amount'], ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Violin Plot of Transaction Amount')\n",
    "\n",
    "# QQ plot (for normality check)\n",
    "from scipy import stats\n",
    "stats.probplot(sales['total_amount'], dist=\"norm\", plot=axes[1, 1])\n",
    "axes[1, 1].set_title('Q-Q Plot (Normality Check)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Analyzing all numeric columns\n",
    "\n",
    "numeric_cols = sales.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "fig, axes = plt.subplots(1, len(numeric_cols), figsize=(5*len(numeric_cols), 4))\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    sns.histplot(sales[col].dropna(), kde=True, ax=axes[i])\n",
    "    axes[i].set_title(f'Distribution: {col}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Variables\n",
    "\n",
    "For categorical variables, we examine:\n",
    "- Frequency distribution\n",
    "- Proportion of each category\n",
    "- Rare categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Categorical variable analysis\n",
    "\n",
    "def analyze_categorical(series, name, top_n=10):\n",
    "    \"\"\"Detailed analysis of a categorical variable.\"\"\"\n",
    "    print(f\"Analysis of: {name}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Basic stats\n",
    "    print(f\"Unique values: {series.nunique()}\")\n",
    "    print(f\"Most common: {series.mode().iloc[0]}\")\n",
    "    print(f\"Missing: {series.isnull().sum()}\")\n",
    "    \n",
    "    # Value counts\n",
    "    print(f\"\\nTop {top_n} Values:\")\n",
    "    vc = series.value_counts()\n",
    "    vc_pct = series.value_counts(normalize=True) * 100\n",
    "    \n",
    "    for i, (val, count) in enumerate(vc.head(top_n).items()):\n",
    "        print(f\"  {val}: {count} ({vc_pct[val]:.1f}%)\")\n",
    "\n",
    "analyze_categorical(sales['product'], 'Product')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Visualizing categorical distributions\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Category distribution\n",
    "sales['category'].value_counts().plot(kind='bar', ax=axes[0], color='steelblue')\n",
    "axes[0].set_title('Transactions by Category')\n",
    "axes[0].set_xlabel('Category')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Region distribution\n",
    "sales['region'].value_counts().plot(kind='bar', ax=axes[1], color='coral')\n",
    "axes[1].set_title('Transactions by Region')\n",
    "axes[1].set_xlabel('Region')\n",
    "\n",
    "# Pie chart for category\n",
    "sales['category'].value_counts().plot(kind='pie', ax=axes[2], autopct='%1.1f%%')\n",
    "axes[2].set_title('Category Distribution')\n",
    "axes[2].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercise 3.1\n",
    "\n",
    "**Task:** Perform univariate analysis on the employees dataset:\n",
    "1. Analyze the salary distribution (statistics and visualization)\n",
    "2. Analyze the department distribution\n",
    "3. Identify any potential outliers in salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3.1\n",
    "\n",
    "employees = pd.read_csv('assets/datasets/employees.csv')\n",
    "\n",
    "# 1. Salary analysis\n",
    "print(\"1. SALARY ANALYSIS\")\n",
    "print(\"=\"*40)\n",
    "analyze_numeric(employees['salary'], 'Salary')\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "sns.histplot(employees['salary'], kde=True, ax=axes[0])\n",
    "axes[0].set_title('Salary Distribution')\n",
    "\n",
    "sns.boxplot(x=employees['salary'], ax=axes[1])\n",
    "axes[1].set_title('Salary Box Plot')\n",
    "\n",
    "# 2. Department distribution\n",
    "employees['department'].value_counts().plot(kind='bar', ax=axes[2], color='steelblue')\n",
    "axes[2].set_title('Employees by Department')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Outlier detection\n",
    "print(\"\\n3. OUTLIER DETECTION\")\n",
    "print(\"=\"*40)\n",
    "Q1 = employees['salary'].quantile(0.25)\n",
    "Q3 = employees['salary'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = employees[(employees['salary'] < lower_bound) | (employees['salary'] > upper_bound)]\n",
    "print(f\"IQR: ${IQR:,.2f}\")\n",
    "print(f\"Bounds: ${lower_bound:,.2f} - ${upper_bound:,.2f}\")\n",
    "print(f\"Outliers found: {len(outliers)}\")\n",
    "if len(outliers) > 0:\n",
    "    print(outliers[['first_name', 'last_name', 'department', 'salary']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 4: Bivariate Analysis\n",
    "---\n",
    "\n",
    "Bivariate analysis examines the relationship between two variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric vs Numeric\n",
    "\n",
    "For two numeric variables, we examine:\n",
    "- Correlation coefficient\n",
    "- Scatter plots\n",
    "- Trend analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Correlation between numeric variables\n",
    "\n",
    "employees = pd.read_csv('assets/datasets/employees.csv')\n",
    "employees['hire_date'] = pd.to_datetime(employees['hire_date'])\n",
    "employees['years_exp'] = (pd.Timestamp.now() - employees['hire_date']).dt.days / 365\n",
    "\n",
    "# Fill missing values for analysis\n",
    "employees['bonus'] = employees['bonus'].fillna(0)\n",
    "employees['performance_rating'] = employees['performance_rating'].fillna(employees['performance_rating'].median())\n",
    "\n",
    "# Calculate correlations\n",
    "numeric_cols = ['salary', 'bonus', 'years_exp', 'performance_rating']\n",
    "correlations = employees[numeric_cols].corr()\n",
    "\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlations.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Visualizing correlations\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Heatmap\n",
    "sns.heatmap(correlations, annot=True, cmap='coolwarm', center=0, ax=axes[0], fmt='.2f')\n",
    "axes[0].set_title('Correlation Heatmap')\n",
    "\n",
    "# Scatter plot: Experience vs Salary\n",
    "sns.scatterplot(data=employees, x='years_exp', y='salary', alpha=0.6, ax=axes[1])\n",
    "axes[1].set_title(f\"Experience vs Salary (r = {correlations.loc['years_exp', 'salary']:.3f})\")\n",
    "axes[1].set_xlabel('Years of Experience')\n",
    "axes[1].set_ylabel('Salary ($)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Scatter with regression line\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "sns.regplot(data=employees, x='years_exp', y='salary', scatter_kws={'alpha': 0.5}, ax=ax)\n",
    "ax.set_title('Experience vs Salary with Trend Line')\n",
    "ax.set_xlabel('Years of Experience')\n",
    "ax.set_ylabel('Salary ($)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical vs Numeric\n",
    "\n",
    "For comparing a numeric variable across categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Salary by department\n",
    "\n",
    "dept_salary = employees.groupby('department')['salary'].agg(['mean', 'median', 'std', 'count'])\n",
    "dept_salary = dept_salary.sort_values('mean', ascending=False)\n",
    "\n",
    "print(\"Salary Statistics by Department:\")\n",
    "print(dept_salary.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Visualizing categorical vs numeric\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Box plot\n",
    "sns.boxplot(data=employees, x='department', y='salary', ax=axes[0])\n",
    "axes[0].set_title('Salary by Department')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Violin plot\n",
    "sns.violinplot(data=employees, x='department', y='salary', ax=axes[1])\n",
    "axes[1].set_title('Salary Distribution by Department')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Bar plot with error bars\n",
    "sns.barplot(data=employees, x='department', y='salary', ax=axes[2], errorbar='sd')\n",
    "axes[2].set_title('Average Salary by Department')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Sales amount by category\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Box plot\n",
    "sns.boxplot(data=sales, x='category', y='total_amount', ax=axes[0])\n",
    "axes[0].set_title('Transaction Amount by Category')\n",
    "\n",
    "# Compare regions\n",
    "sns.boxplot(data=sales, x='region', y='total_amount', ax=axes[1])\n",
    "axes[1].set_title('Transaction Amount by Region')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical vs Categorical\n",
    "\n",
    "For comparing two categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Cross-tabulation\n",
    "\n",
    "crosstab = pd.crosstab(sales['category'], sales['region'])\n",
    "print(\"Transaction Count: Category vs Region\")\n",
    "print(crosstab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Visualizing category vs category\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Heatmap of counts\n",
    "sns.heatmap(crosstab, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title('Transaction Count Heatmap')\n",
    "\n",
    "# Stacked bar\n",
    "crosstab.plot(kind='bar', stacked=True, ax=axes[1])\n",
    "axes[1].set_title('Transactions by Category and Region')\n",
    "axes[1].set_xlabel('Category')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].legend(title='Region')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercise 4.1\n",
    "\n",
    "**Task:** Analyze the relationship between employee performance rating and salary:\n",
    "1. Calculate the correlation\n",
    "2. Create a scatter plot with regression line\n",
    "3. Compare average salary across performance rating levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 4.1\n",
    "\n",
    "employees = pd.read_csv('assets/datasets/employees.csv')\n",
    "employees['performance_rating'] = employees['performance_rating'].fillna(employees['performance_rating'].median())\n",
    "\n",
    "# 1. Correlation\n",
    "corr = employees['performance_rating'].corr(employees['salary'])\n",
    "print(f\"1. Correlation between Performance and Salary: {corr:.3f}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 2. Scatter with regression\n",
    "sns.regplot(data=employees, x='performance_rating', y='salary', \n",
    "            scatter_kws={'alpha': 0.5}, ax=axes[0])\n",
    "axes[0].set_title(f'Performance vs Salary (r = {corr:.3f})')\n",
    "\n",
    "# 3. Average salary by rating\n",
    "rating_salary = employees.groupby('performance_rating')['salary'].agg(['mean', 'count'])\n",
    "print(\"\\n3. Average Salary by Performance Rating:\")\n",
    "print(rating_salary.round(2))\n",
    "\n",
    "sns.barplot(data=employees, x='performance_rating', y='salary', ax=axes[1])\n",
    "axes[1].set_title('Average Salary by Performance Rating')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 5: Multivariate Analysis\n",
    "---\n",
    "\n",
    "Multivariate analysis examines relationships among three or more variables simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Pair plot for multiple variables\n",
    "\n",
    "employees = pd.read_csv('assets/datasets/employees.csv')\n",
    "employees['hire_date'] = pd.to_datetime(employees['hire_date'])\n",
    "employees['years_exp'] = (pd.Timestamp.now() - employees['hire_date']).dt.days / 365\n",
    "employees['performance_rating'] = employees['performance_rating'].fillna(3)\n",
    "\n",
    "# Select columns for pair plot\n",
    "plot_cols = ['salary', 'years_exp', 'performance_rating']\n",
    "\n",
    "g = sns.pairplot(employees[plot_cols + ['department']], \n",
    "                 hue='department', \n",
    "                 height=2.5,\n",
    "                 plot_kws={'alpha': 0.6})\n",
    "g.fig.suptitle('Employee Metrics by Department', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Scatter with multiple dimensions (color and size)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    employees['years_exp'],\n",
    "    employees['salary'],\n",
    "    c=employees['performance_rating'],\n",
    "    s=employees['performance_rating'] * 30,\n",
    "    alpha=0.6,\n",
    "    cmap='viridis'\n",
    ")\n",
    "\n",
    "plt.colorbar(scatter, label='Performance Rating')\n",
    "ax.set_xlabel('Years of Experience')\n",
    "ax.set_ylabel('Salary ($)')\n",
    "ax.set_title('Salary vs Experience (color & size = Performance)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Faceted plots (small multiples)\n",
    "\n",
    "g = sns.FacetGrid(employees, col='department', col_wrap=3, height=4)\n",
    "g.map_dataframe(sns.scatterplot, x='years_exp', y='salary', alpha=0.6)\n",
    "g.set_axis_labels('Years of Experience', 'Salary ($)')\n",
    "g.set_titles('{col_name}')\n",
    "g.fig.suptitle('Experience vs Salary by Department', y=1.02)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Multi-dimensional pivot table\n",
    "\n",
    "sales = pd.read_csv('assets/datasets/sales_data.csv', parse_dates=['date'])\n",
    "sales['quarter'] = sales['date'].dt.quarter\n",
    "sales['year'] = sales['date'].dt.year\n",
    "\n",
    "# Pivot by multiple dimensions\n",
    "pivot = sales.pivot_table(\n",
    "    values='total_amount',\n",
    "    index=['category', 'region'],\n",
    "    columns='year',\n",
    "    aggfunc='sum'\n",
    ")\n",
    "\n",
    "print(\"Sales by Category, Region, and Year:\")\n",
    "print(pivot.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Grouped bar chart with multiple categories\n",
    "\n",
    "# Average transaction by category and region\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "sns.barplot(data=sales, x='category', y='total_amount', hue='region', ax=ax)\n",
    "\n",
    "ax.set_title('Average Transaction by Category and Region')\n",
    "ax.set_ylabel('Average Amount ($)')\n",
    "ax.legend(title='Region', bbox_to_anchor=(1.02, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 6: Identifying Patterns and Anomalies\n",
    "---\n",
    "\n",
    "A key goal of EDA is to find patterns, trends, and anomalies in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-Based Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Temporal patterns in sales\n",
    "\n",
    "sales = pd.read_csv('assets/datasets/sales_data.csv', parse_dates=['date'])\n",
    "\n",
    "# Add time components\n",
    "sales['year'] = sales['date'].dt.year\n",
    "sales['month'] = sales['date'].dt.month\n",
    "sales['day_of_week'] = sales['date'].dt.day_name()\n",
    "sales['quarter'] = sales['date'].dt.quarter\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Monthly pattern\n",
    "monthly = sales.groupby('month')['total_amount'].mean()\n",
    "axes[0, 0].bar(monthly.index, monthly.values, color='steelblue')\n",
    "axes[0, 0].set_title('Average Sales by Month')\n",
    "axes[0, 0].set_xlabel('Month')\n",
    "axes[0, 0].set_ylabel('Average Sales ($)')\n",
    "\n",
    "# Day of week pattern\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "daily = sales.groupby('day_of_week')['total_amount'].mean().reindex(day_order)\n",
    "axes[0, 1].bar(range(7), daily.values, color='coral')\n",
    "axes[0, 1].set_title('Average Sales by Day of Week')\n",
    "axes[0, 1].set_xticks(range(7))\n",
    "axes[0, 1].set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "\n",
    "# Quarterly trend\n",
    "quarterly = sales.groupby(['year', 'quarter'])['total_amount'].sum().reset_index()\n",
    "quarterly['period'] = quarterly['year'].astype(str) + '-Q' + quarterly['quarter'].astype(str)\n",
    "axes[1, 0].plot(quarterly['period'], quarterly['total_amount'], 'o-')\n",
    "axes[1, 0].set_title('Quarterly Sales Trend')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Year over year comparison\n",
    "yearly = sales.groupby('year')['total_amount'].sum()\n",
    "axes[1, 1].bar(yearly.index.astype(str), yearly.values, color='seagreen')\n",
    "axes[1, 1].set_title('Total Sales by Year')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Outlier detection methods\n",
    "\n",
    "def detect_outliers(df, column):\n",
    "    \"\"\"Detect outliers using IQR and Z-score methods.\"\"\"\n",
    "    data = df[column].dropna()\n",
    "    \n",
    "    # IQR method\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    iqr_outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    \n",
    "    # Z-score method\n",
    "    z_scores = np.abs((data - data.mean()) / data.std())\n",
    "    zscore_outliers = df[z_scores > 3]\n",
    "    \n",
    "    print(f\"Outlier Detection for: {column}\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"\\nIQR Method:\")\n",
    "    print(f\"  Bounds: {lower_bound:.2f} - {upper_bound:.2f}\")\n",
    "    print(f\"  Outliers: {len(iqr_outliers)} ({len(iqr_outliers)/len(df)*100:.1f}%)\")\n",
    "    print(f\"\\nZ-Score Method (|z| > 3):\")\n",
    "    print(f\"  Outliers: {len(zscore_outliers)} ({len(zscore_outliers)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    return iqr_outliers, zscore_outliers\n",
    "\n",
    "iqr_out, zscore_out = detect_outliers(sales, 'total_amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Visualizing outliers\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Box plot shows outliers as points\n",
    "sns.boxplot(x=sales['total_amount'], ax=axes[0])\n",
    "axes[0].set_title('Box Plot (outliers as points)')\n",
    "\n",
    "# Histogram with outlier threshold\n",
    "Q1 = sales['total_amount'].quantile(0.25)\n",
    "Q3 = sales['total_amount'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "sns.histplot(sales['total_amount'], ax=axes[1], bins=50)\n",
    "axes[1].axvline(upper_bound, color='red', linestyle='--', label=f'Upper Bound: ${upper_bound:.0f}')\n",
    "axes[1].legend()\n",
    "axes[1].set_title('Distribution with Outlier Threshold')\n",
    "\n",
    "# Scatter plot highlighting outliers\n",
    "is_outlier = sales['total_amount'] > upper_bound\n",
    "axes[2].scatter(range(len(sales)), sales['total_amount'], \n",
    "                c=is_outlier, cmap='coolwarm', alpha=0.5)\n",
    "axes[2].axhline(upper_bound, color='red', linestyle='--')\n",
    "axes[2].set_title('Transactions (red = outliers)')\n",
    "axes[2].set_xlabel('Transaction Index')\n",
    "axes[2].set_ylabel('Amount ($)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Investigating outliers\n",
    "\n",
    "print(\"High-value transactions (outliers):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "outliers = sales[sales['total_amount'] > upper_bound].sort_values('total_amount', ascending=False)\n",
    "print(f\"Count: {len(outliers)}\")\n",
    "print(f\"\\nBy Category:\")\n",
    "print(outliers['category'].value_counts())\n",
    "print(f\"\\nBy Product:\")\n",
    "print(outliers['product'].value_counts())\n",
    "print(f\"\\nSample outliers:\")\n",
    "print(outliers[['date', 'product', 'quantity', 'total_amount']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 7: EDA Case Study\n",
    "---\n",
    "\n",
    "Let's perform a complete EDA on the employees dataset as a comprehensive example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "employees = pd.read_csv('assets/datasets/employees.csv')\n",
    "employees['hire_date'] = pd.to_datetime(employees['hire_date'])\n",
    "employees['years_exp'] = (pd.Timestamp.now() - employees['hire_date']).dt.days / 365\n",
    "\n",
    "print(\"EMPLOYEE DATA - EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initial Overview\n",
    "print(\"\\n1. DATA OVERVIEW\")\n",
    "print(\"-\"*40)\n",
    "print(f\"Shape: {employees.shape}\")\n",
    "print(f\"\\nColumn Types:\")\n",
    "print(employees.dtypes)\n",
    "print(f\"\\nFirst few rows:\")\n",
    "employees.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Data Quality\n",
    "print(\"\\n2. DATA QUALITY\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Missing values\n",
    "print(\"Missing Values:\")\n",
    "missing = employees.isnull().sum()\n",
    "print(missing[missing > 0])\n",
    "\n",
    "# Duplicates\n",
    "print(f\"\\nDuplicate employee IDs: {employees['employee_id'].duplicated().sum()}\")\n",
    "\n",
    "# Status distribution\n",
    "print(f\"\\nEmployee Status:\")\n",
    "print(employees['status'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Univariate Analysis - Key Variables\n",
    "print(\"\\n3. KEY VARIABLE DISTRIBUTIONS\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Salary distribution\n",
    "sns.histplot(employees['salary'], kde=True, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Salary Distribution')\n",
    "axes[0, 0].axvline(employees['salary'].median(), color='red', linestyle='--', label='Median')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Department counts\n",
    "dept_order = employees['department'].value_counts().index\n",
    "sns.countplot(data=employees, y='department', order=dept_order, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Employees by Department')\n",
    "\n",
    "# Years of experience\n",
    "sns.histplot(employees['years_exp'], kde=True, ax=axes[0, 2])\n",
    "axes[0, 2].set_title('Years of Experience')\n",
    "\n",
    "# Performance ratings\n",
    "employees['performance_rating'].value_counts().sort_index().plot(kind='bar', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Performance Rating Distribution')\n",
    "axes[1, 0].set_xlabel('Rating')\n",
    "\n",
    "# Status\n",
    "employees['status'].value_counts().plot(kind='pie', autopct='%1.1f%%', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Employee Status')\n",
    "\n",
    "# Salary box plot\n",
    "sns.boxplot(x=employees['salary'], ax=axes[1, 2])\n",
    "axes[1, 2].set_title('Salary Box Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Bivariate Analysis\n",
    "print(\"\\n4. RELATIONSHIPS BETWEEN VARIABLES\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Salary by department\n",
    "dept_order = employees.groupby('department')['salary'].median().sort_values(ascending=False).index\n",
    "sns.boxplot(data=employees, x='salary', y='department', order=dept_order, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Salary by Department')\n",
    "\n",
    "# Experience vs Salary\n",
    "sns.scatterplot(data=employees, x='years_exp', y='salary', hue='department', alpha=0.6, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Experience vs Salary')\n",
    "axes[0, 1].legend(bbox_to_anchor=(1.02, 1), title='Department')\n",
    "\n",
    "# Performance vs Salary\n",
    "employees_clean = employees.dropna(subset=['performance_rating'])\n",
    "sns.boxplot(data=employees_clean, x='performance_rating', y='salary', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Salary by Performance Rating')\n",
    "\n",
    "# Correlation heatmap\n",
    "numeric_cols = ['salary', 'bonus', 'years_exp', 'performance_rating']\n",
    "employees_for_corr = employees[numeric_cols].dropna()\n",
    "sns.heatmap(employees_for_corr.corr(), annot=True, cmap='coolwarm', center=0, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Correlation Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Key Insights Summary\n",
    "print(\"\\n5. KEY FINDINGS & INSIGHTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Salary insights\n",
    "print(\"\\nSALARY INSIGHTS:\")\n",
    "print(f\"  - Average salary: ${employees['salary'].mean():,.0f}\")\n",
    "print(f\"  - Median salary: ${employees['salary'].median():,.0f}\")\n",
    "print(f\"  - Salary range: ${employees['salary'].min():,.0f} - ${employees['salary'].max():,.0f}\")\n",
    "\n",
    "# Top paying departments\n",
    "print(\"\\nTOP PAYING DEPARTMENTS (by median salary):\")\n",
    "dept_median = employees.groupby('department')['salary'].median().sort_values(ascending=False)\n",
    "for i, (dept, sal) in enumerate(dept_median.head(3).items(), 1):\n",
    "    print(f\"  {i}. {dept}: ${sal:,.0f}\")\n",
    "\n",
    "# Experience correlation\n",
    "exp_sal_corr = employees['years_exp'].corr(employees['salary'])\n",
    "print(f\"\\nEXPERIENCE vs SALARY CORRELATION: {exp_sal_corr:.3f}\")\n",
    "if exp_sal_corr > 0.3:\n",
    "    print(\"  -> Positive relationship: More experience tends to mean higher salary\")\n",
    "\n",
    "# Performance insights\n",
    "perf_sal = employees.groupby('performance_rating')['salary'].mean()\n",
    "print(f\"\\nSALARY BY PERFORMANCE RATING:\")\n",
    "for rating, sal in perf_sal.items():\n",
    "    print(f\"  Rating {rating:.0f}: ${sal:,.0f}\")\n",
    "\n",
    "# Data quality notes\n",
    "print(f\"\\nDATA QUALITY NOTES:\")\n",
    "missing_pct = employees.isnull().sum() / len(employees) * 100\n",
    "for col, pct in missing_pct[missing_pct > 0].items():\n",
    "    print(f\"  - {col}: {pct:.1f}% missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercise 7.1\n",
    "\n",
    "**Task:** Perform a complete EDA on the sales data and create a summary report that includes:\n",
    "1. Data overview and quality assessment\n",
    "2. Top 5 products by total sales\n",
    "3. Sales trends over time (monthly)\n",
    "4. Regional performance comparison\n",
    "5. At least 3 key insights or findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 7.1\n",
    "\n",
    "sales = pd.read_csv('assets/datasets/sales_data.csv', parse_dates=['date'])\n",
    "\n",
    "print(\"SALES DATA - EDA SUMMARY REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Data Overview\n",
    "print(\"\\n1. DATA OVERVIEW\")\n",
    "print(\"-\"*40)\n",
    "print(f\"Total transactions: {len(sales):,}\")\n",
    "print(f\"Date range: {sales['date'].min().date()} to {sales['date'].max().date()}\")\n",
    "print(f\"Total revenue: ${sales['total_amount'].sum():,.2f}\")\n",
    "print(f\"\\nMissing values:\")\n",
    "print(sales.isnull().sum()[sales.isnull().sum() > 0])\n",
    "\n",
    "# 2. Top 5 Products\n",
    "print(\"\\n2. TOP 5 PRODUCTS BY TOTAL SALES\")\n",
    "print(\"-\"*40)\n",
    "top_products = sales.groupby('product')['total_amount'].sum().nlargest(5)\n",
    "for i, (product, amount) in enumerate(top_products.items(), 1):\n",
    "    print(f\"  {i}. {product}: ${amount:,.2f}\")\n",
    "\n",
    "# 3. Monthly Trend\n",
    "print(\"\\n3. MONTHLY SALES TREND\")\n",
    "print(\"-\"*40)\n",
    "monthly = sales.groupby(sales['date'].dt.to_period('M'))['total_amount'].sum()\n",
    "print(f\"Highest month: {monthly.idxmax()} (${monthly.max():,.2f})\")\n",
    "print(f\"Lowest month: {monthly.idxmin()} (${monthly.min():,.2f})\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "monthly.plot(kind='line', marker='o', ax=axes[0])\n",
    "axes[0].set_title('Monthly Sales Trend')\n",
    "axes[0].set_ylabel('Sales ($)')\n",
    "\n",
    "# 4. Regional Performance\n",
    "print(\"\\n4. REGIONAL PERFORMANCE\")\n",
    "print(\"-\"*40)\n",
    "region_stats = sales.groupby('region').agg(\n",
    "    total_sales=('total_amount', 'sum'),\n",
    "    avg_transaction=('total_amount', 'mean'),\n",
    "    num_transactions=('transaction_id', 'count')\n",
    ").sort_values('total_sales', ascending=False)\n",
    "print(region_stats.round(2))\n",
    "\n",
    "region_stats['total_sales'].plot(kind='bar', ax=axes[1], color='steelblue')\n",
    "axes[1].set_title('Total Sales by Region')\n",
    "axes[1].set_ylabel('Sales ($)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Key Insights\n",
    "print(\"\\n5. KEY INSIGHTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"  \n",
    "  1. Electronics dominates sales, contributing the highest revenue\n",
    "     among all categories.\n",
    "  \n",
    "  2. The Central and East regions show the strongest performance,\n",
    "     suggesting potential focus areas for marketing efforts.\n",
    "  \n",
    "  3. Data quality is generally good with minimal missing values\n",
    "     (mainly in customer ratings and sales rep assignments).\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Module Summary\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **EDA is systematic**: Follow a structured approach from data quality to insights\n",
    "2. **Start simple**: Begin with univariate analysis before exploring relationships\n",
    "3. **Visualize everything**: Charts reveal patterns that statistics might miss\n",
    "4. **Document findings**: Keep track of insights for stakeholders and next steps\n",
    "5. **Iterate**: EDA is not linear - discoveries lead to new questions\n",
    "\n",
    "## EDA Checklist\n",
    "\n",
    "```\n",
    "[ ] Data overview (shape, types, head/tail)\n",
    "[ ] Data quality (missing values, duplicates)\n",
    "[ ] Univariate analysis (distributions, statistics)\n",
    "[ ] Bivariate analysis (correlations, comparisons)\n",
    "[ ] Multivariate analysis (patterns, clusters)\n",
    "[ ] Outlier detection\n",
    "[ ] Time-based patterns (if applicable)\n",
    "[ ] Summary of key findings\n",
    "```\n",
    "\n",
    "## Next Module\n",
    "\n",
    "In the next module, we'll cover **Data Cleaning and Preprocessing** - the essential steps to prepare your data for analysis and modeling based on EDA findings.\n",
    "\n",
    "## Additional Practice\n",
    "\n",
    "For extra practice, try these challenges:\n",
    "\n",
    "1. **Comprehensive EDA Report**: Create a complete EDA notebook for the sales data that could be shared with stakeholders, including executive summary and recommendations.\n",
    "\n",
    "2. **Automated EDA Function**: Create a function that takes any DataFrame and generates a complete EDA report with visualizations.\n",
    "\n",
    "3. **Comparative Analysis**: Compare two time periods in the sales data and identify significant changes in patterns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
