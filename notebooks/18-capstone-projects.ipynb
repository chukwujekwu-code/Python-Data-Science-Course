{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 13: Capstone Projects\n",
    "\n",
    "## Projects Covered\n",
    "1. Project 1: Sales Data Analysis\n",
    "2. Project 2: Customer Segmentation\n",
    "3. Project 3: Predictive Modeling\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will be able to:\n",
    "- Apply the complete data analysis workflow to real-world problems\n",
    "- Perform end-to-end exploratory data analysis\n",
    "- Implement customer segmentation using clustering techniques\n",
    "- Build and evaluate predictive models for business problems\n",
    "- Communicate insights effectively through visualizations\n",
    "- Create a portfolio-ready data science project\n",
    "\n",
    "---\n",
    "\n",
    "## About the Capstone Projects\n",
    "\n",
    "These three projects are designed to integrate all the skills you've learned throughout this course:\n",
    "\n",
    "- **Python fundamentals** (Modules 1-5)\n",
    "- **Data manipulation with NumPy and Pandas** (Modules 6-7)\n",
    "- **Data visualization** (Module 8)\n",
    "- **Exploratory Data Analysis** (Module 9)\n",
    "- **Data cleaning and preprocessing** (Module 10)\n",
    "- **Statistics** (Module 11)\n",
    "- **Machine Learning** (Module 12)\n",
    "\n",
    "Each project follows a structured approach and can be added to your data science portfolio.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries we'll use across the capstone projects\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (mean_squared_error, mean_absolute_error, r2_score,\n",
    "                             accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, confusion_matrix, classification_report,\n",
    "                             silhouette_score)\n",
    "\n",
    "# Configuration\n",
    "np.random.seed(42)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Project 1: Sales Data Analysis\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "**Scenario:** You are a data analyst at a retail company. The management wants to understand sales performance across different dimensions and identify opportunities for growth.\n",
    "\n",
    "**Objectives:**\n",
    "1. Analyze overall sales trends over time\n",
    "2. Identify top-performing products and categories\n",
    "3. Analyze regional sales performance\n",
    "4. Discover seasonal patterns\n",
    "5. Provide actionable recommendations\n",
    "\n",
    "**Skills Applied:**\n",
    "- Data loading and cleaning (Pandas)\n",
    "- Exploratory Data Analysis\n",
    "- Data visualization (Matplotlib, Seaborn)\n",
    "- Statistical analysis\n",
    "- Business insights generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic sales data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "n_transactions = 5000\n",
    "start_date = '2022-01-01'\n",
    "end_date = '2023-12-31'\n",
    "\n",
    "# Generate dates\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "dates = np.random.choice(date_range, n_transactions)\n",
    "\n",
    "# Product categories and products\n",
    "categories = {\n",
    "    'Electronics': ['Laptop', 'Smartphone', 'Tablet', 'Headphones', 'Smart Watch'],\n",
    "    'Clothing': ['T-Shirt', 'Jeans', 'Jacket', 'Dress', 'Sneakers'],\n",
    "    'Home & Garden': ['Furniture', 'Kitchenware', 'Bedding', 'Decor', 'Tools'],\n",
    "    'Books': ['Fiction', 'Non-Fiction', 'Educational', 'Comics', 'Magazines']\n",
    "}\n",
    "\n",
    "# Base prices per category\n",
    "category_base_prices = {\n",
    "    'Electronics': (100, 1500),\n",
    "    'Clothing': (20, 200),\n",
    "    'Home & Garden': (30, 500),\n",
    "    'Books': (10, 50)\n",
    "}\n",
    "\n",
    "# Regions\n",
    "regions = ['North', 'South', 'East', 'West', 'Central']\n",
    "region_weights = [0.25, 0.20, 0.20, 0.20, 0.15]\n",
    "\n",
    "# Generate transactions\n",
    "sales_records = []\n",
    "\n",
    "for i in range(n_transactions):\n",
    "    category = np.random.choice(list(categories.keys()))\n",
    "    product = np.random.choice(categories[category])\n",
    "    region = np.random.choice(regions, p=region_weights)\n",
    "    \n",
    "    # Price with seasonal variation\n",
    "    base_low, base_high = category_base_prices[category]\n",
    "    base_price = np.random.uniform(base_low, base_high)\n",
    "    \n",
    "    # Add seasonal factor\n",
    "    month = pd.Timestamp(dates[i]).month\n",
    "    if month in [11, 12]:  # Holiday season\n",
    "        seasonal_factor = 1.3\n",
    "    elif month in [6, 7, 8]:  # Summer\n",
    "        seasonal_factor = 1.1\n",
    "    else:\n",
    "        seasonal_factor = 1.0\n",
    "    \n",
    "    price = base_price * seasonal_factor\n",
    "    quantity = np.random.randint(1, 5)\n",
    "    \n",
    "    # Discount (occasionally)\n",
    "    discount = np.random.choice([0, 0.05, 0.10, 0.15, 0.20], p=[0.6, 0.15, 0.12, 0.08, 0.05])\n",
    "    \n",
    "    total = price * quantity * (1 - discount)\n",
    "    \n",
    "    sales_records.append({\n",
    "        'transaction_id': f'TXN{i+1:05d}',\n",
    "        'date': dates[i],\n",
    "        'category': category,\n",
    "        'product': product,\n",
    "        'region': region,\n",
    "        'unit_price': round(price, 2),\n",
    "        'quantity': quantity,\n",
    "        'discount': discount,\n",
    "        'total_amount': round(total, 2)\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "sales_df = pd.DataFrame(sales_records)\n",
    "sales_df['date'] = pd.to_datetime(sales_df['date'])\n",
    "sales_df = sales_df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "print(\"Sales Dataset Created\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Shape: {sales_df.shape}\")\n",
    "print(f\"Date range: {sales_df['date'].min()} to {sales_df['date'].max()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(sales_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Understanding\n",
    "print(\"STEP 1: Data Understanding\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "print(sales_df.info())\n",
    "\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(sales_df.describe())\n",
    "\n",
    "print(\"\\nCategorical Variables:\")\n",
    "for col in ['category', 'product', 'region']:\n",
    "    print(f\"\\n{col}: {sales_df[col].nunique()} unique values\")\n",
    "    print(sales_df[col].value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Sales Trends Over Time\n",
    "print(\"STEP 2: Sales Trends Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Add time-based columns\n",
    "sales_df['year'] = sales_df['date'].dt.year\n",
    "sales_df['month'] = sales_df['date'].dt.month\n",
    "sales_df['quarter'] = sales_df['date'].dt.quarter\n",
    "sales_df['day_of_week'] = sales_df['date'].dt.day_name()\n",
    "sales_df['year_month'] = sales_df['date'].dt.to_period('M')\n",
    "\n",
    "# Monthly sales trend\n",
    "monthly_sales = sales_df.groupby('year_month').agg({\n",
    "    'total_amount': 'sum',\n",
    "    'transaction_id': 'count',\n",
    "    'quantity': 'sum'\n",
    "}).rename(columns={'transaction_id': 'num_transactions'})\n",
    "\n",
    "monthly_sales['avg_transaction_value'] = monthly_sales['total_amount'] / monthly_sales['num_transactions']\n",
    "\n",
    "print(\"Monthly Sales Summary (last 6 months):\")\n",
    "print(monthly_sales.tail(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sales trends\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Monthly revenue trend\n",
    "ax1 = axes[0, 0]\n",
    "monthly_sales['total_amount'].plot(ax=ax1, marker='o', linewidth=2)\n",
    "ax1.set_xlabel('Month')\n",
    "ax1.set_ylabel('Total Revenue ($)')\n",
    "ax1.set_title('Monthly Revenue Trend')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Quarterly comparison\n",
    "ax2 = axes[0, 1]\n",
    "quarterly_sales = sales_df.groupby(['year', 'quarter'])['total_amount'].sum().unstack(level=0)\n",
    "quarterly_sales.plot(kind='bar', ax=ax2)\n",
    "ax2.set_xlabel('Quarter')\n",
    "ax2.set_ylabel('Total Revenue ($)')\n",
    "ax2.set_title('Quarterly Sales by Year')\n",
    "ax2.legend(title='Year')\n",
    "\n",
    "# Sales by day of week\n",
    "ax3 = axes[1, 0]\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "daily_sales = sales_df.groupby('day_of_week')['total_amount'].sum().reindex(day_order)\n",
    "daily_sales.plot(kind='bar', ax=ax3, color='steelblue')\n",
    "ax3.set_xlabel('Day of Week')\n",
    "ax3.set_ylabel('Total Revenue ($)')\n",
    "ax3.set_title('Sales by Day of Week')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Monthly pattern (seasonality)\n",
    "ax4 = axes[1, 1]\n",
    "monthly_pattern = sales_df.groupby('month')['total_amount'].sum()\n",
    "monthly_pattern.plot(kind='bar', ax=ax4, color='coral')\n",
    "ax4.set_xlabel('Month')\n",
    "ax4.set_ylabel('Total Revenue ($)')\n",
    "ax4.set_title('Sales by Month (Seasonality)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Product and Category Analysis\n",
    "print(\"STEP 3: Product and Category Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Category performance\n",
    "category_performance = sales_df.groupby('category').agg({\n",
    "    'total_amount': 'sum',\n",
    "    'transaction_id': 'count',\n",
    "    'quantity': 'sum'\n",
    "}).rename(columns={'transaction_id': 'transactions'})\n",
    "\n",
    "category_performance['avg_transaction'] = category_performance['total_amount'] / category_performance['transactions']\n",
    "category_performance['revenue_share'] = category_performance['total_amount'] / category_performance['total_amount'].sum() * 100\n",
    "category_performance = category_performance.sort_values('total_amount', ascending=False)\n",
    "\n",
    "print(\"\\nCategory Performance:\")\n",
    "print(category_performance.round(2))\n",
    "\n",
    "# Top products by revenue\n",
    "top_products = sales_df.groupby(['category', 'product'])['total_amount'].sum().sort_values(ascending=False).head(10)\n",
    "print(\"\\nTop 10 Products by Revenue:\")\n",
    "print(top_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize product performance\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Category revenue pie chart\n",
    "ax1 = axes[0]\n",
    "category_performance['total_amount'].plot(kind='pie', ax=ax1, autopct='%1.1f%%')\n",
    "ax1.set_ylabel('')\n",
    "ax1.set_title('Revenue by Category')\n",
    "\n",
    "# Top products bar chart\n",
    "ax2 = axes[1]\n",
    "top_products.plot(kind='barh', ax=ax2)\n",
    "ax2.set_xlabel('Total Revenue ($)')\n",
    "ax2.set_title('Top 10 Products by Revenue')\n",
    "\n",
    "# Category trends over time\n",
    "ax3 = axes[2]\n",
    "category_monthly = sales_df.groupby(['year_month', 'category'])['total_amount'].sum().unstack()\n",
    "category_monthly.plot(ax=ax3, marker='o')\n",
    "ax3.set_xlabel('Month')\n",
    "ax3.set_ylabel('Revenue ($)')\n",
    "ax3.set_title('Category Revenue Trends')\n",
    "ax3.legend(title='Category', bbox_to_anchor=(1.05, 1))\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Regional Analysis\n",
    "print(\"STEP 4: Regional Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Regional performance\n",
    "regional_performance = sales_df.groupby('region').agg({\n",
    "    'total_amount': 'sum',\n",
    "    'transaction_id': 'count',\n",
    "    'quantity': 'sum'\n",
    "}).rename(columns={'transaction_id': 'transactions'})\n",
    "\n",
    "regional_performance['avg_transaction'] = regional_performance['total_amount'] / regional_performance['transactions']\n",
    "regional_performance['market_share'] = regional_performance['total_amount'] / regional_performance['total_amount'].sum() * 100\n",
    "regional_performance = regional_performance.sort_values('total_amount', ascending=False)\n",
    "\n",
    "print(\"Regional Performance:\")\n",
    "print(regional_performance.round(2))\n",
    "\n",
    "# Category preferences by region\n",
    "region_category = sales_df.pivot_table(\n",
    "    values='total_amount',\n",
    "    index='region',\n",
    "    columns='category',\n",
    "    aggfunc='sum'\n",
    ")\n",
    "\n",
    "print(\"\\nCategory Preferences by Region (Revenue):\")\n",
    "print(region_category.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regional analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Regional revenue\n",
    "ax1 = axes[0]\n",
    "regional_performance['total_amount'].plot(kind='bar', ax=ax1, color='teal')\n",
    "ax1.set_xlabel('Region')\n",
    "ax1.set_ylabel('Total Revenue ($)')\n",
    "ax1.set_title('Revenue by Region')\n",
    "ax1.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Heatmap of region-category performance\n",
    "ax2 = axes[1]\n",
    "sns.heatmap(region_category, annot=True, fmt=',.0f', cmap='YlGnBu', ax=ax2)\n",
    "ax2.set_title('Revenue Heatmap: Region vs Category')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Key Insights and Recommendations\n",
    "print(\"STEP 5: Key Insights and Recommendations\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate key metrics\n",
    "total_revenue = sales_df['total_amount'].sum()\n",
    "total_transactions = len(sales_df)\n",
    "avg_transaction = total_revenue / total_transactions\n",
    "best_month = monthly_pattern.idxmax()\n",
    "worst_month = monthly_pattern.idxmin()\n",
    "top_category = category_performance['total_amount'].idxmax()\n",
    "top_region = regional_performance['total_amount'].idxmax()\n",
    "\n",
    "print(\"\\nKEY METRICS:\")\n",
    "print(f\"  Total Revenue: ${total_revenue:,.2f}\")\n",
    "print(f\"  Total Transactions: {total_transactions:,}\")\n",
    "print(f\"  Average Transaction Value: ${avg_transaction:.2f}\")\n",
    "\n",
    "print(\"\\nINSIGHTS:\")\n",
    "print(f\"  1. Best performing month: Month {best_month} (${monthly_pattern[best_month]:,.2f})\")\n",
    "print(f\"  2. Slowest month: Month {worst_month} (${monthly_pattern[worst_month]:,.2f})\")\n",
    "print(f\"  3. Top category: {top_category} ({category_performance.loc[top_category, 'revenue_share']:.1f}% of revenue)\")\n",
    "print(f\"  4. Top region: {top_region} ({regional_performance.loc[top_region, 'market_share']:.1f}% of revenue)\")\n",
    "\n",
    "# Year-over-year growth\n",
    "yearly_sales = sales_df.groupby('year')['total_amount'].sum()\n",
    "if len(yearly_sales) > 1:\n",
    "    yoy_growth = (yearly_sales.iloc[-1] - yearly_sales.iloc[-2]) / yearly_sales.iloc[-2] * 100\n",
    "    print(f\"  5. Year-over-Year Growth: {yoy_growth:.1f}%\")\n",
    "\n",
    "print(\"\\nRECOMMENDATIONS:\")\n",
    "print(\"  1. Increase marketing spend in Q4 to capitalize on holiday season\")\n",
    "print(f\"  2. Focus expansion efforts on {top_category} category\")\n",
    "print(f\"  3. Investigate underperformance in {regional_performance['total_amount'].idxmin()} region\")\n",
    "print(\"  4. Consider promotional campaigns during slower months\")\n",
    "print(\"  5. Analyze high-discount transactions to optimize pricing strategy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Project 2: Customer Segmentation\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "**Scenario:** You are a data scientist at an e-commerce company. The marketing team wants to segment customers to create targeted marketing campaigns.\n",
    "\n",
    "**Objectives:**\n",
    "1. Analyze customer behavior patterns\n",
    "2. Create meaningful customer segments using clustering\n",
    "3. Profile each segment with descriptive characteristics\n",
    "4. Provide marketing recommendations for each segment\n",
    "\n",
    "**Skills Applied:**\n",
    "- Feature engineering\n",
    "- Data preprocessing and scaling\n",
    "- K-Means clustering\n",
    "- Cluster analysis and interpretation\n",
    "- Business recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic customer data\n",
    "np.random.seed(42)\n",
    "\n",
    "n_customers = 1000\n",
    "\n",
    "# Customer base data\n",
    "customer_data = pd.DataFrame({\n",
    "    'customer_id': [f'CUST{i:04d}' for i in range(1, n_customers + 1)],\n",
    "    'age': np.random.randint(18, 70, n_customers),\n",
    "    'gender': np.random.choice(['M', 'F'], n_customers),\n",
    "    'tenure_months': np.random.randint(1, 60, n_customers),\n",
    "})\n",
    "\n",
    "# Generate correlated behavioral features\n",
    "# Create customer archetypes\n",
    "archetypes = np.random.choice(['budget', 'regular', 'premium', 'vip'], n_customers, \n",
    "                               p=[0.30, 0.40, 0.20, 0.10])\n",
    "\n",
    "# Define archetype characteristics\n",
    "archetype_params = {\n",
    "    'budget': {'orders': (1, 5), 'avg_spend': (20, 50), 'frequency': (1, 3)},\n",
    "    'regular': {'orders': (5, 15), 'avg_spend': (50, 100), 'frequency': (3, 6)},\n",
    "    'premium': {'orders': (10, 30), 'avg_spend': (100, 300), 'frequency': (4, 8)},\n",
    "    'vip': {'orders': (20, 50), 'avg_spend': (200, 500), 'frequency': (6, 12)}\n",
    "}\n",
    "\n",
    "# Generate features based on archetypes\n",
    "total_orders = []\n",
    "avg_order_value = []\n",
    "purchase_frequency = []\n",
    "days_since_last_purchase = []\n",
    "\n",
    "for archetype in archetypes:\n",
    "    params = archetype_params[archetype]\n",
    "    total_orders.append(np.random.randint(params['orders'][0], params['orders'][1]))\n",
    "    avg_order_value.append(np.random.uniform(params['avg_spend'][0], params['avg_spend'][1]))\n",
    "    purchase_frequency.append(np.random.uniform(params['frequency'][0], params['frequency'][1]))\n",
    "    # Days since last purchase inversely related to frequency\n",
    "    days_since_last_purchase.append(np.random.randint(1, 180 // params['frequency'][0]))\n",
    "\n",
    "customer_data['total_orders'] = total_orders\n",
    "customer_data['avg_order_value'] = np.round(avg_order_value, 2)\n",
    "customer_data['purchase_frequency_monthly'] = np.round(purchase_frequency, 2)\n",
    "customer_data['days_since_last_purchase'] = days_since_last_purchase\n",
    "customer_data['total_spend'] = np.round(customer_data['total_orders'] * customer_data['avg_order_value'], 2)\n",
    "\n",
    "# Add some noise and variation\n",
    "customer_data['website_visits_monthly'] = np.round(\n",
    "    customer_data['purchase_frequency_monthly'] * np.random.uniform(2, 5, n_customers), 1\n",
    ")\n",
    "customer_data['email_open_rate'] = np.round(\n",
    "    np.random.beta(2, 5, n_customers) * 0.8 + 0.1, 2\n",
    ")\n",
    "customer_data['support_tickets'] = np.random.poisson(1, n_customers)\n",
    "\n",
    "print(\"Customer Dataset Created\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Shape: {customer_data.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(customer_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Exploratory Data Analysis\n",
    "print(\"STEP 1: Customer Data EDA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nDataset Summary:\")\n",
    "print(customer_data.describe())\n",
    "\n",
    "# Key metrics\n",
    "print(\"\\nKey Customer Metrics:\")\n",
    "print(f\"  Total Customers: {len(customer_data):,}\")\n",
    "print(f\"  Total Revenue: ${customer_data['total_spend'].sum():,.2f}\")\n",
    "print(f\"  Average Customer Value: ${customer_data['total_spend'].mean():.2f}\")\n",
    "print(f\"  Average Order Value: ${customer_data['avg_order_value'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize customer distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# Total spend distribution\n",
    "axes[0, 0].hist(customer_data['total_spend'], bins=30, edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Total Spend ($)')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].set_title('Distribution of Total Spend')\n",
    "\n",
    "# Order frequency\n",
    "axes[0, 1].hist(customer_data['purchase_frequency_monthly'], bins=20, edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Purchases per Month')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].set_title('Purchase Frequency Distribution')\n",
    "\n",
    "# Avg order value\n",
    "axes[0, 2].hist(customer_data['avg_order_value'], bins=30, edgecolor='black')\n",
    "axes[0, 2].set_xlabel('Average Order Value ($)')\n",
    "axes[0, 2].set_ylabel('Count')\n",
    "axes[0, 2].set_title('Average Order Value Distribution')\n",
    "\n",
    "# Recency (days since last purchase)\n",
    "axes[1, 0].hist(customer_data['days_since_last_purchase'], bins=30, edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Days Since Last Purchase')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].set_title('Recency Distribution')\n",
    "\n",
    "# Tenure\n",
    "axes[1, 1].hist(customer_data['tenure_months'], bins=20, edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Tenure (Months)')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].set_title('Customer Tenure Distribution')\n",
    "\n",
    "# Total spend vs Frequency scatter\n",
    "axes[1, 2].scatter(customer_data['purchase_frequency_monthly'], \n",
    "                   customer_data['total_spend'], alpha=0.5)\n",
    "axes[1, 2].set_xlabel('Purchase Frequency (Monthly)')\n",
    "axes[1, 2].set_ylabel('Total Spend ($)')\n",
    "axes[1, 2].set_title('Frequency vs Total Spend')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Feature Engineering for Segmentation\n",
    "print(\"STEP 2: Feature Engineering\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# RFM-style features\n",
    "segmentation_features = customer_data[[\n",
    "    'total_spend',              # Monetary\n",
    "    'purchase_frequency_monthly',  # Frequency\n",
    "    'days_since_last_purchase',    # Recency\n",
    "    'avg_order_value',\n",
    "    'total_orders',\n",
    "    'website_visits_monthly'\n",
    "]].copy()\n",
    "\n",
    "print(\"Features for Segmentation:\")\n",
    "print(segmentation_features.describe())\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(segmentation_features)\n",
    "\n",
    "print(f\"\\nScaled features shape: {features_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Find Optimal Number of Clusters\n",
    "print(\"STEP 3: Determine Optimal Clusters\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Elbow method and silhouette score\n",
    "k_range = range(2, 11)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(features_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(features_scaled, kmeans.labels_))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Elbow method\n",
    "axes[0].plot(k_range, inertias, 'bo-', linewidth=2)\n",
    "axes[0].set_xlabel('Number of Clusters (k)')\n",
    "axes[0].set_ylabel('Inertia')\n",
    "axes[0].set_title('Elbow Method')\n",
    "\n",
    "# Silhouette score\n",
    "axes[1].plot(k_range, silhouette_scores, 'go-', linewidth=2)\n",
    "axes[1].set_xlabel('Number of Clusters (k)')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].set_title('Silhouette Score by Number of Clusters')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print silhouette scores\n",
    "print(\"\\nSilhouette Scores:\")\n",
    "for k, score in zip(k_range, silhouette_scores):\n",
    "    print(f\"  k={k}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Apply K-Means Clustering\n",
    "print(\"STEP 4: Customer Segmentation\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Choose k=4 based on elbow and silhouette\n",
    "optimal_k = 4\n",
    "\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "customer_data['segment'] = kmeans_final.fit_predict(features_scaled)\n",
    "\n",
    "print(f\"\\nNumber of clusters: {optimal_k}\")\n",
    "print(f\"Silhouette Score: {silhouette_score(features_scaled, customer_data['segment']):.4f}\")\n",
    "\n",
    "# Segment distribution\n",
    "print(\"\\nSegment Distribution:\")\n",
    "print(customer_data['segment'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Segment Profiling\n",
    "print(\"STEP 5: Segment Profiling\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate segment statistics\n",
    "segment_profile = customer_data.groupby('segment').agg({\n",
    "    'customer_id': 'count',\n",
    "    'total_spend': 'mean',\n",
    "    'avg_order_value': 'mean',\n",
    "    'purchase_frequency_monthly': 'mean',\n",
    "    'days_since_last_purchase': 'mean',\n",
    "    'total_orders': 'mean',\n",
    "    'website_visits_monthly': 'mean',\n",
    "    'email_open_rate': 'mean',\n",
    "    'tenure_months': 'mean'\n",
    "}).rename(columns={'customer_id': 'count'})\n",
    "\n",
    "# Add percentage\n",
    "segment_profile['pct_customers'] = segment_profile['count'] / segment_profile['count'].sum() * 100\n",
    "\n",
    "print(\"\\nSegment Profiles:\")\n",
    "print(segment_profile.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name the segments based on characteristics\n",
    "segment_names = {\n",
    "    segment_profile['total_spend'].idxmax(): 'VIP Champions',\n",
    "    segment_profile['total_spend'].idxmin(): 'Budget Shoppers',\n",
    "}\n",
    "\n",
    "# Assign remaining segments\n",
    "remaining = [s for s in range(optimal_k) if s not in segment_names]\n",
    "remaining_sorted = segment_profile.loc[remaining, 'total_spend'].sort_values(ascending=False)\n",
    "\n",
    "if len(remaining) >= 1:\n",
    "    segment_names[remaining_sorted.index[0]] = 'Loyal Regulars'\n",
    "if len(remaining) >= 2:\n",
    "    segment_names[remaining_sorted.index[1]] = 'Occasional Buyers'\n",
    "\n",
    "customer_data['segment_name'] = customer_data['segment'].map(segment_names)\n",
    "\n",
    "print(\"Segment Names:\")\n",
    "for seg, name in sorted(segment_names.items()):\n",
    "    count = (customer_data['segment'] == seg).sum()\n",
    "    spend = segment_profile.loc[seg, 'total_spend']\n",
    "    print(f\"  Segment {seg}: {name} ({count} customers, avg spend: ${spend:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize segments\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Segment sizes\n",
    "ax1 = axes[0, 0]\n",
    "segment_counts = customer_data['segment_name'].value_counts()\n",
    "segment_counts.plot(kind='pie', ax=ax1, autopct='%1.1f%%')\n",
    "ax1.set_ylabel('')\n",
    "ax1.set_title('Customer Segments Distribution')\n",
    "\n",
    "# Spend by segment (box plot)\n",
    "ax2 = axes[0, 1]\n",
    "customer_data.boxplot(column='total_spend', by='segment_name', ax=ax2)\n",
    "ax2.set_xlabel('Segment')\n",
    "ax2.set_ylabel('Total Spend ($)')\n",
    "ax2.set_title('Total Spend by Segment')\n",
    "plt.suptitle('')\n",
    "\n",
    "# Frequency vs Spend scatter by segment\n",
    "ax3 = axes[1, 0]\n",
    "for name in segment_names.values():\n",
    "    subset = customer_data[customer_data['segment_name'] == name]\n",
    "    ax3.scatter(subset['purchase_frequency_monthly'], subset['total_spend'], \n",
    "                label=name, alpha=0.6)\n",
    "ax3.set_xlabel('Purchase Frequency (Monthly)')\n",
    "ax3.set_ylabel('Total Spend ($)')\n",
    "ax3.set_title('Segment Distribution: Frequency vs Spend')\n",
    "ax3.legend()\n",
    "\n",
    "# Radar chart data preparation\n",
    "ax4 = axes[1, 1]\n",
    "metrics = ['total_spend', 'avg_order_value', 'purchase_frequency_monthly', 'website_visits_monthly']\n",
    "segment_means = customer_data.groupby('segment_name')[metrics].mean()\n",
    "\n",
    "# Normalize for comparison\n",
    "segment_means_normalized = (segment_means - segment_means.min()) / (segment_means.max() - segment_means.min())\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.2\n",
    "\n",
    "for i, segment in enumerate(segment_means_normalized.index):\n",
    "    ax4.bar(x + i*width, segment_means_normalized.loc[segment], width, label=segment)\n",
    "\n",
    "ax4.set_xticks(x + width * 1.5)\n",
    "ax4.set_xticklabels(['Total Spend', 'Avg Order', 'Frequency', 'Web Visits'])\n",
    "ax4.set_ylabel('Normalized Score')\n",
    "ax4.set_title('Segment Comparison (Normalized)')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Marketing Recommendations\n",
    "print(\"STEP 6: Marketing Recommendations\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "recommendations = {\n",
    "    'VIP Champions': {\n",
    "        'description': 'High-value, frequent buyers with strong engagement',\n",
    "        'strategy': [\n",
    "            'Exclusive early access to new products',\n",
    "            'Personalized VIP loyalty rewards',\n",
    "            'Dedicated customer success manager',\n",
    "            'Premium shipping and returns',\n",
    "            'Invite-only events and experiences'\n",
    "        ]\n",
    "    },\n",
    "    'Loyal Regulars': {\n",
    "        'description': 'Consistent buyers with moderate spend',\n",
    "        'strategy': [\n",
    "            'Upsell and cross-sell campaigns',\n",
    "            'Tiered loyalty program to encourage upgrades',\n",
    "            'Product bundles and volume discounts',\n",
    "            'Referral bonuses',\n",
    "            'Birthday/anniversary special offers'\n",
    "        ]\n",
    "    },\n",
    "    'Occasional Buyers': {\n",
    "        'description': 'Infrequent purchasers with growth potential',\n",
    "        'strategy': [\n",
    "            'Win-back email campaigns',\n",
    "            'Time-limited discount offers',\n",
    "            'Product recommendations based on past purchases',\n",
    "            'Re-engagement through social media',\n",
    "            'Simplified checkout experience'\n",
    "        ]\n",
    "    },\n",
    "    'Budget Shoppers': {\n",
    "        'description': 'Price-sensitive customers seeking deals',\n",
    "        'strategy': [\n",
    "            'Flash sales and clearance notifications',\n",
    "            'Price drop alerts on wishlist items',\n",
    "            'Budget-friendly product recommendations',\n",
    "            'Free shipping thresholds to increase basket size',\n",
    "            'Loyalty points for every purchase'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "for segment, info in recommendations.items():\n",
    "    count = (customer_data['segment_name'] == segment).sum()\n",
    "    revenue_share = customer_data[customer_data['segment_name'] == segment]['total_spend'].sum() / customer_data['total_spend'].sum() * 100\n",
    "    \n",
    "    print(f\"\\n{segment.upper()}\")\n",
    "    print(f\"  Customers: {count} ({count/len(customer_data)*100:.1f}%)\")\n",
    "    print(f\"  Revenue Share: {revenue_share:.1f}%\")\n",
    "    print(f\"  Profile: {info['description']}\")\n",
    "    print(f\"  Recommended Strategies:\")\n",
    "    for strategy in info['strategy']:\n",
    "        print(f\"    - {strategy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Project 3: Predictive Modeling\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "**Scenario:** You are a data scientist at a subscription-based company. The business wants to predict which customers are likely to churn so they can take proactive retention actions.\n",
    "\n",
    "**Objectives:**\n",
    "1. Build a predictive model for customer churn\n",
    "2. Identify key factors driving churn\n",
    "3. Evaluate model performance\n",
    "4. Provide actionable insights for retention\n",
    "\n",
    "**Skills Applied:**\n",
    "- Complete ML workflow\n",
    "- Feature engineering\n",
    "- Model training and evaluation\n",
    "- Cross-validation\n",
    "- Feature importance analysis\n",
    "- Business recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic churn dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "n_customers = 2000\n",
    "\n",
    "# Customer features\n",
    "churn_data = pd.DataFrame({\n",
    "    'customer_id': [f'SUB{i:05d}' for i in range(1, n_customers + 1)],\n",
    "    'tenure_months': np.random.randint(1, 72, n_customers),\n",
    "    'monthly_charges': np.random.uniform(20, 100, n_customers),\n",
    "    'contract_type': np.random.choice(['Month-to-Month', 'One Year', 'Two Year'], \n",
    "                                       n_customers, p=[0.5, 0.3, 0.2]),\n",
    "    'payment_method': np.random.choice(['Credit Card', 'Bank Transfer', 'Electronic Check'], \n",
    "                                        n_customers, p=[0.4, 0.35, 0.25]),\n",
    "    'num_support_tickets': np.random.poisson(2, n_customers),\n",
    "    'num_services': np.random.randint(1, 6, n_customers),\n",
    "    'has_premium_support': np.random.choice([0, 1], n_customers, p=[0.7, 0.3]),\n",
    "    'account_age_days': np.random.randint(30, 2000, n_customers)\n",
    "})\n",
    "\n",
    "# Calculate total charges\n",
    "churn_data['total_charges'] = churn_data['tenure_months'] * churn_data['monthly_charges']\n",
    "\n",
    "# Create churn based on features (simulating real patterns)\n",
    "churn_prob = (\n",
    "    0.35  # Base probability\n",
    "    - 0.01 * churn_data['tenure_months']  # Longer tenure = less likely to churn\n",
    "    + 0.002 * churn_data['monthly_charges']  # Higher charges = more likely to churn\n",
    "    + 0.05 * (churn_data['contract_type'] == 'Month-to-Month').astype(int)  # Month-to-month = more churn\n",
    "    + 0.03 * churn_data['num_support_tickets']  # More tickets = more churn\n",
    "    - 0.03 * churn_data['num_services']  # More services = less churn\n",
    "    - 0.1 * churn_data['has_premium_support']  # Premium support = less churn\n",
    "    + 0.04 * (churn_data['payment_method'] == 'Electronic Check').astype(int)  # E-check = more churn\n",
    ").clip(0.05, 0.8)\n",
    "\n",
    "churn_data['churned'] = np.random.binomial(1, churn_prob)\n",
    "\n",
    "print(\"Churn Dataset Created\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Shape: {churn_data.shape}\")\n",
    "print(f\"Churn Rate: {churn_data['churned'].mean():.2%}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(churn_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Exploratory Data Analysis\n",
    "print(\"STEP 1: Churn Data EDA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "print(churn_data.info())\n",
    "\n",
    "print(\"\\nNumerical Summary:\")\n",
    "print(churn_data.describe())\n",
    "\n",
    "print(\"\\nCategorical Summary:\")\n",
    "for col in ['contract_type', 'payment_method']:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(churn_data[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze churn by different factors\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# Churn by contract type\n",
    "ax1 = axes[0, 0]\n",
    "churn_by_contract = churn_data.groupby('contract_type')['churned'].mean().sort_values(ascending=False)\n",
    "churn_by_contract.plot(kind='bar', ax=ax1, color='coral')\n",
    "ax1.set_ylabel('Churn Rate')\n",
    "ax1.set_title('Churn Rate by Contract Type')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Churn by payment method\n",
    "ax2 = axes[0, 1]\n",
    "churn_by_payment = churn_data.groupby('payment_method')['churned'].mean().sort_values(ascending=False)\n",
    "churn_by_payment.plot(kind='bar', ax=ax2, color='steelblue')\n",
    "ax2.set_ylabel('Churn Rate')\n",
    "ax2.set_title('Churn Rate by Payment Method')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Tenure distribution by churn\n",
    "ax3 = axes[0, 2]\n",
    "churn_data[churn_data['churned']==0]['tenure_months'].hist(ax=ax3, alpha=0.5, label='Stayed', bins=20)\n",
    "churn_data[churn_data['churned']==1]['tenure_months'].hist(ax=ax3, alpha=0.5, label='Churned', bins=20)\n",
    "ax3.set_xlabel('Tenure (Months)')\n",
    "ax3.set_ylabel('Count')\n",
    "ax3.set_title('Tenure Distribution by Churn Status')\n",
    "ax3.legend()\n",
    "\n",
    "# Monthly charges by churn\n",
    "ax4 = axes[1, 0]\n",
    "churn_data.boxplot(column='monthly_charges', by='churned', ax=ax4)\n",
    "ax4.set_xlabel('Churned')\n",
    "ax4.set_ylabel('Monthly Charges ($)')\n",
    "ax4.set_title('Monthly Charges by Churn Status')\n",
    "plt.suptitle('')\n",
    "\n",
    "# Support tickets by churn\n",
    "ax5 = axes[1, 1]\n",
    "churn_by_tickets = churn_data.groupby('num_support_tickets')['churned'].mean()\n",
    "churn_by_tickets.plot(kind='bar', ax=ax5, color='purple')\n",
    "ax5.set_xlabel('Number of Support Tickets')\n",
    "ax5.set_ylabel('Churn Rate')\n",
    "ax5.set_title('Churn Rate by Support Tickets')\n",
    "\n",
    "# Services by churn\n",
    "ax6 = axes[1, 2]\n",
    "churn_by_services = churn_data.groupby('num_services')['churned'].mean()\n",
    "churn_by_services.plot(kind='bar', ax=ax6, color='green')\n",
    "ax6.set_xlabel('Number of Services')\n",
    "ax6.set_ylabel('Churn Rate')\n",
    "ax6.set_title('Churn Rate by Number of Services')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Feature Engineering and Preprocessing\n",
    "print(\"STEP 2: Feature Engineering\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create a copy for modeling\n",
    "model_data = churn_data.copy()\n",
    "\n",
    "# Encode categorical variables\n",
    "model_data['contract_monthly'] = (model_data['contract_type'] == 'Month-to-Month').astype(int)\n",
    "model_data['contract_one_year'] = (model_data['contract_type'] == 'One Year').astype(int)\n",
    "model_data['payment_echeck'] = (model_data['payment_method'] == 'Electronic Check').astype(int)\n",
    "model_data['payment_credit'] = (model_data['payment_method'] == 'Credit Card').astype(int)\n",
    "\n",
    "# Create derived features\n",
    "model_data['avg_monthly_tickets'] = model_data['num_support_tickets'] / (model_data['tenure_months'] + 1)\n",
    "model_data['charge_per_service'] = model_data['monthly_charges'] / model_data['num_services']\n",
    "\n",
    "# Select features for modeling\n",
    "feature_columns = [\n",
    "    'tenure_months', 'monthly_charges', 'total_charges',\n",
    "    'num_support_tickets', 'num_services', 'has_premium_support',\n",
    "    'contract_monthly', 'contract_one_year',\n",
    "    'payment_echeck', 'payment_credit',\n",
    "    'avg_monthly_tickets', 'charge_per_service'\n",
    "]\n",
    "\n",
    "X = model_data[feature_columns]\n",
    "y = model_data['churned']\n",
    "\n",
    "print(f\"Features: {len(feature_columns)}\")\n",
    "print(f\"Samples: {len(X)}\")\n",
    "print(f\"\\nFeature columns:\")\n",
    "for col in feature_columns:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train-Test Split and Scaling\n",
    "print(\"STEP 3: Data Preparation\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")\n",
    "print(f\"\\nChurn rate in training: {y_train.mean():.2%}\")\n",
    "print(f\"Churn rate in testing: {y_test.mean():.2%}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Model Training and Comparison\n",
    "print(\"STEP 4: Model Training\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Use scaled data for logistic regression, unscaled for trees\n",
    "    if name == 'Logistic Regression':\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1-Score': f1_score(y_test, y_pred)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(results_df.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Cross-Validation for Best Model\n",
    "print(\"STEP 5: Cross-Validation\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Random Forest performed best, let's validate with CV\n",
    "best_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "\n",
    "cv_scores = cross_val_score(best_model, X, y, cv=5, scoring='f1')\n",
    "cv_accuracy = cross_val_score(best_model, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "print(f\"\\nRandom Forest - 5-Fold Cross-Validation:\")\n",
    "print(f\"  F1-Score: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n",
    "print(f\"  Accuracy: {cv_accuracy.mean():.4f} (+/- {cv_accuracy.std()*2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Feature Importance Analysis\n",
    "print(\"STEP 6: Feature Importance\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Train final model\n",
    "final_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_columns,\n",
    "    'Importance': final_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance Ranking:\")\n",
    "print(feature_importance.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance for Churn Prediction')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Final Model Evaluation\n",
    "print(\"STEP 7: Final Model Evaluation\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Predictions\n",
    "y_pred_final = final_model.predict(X_test)\n",
    "y_prob_final = final_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_final)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion matrix heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Stayed', 'Churned'],\n",
    "            yticklabels=['Stayed', 'Churned'])\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title('Confusion Matrix')\n",
    "\n",
    "# ROC Curve\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob_final)\n",
    "auc = roc_auc_score(y_test, y_prob_final)\n",
    "\n",
    "axes[1].plot(fpr, tpr, linewidth=2, label=f'Random Forest (AUC = {auc:.3f})')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "axes[1].fill_between(fpr, tpr, alpha=0.3)\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title('ROC Curve')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_final, target_names=['Stayed', 'Churned']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Business Insights and Recommendations\n",
    "print(\"STEP 8: Business Insights and Recommendations\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nMODEL PERFORMANCE SUMMARY:\")\n",
    "print(f\"  - Accuracy: {accuracy_score(y_test, y_pred_final):.2%}\")\n",
    "print(f\"  - Recall (Churn Detection): {recall_score(y_test, y_pred_final):.2%}\")\n",
    "print(f\"  - Precision: {precision_score(y_test, y_pred_final):.2%}\")\n",
    "print(f\"  - AUC-ROC: {auc:.4f}\")\n",
    "\n",
    "print(\"\\nKEY CHURN DRIVERS (Top 5):\")\n",
    "for i, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"  {i+1}. {row['Feature']}: {row['Importance']:.4f}\")\n",
    "\n",
    "print(\"\\nACTIONABLE RECOMMENDATIONS:\")\n",
    "recommendations_list = [\n",
    "    (\"Tenure & Engagement\", \n",
    "     \"Focus retention efforts on customers with < 12 months tenure. \"\n",
    "     \"Implement onboarding programs and early engagement initiatives.\"),\n",
    "    \n",
    "    (\"Contract Type\", \n",
    "     \"Incentivize month-to-month customers to switch to annual contracts. \"\n",
    "     \"Offer discounts for longer commitments.\"),\n",
    "    \n",
    "    (\"Support Experience\", \n",
    "     \"Customers with multiple support tickets are at higher risk. \"\n",
    "     \"Proactively reach out after 2+ tickets to resolve issues.\"),\n",
    "    \n",
    "    (\"Service Bundling\", \n",
    "     \"Customers with more services churn less. \"\n",
    "     \"Create attractive bundles to increase service adoption.\"),\n",
    "    \n",
    "    (\"Payment Method\", \n",
    "     \"Electronic check users have higher churn. \"\n",
    "     \"Encourage credit card or bank transfer with small incentives.\"),\n",
    "    \n",
    "    (\"Premium Support\", \n",
    "     \"Premium support customers churn significantly less. \"\n",
    "     \"Promote premium support to high-risk customers.\")\n",
    "]\n",
    "\n",
    "for i, (area, rec) in enumerate(recommendations_list, 1):\n",
    "    print(f\"\\n  {i}. {area}:\")\n",
    "    print(f\"     {rec}\")\n",
    "\n",
    "print(\"\\nNEXT STEPS:\")\n",
    "print(\"  1. Deploy model to score all active customers\")\n",
    "print(\"  2. Create high-risk customer list for proactive outreach\")\n",
    "print(\"  3. A/B test retention strategies on different risk segments\")\n",
    "print(\"  4. Monitor model performance and retrain quarterly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Module Summary\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You have completed the Python for Data Analysis and Data Science course! Through these 13 modules, you've built a strong foundation in:\n",
    "\n",
    "### Skills Acquired\n",
    "\n",
    "**Python Programming (Modules 1-5)**\n",
    "- Python fundamentals and data structures\n",
    "- Control flow and functions\n",
    "- File handling and data I/O\n",
    "\n",
    "**Data Manipulation (Modules 6-7)**\n",
    "- NumPy for numerical computing\n",
    "- Pandas for data analysis\n",
    "\n",
    "**Data Visualization (Module 8)**\n",
    "- Matplotlib for basic plots\n",
    "- Seaborn for statistical visualizations\n",
    "\n",
    "**Data Analysis (Modules 9-10)**\n",
    "- Exploratory Data Analysis workflow\n",
    "- Data cleaning and preprocessing\n",
    "\n",
    "**Statistics & ML (Modules 11-12)**\n",
    "- Statistical concepts and hypothesis testing\n",
    "- Machine learning fundamentals\n",
    "\n",
    "**Applied Projects (Module 13)**\n",
    "- Sales data analysis\n",
    "- Customer segmentation\n",
    "- Predictive modeling\n",
    "\n",
    "## Portfolio Projects\n",
    "\n",
    "The three capstone projects can be added to your data science portfolio:\n",
    "\n",
    "1. **Sales Data Analysis**: Demonstrates business analytics and visualization skills\n",
    "2. **Customer Segmentation**: Shows unsupervised learning and customer insights\n",
    "3. **Churn Prediction**: Exhibits end-to-end machine learning workflow\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To continue your data science journey:\n",
    "\n",
    "1. **Practice**: Apply these skills to real datasets (Kaggle, UCI ML Repository)\n",
    "2. **Deep Dive**: Explore advanced topics (deep learning, NLP, time series)\n",
    "3. **Tools**: Learn SQL, cloud platforms (AWS, GCP), and deployment\n",
    "4. **Projects**: Build more portfolio projects in your domain of interest\n",
    "5. **Community**: Join data science communities and participate in competitions\n",
    "\n",
    "## Resources for Continued Learning\n",
    "\n",
    "- **Kaggle**: Competitions and datasets\n",
    "- **Towards Data Science**: Articles and tutorials\n",
    "- **Scikit-learn Documentation**: ML algorithms reference\n",
    "- **Stack Overflow**: Problem-solving community\n",
    "\n",
    "---\n",
    "\n",
    "Thank you for completing this course. Best of luck on your data science journey!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
