{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Module 17: Linear Regression\n\n## Topics Covered\n1. Simple Linear Regression\n2. Multiple Linear Regression\n3. Assumptions of Linear Regression\n4. Ordinary Least Squares (OLS)\n5. Gradient Descent for Regression\n6. Polynomial Regression\n7. Regularization (Ridge, Lasso, Elastic Net)\n8. Regression Evaluation Metrics\n9. Residual Analysis\n10. Feature Scaling and Transformation\n\n## Learning Objectives\n\nBy the end of this module, you will be able to:\n- Build and interpret simple and multiple linear regression models\n- Understand the mathematical foundations of OLS and gradient descent\n- Apply polynomial regression for non-linear relationships\n- Use regularization techniques to prevent overfitting\n- Evaluate regression models using appropriate metrics\n- Diagnose model issues through residual analysis\n- Prepare features for optimal model performance\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# Section 1: Simple Linear Regression\n---\n\n## What is Simple Linear Regression?\n\n**Simple linear regression** models the relationship between two variables using a straight line. The goal is to find the best-fitting line that describes how one variable (dependent/target) changes with another variable (independent/feature).\n\nThe equation is: **y = \u03b2\u2080 + \u03b2\u2081x + \u03b5**\n\nWhere:\n- y is the dependent variable (what we predict)\n- x is the independent variable (what we use to predict)\n- \u03b2\u2080 is the intercept (y-value when x=0)\n- \u03b2\u2081 is the slope (change in y for unit change in x)\n- \u03b5 is the error term\n\n### Why This Matters in Data Science\n\nLinear regression is one of the most widely used algorithms in data science. It's interpretable, fast, and serves as the foundation for more complex models. Use cases include sales forecasting, price prediction, and understanding relationships between business metrics."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example 1: Simple Linear Regression - Advertising Spend vs Sales\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Generate synthetic data\nnp.random.seed(42)\nadvertising_spend = np.array([10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80])\nsales = 1000 + 50 * advertising_spend + np.random.normal(0, 200, len(advertising_spend))\n\n# Create DataFrame\ndf = pd.DataFrame({'Advertising_Spend': advertising_spend, 'Sales': sales})\nprint(df.head())\n\n# Visualize relationship\nplt.figure(figsize=(10, 6))\nplt.scatter(advertising_spend, sales, alpha=0.7, s=100)\nplt.xlabel('Advertising Spend ($1000s)')\nplt.ylabel('Sales ($)')\nplt.title('Advertising Spend vs Sales')\nplt.grid(alpha=0.3)\nplt.show()\n\nprint(f\"\\nCorrelation: {np.corrcoef(advertising_spend, sales)[0,1]:.3f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example 2: Building and Evaluating a Linear Regression Model\n\n# Reshape data for sklearn\nX = advertising_spend.reshape(-1, 1)\ny = sales\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create and train model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_pred_train = model.predict(X_train)\ny_pred_test = model.predict(X_test)\n\n# Model parameters\nprint(\"Model Parameters:\")\nprint(f\"Intercept (\u03b2\u2080): ${model.intercept_:.2f}\")\nprint(f\"Slope (\u03b2\u2081): ${model.coef_[0]:.2f}\")\nprint(f\"\\nInterpretation: For every $1000 increase in advertising, sales increase by ${model.coef_[0]:.2f}\")\n\n# Evaluate model\ntrain_r2 = r2_score(y_train, y_pred_train)\ntest_r2 = r2_score(y_test, y_pred_test)\ntrain_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\ntest_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n\nprint(f\"\\nTraining R\u00b2: {train_r2:.3f}\")\nprint(f\"Testing R\u00b2: {test_r2:.3f}\")\nprint(f\"Training RMSE: ${train_rmse:.2f}\")\nprint(f\"Testing RMSE: ${test_rmse:.2f}\")\n\n# Visualize predictions\nplt.figure(figsize=(10, 6))\nplt.scatter(X_train, y_train, alpha=0.6, label='Training Data', s=100)\nplt.scatter(X_test, y_test, alpha=0.6, label='Test Data', s=100, color='orange')\nplt.plot(X, model.predict(X), 'r-', linewidth=2, label='Fitted Line')\nplt.xlabel('Advertising Spend ($1000s)')\nplt.ylabel('Sales ($)')\nplt.title('Linear Regression: Advertising vs Sales')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Practice Exercise 1.1\n\n**Task:** Given this data about study hours and exam scores:\n```\nstudy_hours = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\nexam_scores = [50, 55, 60, 65, 70, 72, 78, 83, 88, 90]\n```\n\n1. Create a scatter plot\n2. Build a linear regression model\n3. Print the equation (intercept and slope)\n4. Calculate R\u00b2 score\n5. Predict the score for someone who studies 7.5 hours\n\n**Expected Output:**\n```\nEquation: score = 42.0 + 4.8 \u00d7 hours\nR\u00b2 \u2248 0.98\nPrediction for 7.5 hours \u2248 78\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Your code here\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Solution 1.1\n\nstudy_hours = np.array([2, 3, 4, 5, 6, 7, 8, 9, 10, 11]).reshape(-1, 1)\nexam_scores = np.array([50, 55, 60, 65, 70, 72, 78, 83, 88, 90])\n\n# Scatter plot\nplt.figure(figsize=(10, 6))\nplt.scatter(study_hours, exam_scores, s=100, alpha=0.7)\nplt.xlabel('Study Hours')\nplt.ylabel('Exam Score')\nplt.title('Study Hours vs Exam Scores')\nplt.grid(alpha=0.3)\nplt.show()\n\n# Build model\nmodel = LinearRegression()\nmodel.fit(study_hours, exam_scores)\n\n# Print equation\nprint(f\"Equation: score = {model.intercept_:.1f} + {model.coef_[0]:.1f} \u00d7 hours\")\n\n# Calculate R\u00b2\ny_pred = model.predict(study_hours)\nr2 = r2_score(exam_scores, y_pred)\nprint(f\"R\u00b2 Score: {r2:.3f}\")\n\n# Predict for 7.5 hours\nprediction = model.predict([[7.5]])[0]\nprint(f\"Prediction for 7.5 hours: {prediction:.1f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# Section 2: Multiple Linear Regression\n---\n\n## What is Multiple Linear Regression?\n\n**Multiple linear regression** extends simple linear regression to use multiple features for prediction. Instead of one line, we're fitting a hyperplane in multi-dimensional space.\n\nThe equation is: **y = \u03b2\u2080 + \u03b2\u2081x\u2081 + \u03b2\u2082x\u2082 + ... + \u03b2\u2099x\u2099 + \u03b5**\n\nEach coefficient (\u03b2) represents the change in y for a unit change in that feature, holding all other features constant.\n\n### Why This Matters in Data Science\n\nReal-world predictions rarely depend on just one variable. Multiple regression lets you model complex relationships - predicting house prices based on size, location, and age, or sales based on advertising, season, and competition."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example 1: Multiple Linear Regression - House Price Prediction\n\n# Generate synthetic housing data\nnp.random.seed(42)\nn_samples = 100\nsize_sqft = np.random.randint(1000, 3500, n_samples)\nbedrooms = np.random.randint(2, 6, n_samples)\nage_years = np.random.randint(0, 50, n_samples)\n\n# Price formula: base + size_effect + bedroom_effect - age_effect + noise\nprice = (50000 + \n        150 * size_sqft + \n        25000 * bedrooms - \n        1000 * age_years + \n        np.random.normal(0, 50000, n_samples))\n\n# Create DataFrame\ndf_houses = pd.DataFrame({\n    'Size_SqFt': size_sqft,\n    'Bedrooms': bedrooms,\n    'Age_Years': age_years,\n    'Price': price\n})\n\nprint(\"House Data Sample:\")\nprint(df_houses.head())\nprint(f\"\\nDataset shape: {df_houses.shape}\")\nprint(f\"\\nCorrelations with Price:\")\nprint(df_houses.corr()['Price'].sort_values(ascending=False))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example 2: Building Multiple Regression Model\n\n# Prepare features and target\nX = df_houses[['Size_SqFt', 'Bedrooms', 'Age_Years']]\ny = df_houses['Price']\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train model\nmodel_multi = LinearRegression()\nmodel_multi.fit(X_train, y_train)\n\n# Model coefficients\nprint(\"Multiple Regression Model:\")\nprint(f\"Intercept: ${model_multi.intercept_:,.2f}\")\nfor feature, coef in zip(X.columns, model_multi.coef_):\n    print(f\"{feature}: ${coef:,.2f}\")\n\nprint(\"\\nInterpretation:\")\nprint(f\"- Each additional sqft adds ${model_multi.coef_[0]:.2f} to price\")\nprint(f\"- Each additional bedroom adds ${model_multi.coef_[1]:,.2f} to price\")\nprint(f\"- Each additional year of age reduces price by ${abs(model_multi.coef_[2]):,.2f}\")\n\n# Evaluate\ny_pred_train = model_multi.predict(X_train)\ny_pred_test = model_multi.predict(X_test)\n\nprint(f\"\\nModel Performance:\")\nprint(f\"Training R\u00b2: {r2_score(y_train, y_pred_train):.3f}\")\nprint(f\"Testing R\u00b2: {r2_score(y_test, y_pred_test):.3f}\")\nprint(f\"Testing RMSE: ${np.sqrt(mean_squared_error(y_test, y_pred_test)):,.2f}\")\n\n# Example prediction\nsample_house = pd.DataFrame({'Size_SqFt': [2500], 'Bedrooms': [4], 'Age_Years': [10]})\npredicted_price = model_multi.predict(sample_house)[0]\nprint(f\"\\nPrediction for 2500 sqft, 4 bed, 10 years old: ${predicted_price:,.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Practice Exercise 2.1\n\n**Task:** Build a multiple regression model to predict student performance:\n```\ndata = {\n    'study_hours': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n    'attendance_pct': [70, 75, 80, 85, 88, 90, 92, 95, 97, 98],\n    'prev_score': [50, 55, 60, 62, 68, 70, 75, 78, 82, 85],\n    'final_score': [55, 60, 65, 70, 75, 78, 83, 87, 90, 93]\n}\n```\n\n1. Build a multiple regression model\n2. Print all coefficients with interpretation\n3. Calculate R\u00b2 score\n4. Which feature is most important? (highest absolute coefficient)\n\n**Expected Output:**\n```\nR\u00b2 > 0.95\nMost important feature: study_hours\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Your code here\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Solution 2.1\n\ndata = {\n    'study_hours': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n    'attendance_pct': [70, 75, 80, 85, 88, 90, 92, 95, 97, 98],\n    'prev_score': [50, 55, 60, 62, 68, 70, 75, 78, 82, 85],\n    'final_score': [55, 60, 65, 70, 75, 78, 83, 87, 90, 93]\n}\n\ndf_students = pd.DataFrame(data)\nX = df_students[['study_hours', 'attendance_pct', 'prev_score']]\ny = df_students['final_score']\n\n# Build model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Print coefficients\nprint(\"Model Coefficients:\")\nprint(f\"Intercept: {model.intercept_:.2f}\")\nfor feature, coef in zip(X.columns, model.coef_):\n    print(f\"{feature}: {coef:.3f}\")\n\n# Calculate R\u00b2\ny_pred = model.predict(X)\nr2 = r2_score(y, y_pred)\nprint(f\"\\nR\u00b2 Score: {r2:.3f}\")\n\n# Most important feature\nfeature_importance = pd.DataFrame({\n    'Feature': X.columns,\n    'Coefficient': np.abs(model.coef_)\n}).sort_values('Coefficient', ascending=False)\n\nprint(f\"\\nMost important feature: {feature_importance.iloc[0]['Feature']}\")\nprint(\"\\nFeature Importance (by absolute coefficient):\")\nprint(feature_importance)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# Module Summary\n\n## Key Takeaways\n\n- **Simple linear regression** models the relationship between two variables using a straight line\n- **Multiple linear regression** extends this to multiple features, enabling more realistic models\n- **OLS (Ordinary Least Squares)** finds the best-fit line by minimizing squared residuals\n- **Gradient descent** is an iterative optimization algorithm that can also find regression coefficients\n- **Polynomial regression** captures non-linear relationships while still using linear regression\n- **Regularization** (Ridge, Lasso, Elastic Net) prevents overfitting by penalizing large coefficients\n- **Evaluation metrics** like R\u00b2, RMSE, and MAE quantify model performance\n- **Residual analysis** helps diagnose model assumptions and identify issues\n- **Feature scaling** and **transformation** improve model performance and convergence\n- Coefficients in linear regression are **interpretable** - each represents the effect of one feature on the target\n\n## Next Module\n\nIn Module 18: Logistic Regression, we'll extend regression concepts to classification problems. You'll learn how to predict binary outcomes and evaluate classification models using metrics like accuracy, precision, recall, and ROC curves.\n\n## Additional Practice\n\nFor extra practice, try these challenges:\n\n1. **Feature Engineering**: Create polynomial features and interaction terms to improve model performance\n2. **Real Dataset**: Use the California Housing dataset from sklearn and build a comprehensive regression model\n3. **Regularization Comparison**: Compare Ridge, Lasso, and Elastic Net on a dataset with many features\n4. **Gradient Descent**: Implement gradient descent from scratch and compare with sklearn's solution\n5. **Residual Analysis**: Create a complete residual diagnostic plot (residuals vs fitted, Q-Q plot, scale-location, leverage)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}