{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5: File Handling\n",
    "\n",
    "## Topics Covered\n",
    "1. Reading Text Files\n",
    "2. Writing to Files\n",
    "3. Working with CSV Files (without pandas)\n",
    "4. Working with JSON Files\n",
    "5. Context Managers (with statement)\n",
    "6. File Paths and OS Module\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will be able to:\n",
    "- Read and write text files using Python's built-in functions\n",
    "- Process CSV data without external libraries\n",
    "- Parse and create JSON files for configuration and data exchange\n",
    "- Use context managers for safe file handling\n",
    "- Navigate and manipulate file paths programmatically\n",
    "- Build data pipelines that read, process, and write files\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 1: Reading Text Files\n",
    "---\n",
    "\n",
    "## What is File Handling?\n",
    "\n",
    "File handling allows your Python programs to interact with files stored on your computer. This is fundamental to data science because real-world data comes from files - log files, data exports, configuration files, and more.\n",
    "\n",
    "### Why This Matters in Data Science\n",
    "\n",
    "As a data analyst or data scientist, you'll constantly work with files:\n",
    "- **Reading data** from CSV exports, JSON APIs, or text logs\n",
    "- **Writing results** to reports, processed datasets, or output files\n",
    "- **Parsing configuration** files for data pipelines\n",
    "- **Processing log files** to extract insights\n",
    "\n",
    "Understanding how files work at a fundamental level helps you debug issues and work with any file format, even when pandas isn't available or appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntax\n",
    "\n",
    "```python\n",
    "# Opening a file for reading\n",
    "file = open(filename, mode)\n",
    "\n",
    "# Common modes:\n",
    "# 'r'  - Read (default) - file must exist\n",
    "# 'w'  - Write - creates new file or overwrites existing\n",
    "# 'a'  - Append - adds to end of file\n",
    "# 'r+' - Read and write\n",
    "```\n",
    "\n",
    "**Parameters:**\n",
    "- `filename`: Path to the file (string)\n",
    "- `mode`: How to open the file ('r', 'w', 'a', etc.)\n",
    "\n",
    "**Returns:** A file object that you can read from or write to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Methods\n",
    "\n",
    "Python provides several methods to read file contents:\n",
    "\n",
    "| Method | Description |\n",
    "|--------|-------------|\n",
    "| `read()` | Read entire file as a single string |\n",
    "| `readline()` | Read one line at a time |\n",
    "| `readlines()` | Read all lines into a list |\n",
    "| Iteration | Loop through lines directly |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Reading an entire file with read()\n",
    "# Let's read a simple text file\n",
    "\n",
    "file = open('assets/datasets/meeting_notes.txt', 'r')\n",
    "content = file.read()\n",
    "file.close()  # Always close the file when done!\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Reading line by line with readline()\n",
    "# Useful when you only need specific lines\n",
    "\n",
    "file = open('assets/datasets/meeting_notes.txt', 'r')\n",
    "\n",
    "# Read just the first three lines\n",
    "line1 = file.readline()\n",
    "line2 = file.readline()\n",
    "line3 = file.readline()\n",
    "\n",
    "file.close()\n",
    "\n",
    "print(\"First line:\", line1)\n",
    "print(\"Second line:\", line2)\n",
    "print(\"Third line:\", line3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Reading all lines into a list with readlines()\n",
    "# Each line becomes an element in the list\n",
    "\n",
    "file = open('assets/datasets/meeting_notes.txt', 'r')\n",
    "lines = file.readlines()\n",
    "file.close()\n",
    "\n",
    "print(f\"Total lines: {len(lines)}\")\n",
    "print(f\"Type: {type(lines)}\")\n",
    "print(\"\\nFirst 5 lines:\")\n",
    "for i, line in enumerate(lines[:5]):\n",
    "    print(f\"  {i}: {repr(line)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Iterating through a file directly\n",
    "# Most memory-efficient way to process large files\n",
    "\n",
    "file = open('assets/datasets/server_log.txt', 'r')\n",
    "\n",
    "error_count = 0\n",
    "warning_count = 0\n",
    "\n",
    "for line in file:\n",
    "    if 'ERROR' in line:\n",
    "        error_count += 1\n",
    "        print(f\"Error found: {line.strip()}\")\n",
    "    elif 'WARNING' in line:\n",
    "        warning_count += 1\n",
    "\n",
    "file.close()\n",
    "\n",
    "print(f\"\\nSummary: {error_count} errors, {warning_count} warnings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Issue: Newline Characters\n",
    "\n",
    "When reading files, each line includes the newline character `\\n` at the end. Use `strip()` to remove it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Handling newline characters\n",
    "\n",
    "file = open('assets/datasets/meeting_notes.txt', 'r')\n",
    "lines = file.readlines()\n",
    "file.close()\n",
    "\n",
    "# With newline (notice the extra blank line between prints)\n",
    "print(\"With newline:\")\n",
    "print(lines[0])\n",
    "print(lines[1])\n",
    "\n",
    "print(\"---\")\n",
    "\n",
    "# Using strip() to remove newline\n",
    "print(\"Without newline (using strip):\")\n",
    "print(lines[0].strip())\n",
    "print(lines[1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercise 1.1\n",
    "\n",
    "**Task:** Read the file `assets/datasets/server_log.txt` and count how many log entries there are for each log level (INFO, DEBUG, WARNING, ERROR).\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "Log Level Counts:\n",
    "INFO: 12\n",
    "DEBUG: 2\n",
    "WARNING: 2\n",
    "ERROR: 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1.1\n",
    "\n",
    "file = open('assets/datasets/server_log.txt', 'r')\n",
    "\n",
    "# Initialize counters\n",
    "counts = {'INFO': 0, 'DEBUG': 0, 'WARNING': 0, 'ERROR': 0}\n",
    "\n",
    "for line in file:\n",
    "    for level in counts:\n",
    "        if level in line:\n",
    "            counts[level] += 1\n",
    "            break  # Each line has only one level\n",
    "\n",
    "file.close()\n",
    "\n",
    "print(\"Log Level Counts:\")\n",
    "for level, count in counts.items():\n",
    "    print(f\"{level}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercise 1.2\n",
    "\n",
    "**Task:** Read `assets/datasets/meeting_notes.txt` and extract all the action items (lines that start with \"- \"). Store them in a list and print each action item.\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "Action Items Found:\n",
    "1. Sarah: Finalize budget allocation by Jan 15\n",
    "2. Mike: Complete hiring pipeline by Jan 20\n",
    "3. Jennifer: Prepare product launch materials\n",
    "4. David: Set up analytics dashboard\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1.2\n",
    "\n",
    "file = open('assets/datasets/meeting_notes.txt', 'r')\n",
    "action_items = []\n",
    "\n",
    "for line in file:\n",
    "    line = line.strip()\n",
    "    if line.startswith('- '):\n",
    "        # Remove the '- ' prefix\n",
    "        action_items.append(line[2:])\n",
    "\n",
    "file.close()\n",
    "\n",
    "print(\"Action Items Found:\")\n",
    "for i, item in enumerate(action_items, 1):\n",
    "    print(f\"{i}. {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 2: Writing to Files\n",
    "---\n",
    "\n",
    "## Writing Modes\n",
    "\n",
    "Python provides different modes for writing files:\n",
    "\n",
    "| Mode | Description |\n",
    "|------|-------------|\n",
    "| `'w'` | Write mode - creates new file or **overwrites** existing |\n",
    "| `'a'` | Append mode - adds to the end of existing file |\n",
    "| `'x'` | Exclusive creation - fails if file already exists |\n",
    "\n",
    "**Warning:** Using `'w'` mode on an existing file will erase all its contents!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntax\n",
    "\n",
    "```python\n",
    "# Writing to a file\n",
    "file = open(filename, 'w')  # or 'a' for append\n",
    "file.write(string)          # Write a string\n",
    "file.writelines(list)       # Write a list of strings\n",
    "file.close()\n",
    "```\n",
    "\n",
    "**Note:** `write()` does not automatically add newlines - you must include `\\n` yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Writing a new file with write()\n",
    "\n",
    "file = open('assets/datasets/output_example.txt', 'w')\n",
    "\n",
    "file.write(\"Data Analysis Report\\n\")\n",
    "file.write(\"=\" * 20 + \"\\n\")\n",
    "file.write(\"\\n\")\n",
    "file.write(\"Key Findings:\\n\")\n",
    "file.write(\"1. Sales increased by 15%\\n\")\n",
    "file.write(\"2. Customer satisfaction is at 4.2/5\\n\")\n",
    "file.write(\"3. Top region: Central\\n\")\n",
    "\n",
    "file.close()\n",
    "\n",
    "# Verify by reading it back\n",
    "file = open('assets/datasets/output_example.txt', 'r')\n",
    "print(file.read())\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Writing multiple lines with writelines()\n",
    "\n",
    "lines = [\n",
    "    \"Product Inventory Report\\n\",\n",
    "    \"Generated: 2024-01-15\\n\",\n",
    "    \"\\n\",\n",
    "    \"Low Stock Items:\\n\",\n",
    "    \"- Standing Desk: 15 units\\n\",\n",
    "    \"- Ergonomic Chair: 28 units\\n\",\n",
    "    \"- 4K Monitor: 32 units\\n\"\n",
    "]\n",
    "\n",
    "file = open('assets/datasets/inventory_report.txt', 'w')\n",
    "file.writelines(lines)  # Note: each string must include \\n\n",
    "file.close()\n",
    "\n",
    "# Verify\n",
    "file = open('assets/datasets/inventory_report.txt', 'r')\n",
    "print(file.read())\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Appending to an existing file\n",
    "\n",
    "# First, let's see what's in the file\n",
    "file = open('assets/datasets/inventory_report.txt', 'r')\n",
    "print(\"Before append:\")\n",
    "print(file.read())\n",
    "file.close()\n",
    "\n",
    "# Now append new content\n",
    "file = open('assets/datasets/inventory_report.txt', 'a')\n",
    "file.write(\"\\n--- Update ---\\n\")\n",
    "file.write(\"- USB-C Dock: Restocked to 150 units\\n\")\n",
    "file.close()\n",
    "\n",
    "# Check the result\n",
    "file = open('assets/datasets/inventory_report.txt', 'r')\n",
    "print(\"After append:\")\n",
    "print(file.read())\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Example: Processing Log Data\n",
    "\n",
    "A common data task: read a log file, filter for specific entries, and write results to a new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract errors from log and save to separate file\n",
    "\n",
    "# Read the source file\n",
    "source = open('assets/datasets/server_log.txt', 'r')\n",
    "output = open('assets/datasets/errors_only.txt', 'w')\n",
    "\n",
    "# Write header\n",
    "output.write(\"Error Log Extract\\n\")\n",
    "output.write(\"=\" * 40 + \"\\n\\n\")\n",
    "\n",
    "error_count = 0\n",
    "for line in source:\n",
    "    if 'ERROR' in line:\n",
    "        output.write(line)\n",
    "        error_count += 1\n",
    "\n",
    "output.write(f\"\\nTotal errors: {error_count}\\n\")\n",
    "\n",
    "source.close()\n",
    "output.close()\n",
    "\n",
    "# Verify output\n",
    "file = open('assets/datasets/errors_only.txt', 'r')\n",
    "print(file.read())\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercise 2.1\n",
    "\n",
    "**Task:** Read the server log file and create a summary report that shows:\n",
    "1. Total number of log entries\n",
    "2. Count of each log level\n",
    "3. Timestamp of first and last entry\n",
    "\n",
    "Write this summary to `assets/datasets/log_summary.txt`.\n",
    "\n",
    "**Expected Output (in log_summary.txt):**\n",
    "```\n",
    "Server Log Summary\n",
    "==================\n",
    "\n",
    "Total entries: 20\n",
    "\n",
    "Log Levels:\n",
    "  INFO: 12\n",
    "  DEBUG: 2\n",
    "  WARNING: 2\n",
    "  ERROR: 2\n",
    "\n",
    "Time Range:\n",
    "  First: 2024-01-15 08:23:45\n",
    "  Last: 2024-01-15 09:00:00\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2.1\n",
    "\n",
    "# Read and analyze the log\n",
    "source = open('assets/datasets/server_log.txt', 'r')\n",
    "lines = source.readlines()\n",
    "source.close()\n",
    "\n",
    "# Count log levels\n",
    "counts = {'INFO': 0, 'DEBUG': 0, 'WARNING': 0, 'ERROR': 0}\n",
    "first_timestamp = None\n",
    "last_timestamp = None\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    \n",
    "    # Extract timestamp (first 19 characters: YYYY-MM-DD HH:MM:SS)\n",
    "    timestamp = line[:19]\n",
    "    if first_timestamp is None:\n",
    "        first_timestamp = timestamp\n",
    "    last_timestamp = timestamp\n",
    "    \n",
    "    # Count log levels\n",
    "    for level in counts:\n",
    "        if level in line:\n",
    "            counts[level] += 1\n",
    "            break\n",
    "\n",
    "# Write summary\n",
    "output = open('assets/datasets/log_summary.txt', 'w')\n",
    "output.write(\"Server Log Summary\\n\")\n",
    "output.write(\"==================\\n\\n\")\n",
    "output.write(f\"Total entries: {len(lines)}\\n\\n\")\n",
    "output.write(\"Log Levels:\\n\")\n",
    "for level, count in counts.items():\n",
    "    output.write(f\"  {level}: {count}\\n\")\n",
    "output.write(f\"\\nTime Range:\\n\")\n",
    "output.write(f\"  First: {first_timestamp}\\n\")\n",
    "output.write(f\"  Last: {last_timestamp}\\n\")\n",
    "output.close()\n",
    "\n",
    "# Verify\n",
    "file = open('assets/datasets/log_summary.txt', 'r')\n",
    "print(file.read())\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 3: Working with CSV Files (without pandas)\n",
    "---\n",
    "\n",
    "## What is CSV?\n",
    "\n",
    "CSV (Comma-Separated Values) is one of the most common formats for tabular data. Each line represents a row, and values are separated by commas (or other delimiters).\n",
    "\n",
    "Example CSV:\n",
    "```\n",
    "name,age,city\n",
    "Alice,30,New York\n",
    "Bob,25,Los Angeles\n",
    "```\n",
    "\n",
    "### Why Learn CSV Without pandas?\n",
    "\n",
    "While pandas is the go-to tool for CSV in data science, understanding the `csv` module helps you:\n",
    "- Work in environments where pandas isn't available\n",
    "- Handle edge cases pandas might not catch\n",
    "- Process very large files more efficiently\n",
    "- Understand what pandas does under the hood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The csv Module\n",
    "\n",
    "Python's built-in `csv` module handles the complexities of CSV parsing:\n",
    "- Properly handles quoted fields\n",
    "- Deals with commas inside values\n",
    "- Supports different delimiters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntax\n",
    "\n",
    "```python\n",
    "import csv\n",
    "\n",
    "# Reading CSV\n",
    "with open('file.csv', 'r') as file:\n",
    "    reader = csv.reader(file)      # Returns rows as lists\n",
    "    # or\n",
    "    reader = csv.DictReader(file)  # Returns rows as dictionaries\n",
    "\n",
    "# Writing CSV\n",
    "with open('file.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)           # Write lists as rows\n",
    "    # or\n",
    "    writer = csv.DictWriter(file, fieldnames=['col1', 'col2'])\n",
    "```\n",
    "\n",
    "**Note:** Always use `newline=''` when opening files for CSV writing to prevent extra blank rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Example: Reading CSV with csv.reader()\n",
    "# Returns each row as a list\n",
    "\n",
    "file = open('assets/datasets/sales_data.csv', 'r')\n",
    "reader = csv.reader(file)\n",
    "\n",
    "# Get the header row\n",
    "headers = next(reader)\n",
    "print(\"Headers:\", headers)\n",
    "print()\n",
    "\n",
    "# Read first 5 data rows\n",
    "print(\"First 5 rows:\")\n",
    "for i, row in enumerate(reader):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    print(row)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Reading CSV with csv.DictReader()\n",
    "# Returns each row as a dictionary with column names as keys\n",
    "\n",
    "file = open('assets/datasets/sales_data.csv', 'r')\n",
    "reader = csv.DictReader(file)\n",
    "\n",
    "print(\"First 3 transactions:\\n\")\n",
    "for i, row in enumerate(reader):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    print(f\"Transaction: {row['transaction_id']}\")\n",
    "    print(f\"  Product: {row['product']}\")\n",
    "    print(f\"  Amount: ${row['total_amount']}\")\n",
    "    print(f\"  Region: {row['region']}\")\n",
    "    print()\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Analyzing CSV data - Calculate total sales by region\n",
    "\n",
    "file = open('assets/datasets/sales_data.csv', 'r')\n",
    "reader = csv.DictReader(file)\n",
    "\n",
    "# Aggregate sales by region\n",
    "sales_by_region = {}\n",
    "\n",
    "for row in reader:\n",
    "    region = row['region']\n",
    "    amount = float(row['total_amount'])\n",
    "    \n",
    "    if region in sales_by_region:\n",
    "        sales_by_region[region] += amount\n",
    "    else:\n",
    "        sales_by_region[region] = amount\n",
    "\n",
    "file.close()\n",
    "\n",
    "print(\"Total Sales by Region:\")\n",
    "print(\"-\" * 30)\n",
    "for region, total in sorted(sales_by_region.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{region:12} ${total:>12,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Handling missing values in CSV\n",
    "\n",
    "file = open('assets/datasets/sales_data.csv', 'r')\n",
    "reader = csv.DictReader(file)\n",
    "\n",
    "# Find rows with missing data\n",
    "missing_price = 0\n",
    "missing_rep = 0\n",
    "missing_rating = 0\n",
    "\n",
    "for row in reader:\n",
    "    if row['unit_price'] == '':\n",
    "        missing_price += 1\n",
    "    if row['sales_rep'] == '':\n",
    "        missing_rep += 1\n",
    "    if row['customer_rating'] == '':\n",
    "        missing_rating += 1\n",
    "\n",
    "file.close()\n",
    "\n",
    "print(\"Missing Value Analysis:\")\n",
    "print(f\"  unit_price: {missing_price} missing\")\n",
    "print(f\"  sales_rep: {missing_rep} missing\")\n",
    "print(f\"  customer_rating: {missing_rating} missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Writing CSV with csv.writer()\n",
    "\n",
    "# Create a summary report as CSV\n",
    "file = open('assets/datasets/sales_data.csv', 'r')\n",
    "reader = csv.DictReader(file)\n",
    "\n",
    "# Calculate category summaries\n",
    "category_stats = {}\n",
    "for row in reader:\n",
    "    category = row['category']\n",
    "    amount = float(row['total_amount'])\n",
    "    \n",
    "    if category not in category_stats:\n",
    "        category_stats[category] = {'total': 0, 'count': 0}\n",
    "    \n",
    "    category_stats[category]['total'] += amount\n",
    "    category_stats[category]['count'] += 1\n",
    "\n",
    "file.close()\n",
    "\n",
    "# Write summary to new CSV\n",
    "output = open('assets/datasets/category_summary.csv', 'w', newline='')\n",
    "writer = csv.writer(output)\n",
    "\n",
    "# Write header\n",
    "writer.writerow(['Category', 'Transaction Count', 'Total Sales', 'Average Sale'])\n",
    "\n",
    "# Write data rows\n",
    "for category, stats in category_stats.items():\n",
    "    avg = stats['total'] / stats['count']\n",
    "    writer.writerow([category, stats['count'], round(stats['total'], 2), round(avg, 2)])\n",
    "\n",
    "output.close()\n",
    "\n",
    "# Verify by reading back\n",
    "print(\"Category Summary CSV:\")\n",
    "file = open('assets/datasets/category_summary.csv', 'r')\n",
    "print(file.read())\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Writing CSV with csv.DictWriter()\n",
    "# More readable when you have dictionaries\n",
    "\n",
    "# Sample employee data to write\n",
    "employees = [\n",
    "    {'id': 'E001', 'name': 'Alice Johnson', 'department': 'Engineering', 'salary': 95000},\n",
    "    {'id': 'E002', 'name': 'Bob Smith', 'department': 'Marketing', 'salary': 75000},\n",
    "    {'id': 'E003', 'name': 'Carol Davis', 'department': 'Engineering', 'salary': 88000}\n",
    "]\n",
    "\n",
    "# Define column order\n",
    "fieldnames = ['id', 'name', 'department', 'salary']\n",
    "\n",
    "output = open('assets/datasets/employees_sample.csv', 'w', newline='')\n",
    "writer = csv.DictWriter(output, fieldnames=fieldnames)\n",
    "\n",
    "writer.writeheader()  # Writes the column names\n",
    "writer.writerows(employees)  # Writes all rows at once\n",
    "\n",
    "output.close()\n",
    "\n",
    "# Verify\n",
    "print(\"Employees Sample CSV:\")\n",
    "file = open('assets/datasets/employees_sample.csv', 'r')\n",
    "print(file.read())\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercise 3.1\n",
    "\n",
    "**Task:** Read the `employees.csv` file and create a report showing:\n",
    "1. Number of employees per department\n",
    "2. Average salary per department\n",
    "\n",
    "Write the results to `assets/datasets/department_report.csv`.\n",
    "\n",
    "**Expected Output (printed and in CSV):**\n",
    "```\n",
    "Department,Employee Count,Average Salary\n",
    "Engineering,XX,XXXXX.XX\n",
    "Sales,XX,XXXXX.XX\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3.1\n",
    "\n",
    "import csv\n",
    "\n",
    "# Read employees data\n",
    "file = open('assets/datasets/employees.csv', 'r')\n",
    "reader = csv.DictReader(file)\n",
    "\n",
    "# Aggregate by department\n",
    "dept_stats = {}\n",
    "\n",
    "for row in reader:\n",
    "    dept = row['department']\n",
    "    salary = int(row['salary'])\n",
    "    \n",
    "    if dept not in dept_stats:\n",
    "        dept_stats[dept] = {'count': 0, 'total_salary': 0}\n",
    "    \n",
    "    dept_stats[dept]['count'] += 1\n",
    "    dept_stats[dept]['total_salary'] += salary\n",
    "\n",
    "file.close()\n",
    "\n",
    "# Write report CSV\n",
    "output = open('assets/datasets/department_report.csv', 'w', newline='')\n",
    "writer = csv.writer(output)\n",
    "\n",
    "writer.writerow(['Department', 'Employee Count', 'Average Salary'])\n",
    "\n",
    "for dept, stats in sorted(dept_stats.items()):\n",
    "    avg_salary = stats['total_salary'] / stats['count']\n",
    "    writer.writerow([dept, stats['count'], round(avg_salary, 2)])\n",
    "\n",
    "output.close()\n",
    "\n",
    "# Display results\n",
    "print(\"Department Report:\")\n",
    "file = open('assets/datasets/department_report.csv', 'r')\n",
    "print(file.read())\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercise 3.2\n",
    "\n",
    "**Task:** Find the top 10 highest-value transactions from `sales_data.csv`. Write these to a new file `assets/datasets/top_transactions.csv` with just the columns: transaction_id, date, product, total_amount.\n",
    "\n",
    "**Hint:** You'll need to read all data, sort it, then write the top 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3.2\n",
    "\n",
    "import csv\n",
    "\n",
    "# Read all transactions\n",
    "file = open('assets/datasets/sales_data.csv', 'r')\n",
    "reader = csv.DictReader(file)\n",
    "\n",
    "transactions = []\n",
    "for row in reader:\n",
    "    transactions.append({\n",
    "        'transaction_id': row['transaction_id'],\n",
    "        'date': row['date'],\n",
    "        'product': row['product'],\n",
    "        'total_amount': float(row['total_amount'])\n",
    "    })\n",
    "\n",
    "file.close()\n",
    "\n",
    "# Sort by total_amount descending and get top 10\n",
    "top_10 = sorted(transactions, key=lambda x: x['total_amount'], reverse=True)[:10]\n",
    "\n",
    "# Write to new CSV\n",
    "output = open('assets/datasets/top_transactions.csv', 'w', newline='')\n",
    "writer = csv.DictWriter(output, fieldnames=['transaction_id', 'date', 'product', 'total_amount'])\n",
    "\n",
    "writer.writeheader()\n",
    "writer.writerows(top_10)\n",
    "\n",
    "output.close()\n",
    "\n",
    "# Display results\n",
    "print(\"Top 10 Highest-Value Transactions:\")\n",
    "print(\"-\" * 60)\n",
    "file = open('assets/datasets/top_transactions.csv', 'r')\n",
    "print(file.read())\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 4: Working with JSON Files\n",
    "---\n",
    "\n",
    "## What is JSON?\n",
    "\n",
    "JSON (JavaScript Object Notation) is a lightweight data format that's easy for humans to read and machines to parse. It's widely used for:\n",
    "\n",
    "- **API responses** - Most web APIs return JSON\n",
    "- **Configuration files** - Application settings\n",
    "- **Data exchange** - Sharing structured data between systems\n",
    "- **NoSQL databases** - MongoDB, CouchDB store JSON documents\n",
    "\n",
    "### JSON Data Types\n",
    "\n",
    "| JSON Type | Python Type |\n",
    "|-----------|-------------|\n",
    "| object | dict |\n",
    "| array | list |\n",
    "| string | str |\n",
    "| number | int or float |\n",
    "| true/false | True/False |\n",
    "| null | None |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntax\n",
    "\n",
    "```python\n",
    "import json\n",
    "\n",
    "# Reading JSON from file\n",
    "with open('file.json', 'r') as f:\n",
    "    data = json.load(f)  # Parses file into Python object\n",
    "\n",
    "# Reading JSON from string\n",
    "data = json.loads(json_string)\n",
    "\n",
    "# Writing JSON to file\n",
    "with open('file.json', 'w') as f:\n",
    "    json.dump(data, f)  # Writes Python object as JSON\n",
    "\n",
    "# Converting to JSON string\n",
    "json_string = json.dumps(data)\n",
    "```\n",
    "\n",
    "**Common parameters:**\n",
    "- `indent`: Number of spaces for pretty-printing\n",
    "- `sort_keys`: Sort dictionary keys alphabetically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Example: Reading a JSON configuration file\n",
    "\n",
    "file = open('assets/datasets/config.json', 'r')\n",
    "config = json.load(file)\n",
    "file.close()\n",
    "\n",
    "print(f\"Application: {config['application']['name']}\")\n",
    "print(f\"Version: {config['application']['version']}\")\n",
    "print(f\"Database Host: {config['database']['host']}\")\n",
    "print(f\"Debug Mode: {config['application']['debug_mode']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Navigating nested JSON structure\n",
    "\n",
    "file = open('assets/datasets/products.json', 'r')\n",
    "catalog = json.load(file)\n",
    "file.close()\n",
    "\n",
    "print(f\"Catalog Version: {catalog['catalog_version']}\")\n",
    "print(f\"Last Updated: {catalog['last_updated']}\")\n",
    "print(f\"Currency: {catalog['currency']}\")\n",
    "print()\n",
    "\n",
    "# Access nested categories\n",
    "for category_name, category_data in catalog['categories'].items():\n",
    "    product_count = len(category_data['products'])\n",
    "    tax_rate = category_data['tax_rate']\n",
    "    print(f\"{category_name}: {product_count} products (Tax: {tax_rate*100}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extracting specific data from nested JSON\n",
    "\n",
    "file = open('assets/datasets/products.json', 'r')\n",
    "catalog = json.load(file)\n",
    "file.close()\n",
    "\n",
    "# Find all products with low stock (less than 50 units)\n",
    "print(\"Low Stock Alert (< 50 units):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for category_name, category_data in catalog['categories'].items():\n",
    "    for product in category_data['products']:\n",
    "        stock = product['inventory']['stock']\n",
    "        if stock < 50:\n",
    "            print(f\"{product['name']:30} Stock: {stock:4} (Reorder: {product['inventory']['reorder_point']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Writing JSON with pretty printing\n",
    "\n",
    "# Create a data summary to save\n",
    "file = open('assets/datasets/products.json', 'r')\n",
    "catalog = json.load(file)\n",
    "file.close()\n",
    "\n",
    "# Calculate statistics\n",
    "summary = {\n",
    "    \"report_type\": \"Product Catalog Summary\",\n",
    "    \"generated\": \"2024-01-15\",\n",
    "    \"categories\": {}\n",
    "}\n",
    "\n",
    "total_products = 0\n",
    "total_inventory = 0\n",
    "\n",
    "for cat_name, cat_data in catalog['categories'].items():\n",
    "    products = cat_data['products']\n",
    "    product_count = len(products)\n",
    "    total_stock = sum(p['inventory']['stock'] for p in products)\n",
    "    avg_rating = sum(p['ratings']['average'] for p in products) / product_count\n",
    "    \n",
    "    summary['categories'][cat_name] = {\n",
    "        \"product_count\": product_count,\n",
    "        \"total_stock\": total_stock,\n",
    "        \"average_rating\": round(avg_rating, 2)\n",
    "    }\n",
    "    \n",
    "    total_products += product_count\n",
    "    total_inventory += total_stock\n",
    "\n",
    "summary['totals'] = {\n",
    "    \"total_products\": total_products,\n",
    "    \"total_inventory\": total_inventory\n",
    "}\n",
    "\n",
    "# Write with pretty printing\n",
    "output = open('assets/datasets/catalog_summary.json', 'w')\n",
    "json.dump(summary, output, indent=2)\n",
    "output.close()\n",
    "\n",
    "# Display result\n",
    "print(\"Written to catalog_summary.json:\")\n",
    "file = open('assets/datasets/catalog_summary.json', 'r')\n",
    "print(file.read())\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Modifying and saving JSON\n",
    "\n",
    "# Read existing config\n",
    "file = open('assets/datasets/config.json', 'r')\n",
    "config = json.load(file)\n",
    "file.close()\n",
    "\n",
    "# Create a modified copy for testing\n",
    "test_config = config.copy()\n",
    "test_config['application']['environment'] = 'testing'\n",
    "test_config['application']['debug_mode'] = True\n",
    "test_config['database']['host'] = 'localhost'\n",
    "test_config['database']['name'] = 'analytics_test'\n",
    "test_config['logging']['level'] = 'DEBUG'\n",
    "\n",
    "# Save the test configuration\n",
    "output = open('assets/datasets/config_test.json', 'w')\n",
    "json.dump(test_config, output, indent=2)\n",
    "output.close()\n",
    "\n",
    "print(\"Test configuration saved!\")\n",
    "print(\"\\nKey differences:\")\n",
    "print(f\"  Environment: {config['application']['environment']} -> {test_config['application']['environment']}\")\n",
    "print(f\"  Debug Mode: {config['application']['debug_mode']} -> {test_config['application']['debug_mode']}\")\n",
    "print(f\"  Database: {config['database']['name']} -> {test_config['database']['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercise 4.1\n",
    "\n",
    "**Task:** Read `products.json` and find:\n",
    "1. The product with the highest rating\n",
    "2. The product with the most reviews\n",
    "3. The most expensive product\n",
    "\n",
    "Print the results in a formatted way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 4.1\n",
    "\n",
    "import json\n",
    "\n",
    "file = open('assets/datasets/products.json', 'r')\n",
    "catalog = json.load(file)\n",
    "file.close()\n",
    "\n",
    "# Initialize trackers\n",
    "highest_rated = {'name': '', 'rating': 0, 'category': ''}\n",
    "most_reviewed = {'name': '', 'reviews': 0, 'category': ''}\n",
    "most_expensive = {'name': '', 'price': 0, 'category': ''}\n",
    "\n",
    "# Search through all products\n",
    "for cat_name, cat_data in catalog['categories'].items():\n",
    "    for product in cat_data['products']:\n",
    "        # Check rating\n",
    "        if product['ratings']['average'] > highest_rated['rating']:\n",
    "            highest_rated = {\n",
    "                'name': product['name'],\n",
    "                'rating': product['ratings']['average'],\n",
    "                'category': cat_name\n",
    "            }\n",
    "        \n",
    "        # Check reviews\n",
    "        if product['ratings']['count'] > most_reviewed['reviews']:\n",
    "            most_reviewed = {\n",
    "                'name': product['name'],\n",
    "                'reviews': product['ratings']['count'],\n",
    "                'category': cat_name\n",
    "            }\n",
    "        \n",
    "        # Check price\n",
    "        if product['price'] > most_expensive['price']:\n",
    "            most_expensive = {\n",
    "                'name': product['name'],\n",
    "                'price': product['price'],\n",
    "                'category': cat_name\n",
    "            }\n",
    "\n",
    "print(\"Product Analysis Results\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(f\"Highest Rated Product:\")\n",
    "print(f\"  {highest_rated['name']} ({highest_rated['category']})\")\n",
    "print(f\"  Rating: {highest_rated['rating']} / 5.0\")\n",
    "print()\n",
    "print(f\"Most Reviewed Product:\")\n",
    "print(f\"  {most_reviewed['name']} ({most_reviewed['category']})\")\n",
    "print(f\"  Reviews: {most_reviewed['reviews']}\")\n",
    "print()\n",
    "print(f\"Most Expensive Product:\")\n",
    "print(f\"  {most_expensive['name']} ({most_expensive['category']})\")\n",
    "print(f\"  Price: ${most_expensive['price']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercise 4.2\n",
    "\n",
    "**Task:** Create a JSON file called `inventory_alerts.json` that contains:\n",
    "1. A list of all products with stock below their reorder point\n",
    "2. For each product, include: sku, name, current_stock, reorder_point, shortage (how many units below reorder)\n",
    "\n",
    "The output should look like:\n",
    "```json\n",
    "{\n",
    "  \"alert_type\": \"Low Inventory\",\n",
    "  \"products\": [\n",
    "    {\n",
    "      \"sku\": \"XXX\",\n",
    "      \"name\": \"Product Name\",\n",
    "      \"current_stock\": 10,\n",
    "      \"reorder_point\": 15,\n",
    "      \"shortage\": 5\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 4.2\n",
    "\n",
    "import json\n",
    "\n",
    "# Read product catalog\n",
    "file = open('assets/datasets/products.json', 'r')\n",
    "catalog = json.load(file)\n",
    "file.close()\n",
    "\n",
    "# Find products below reorder point\n",
    "low_stock_products = []\n",
    "\n",
    "for cat_name, cat_data in catalog['categories'].items():\n",
    "    for product in cat_data['products']:\n",
    "        stock = product['inventory']['stock']\n",
    "        reorder = product['inventory']['reorder_point']\n",
    "        \n",
    "        if stock < reorder:\n",
    "            low_stock_products.append({\n",
    "                'sku': product['sku'],\n",
    "                'name': product['name'],\n",
    "                'current_stock': stock,\n",
    "                'reorder_point': reorder,\n",
    "                'shortage': reorder - stock\n",
    "            })\n",
    "\n",
    "# Create alert document\n",
    "alerts = {\n",
    "    'alert_type': 'Low Inventory',\n",
    "    'generated': '2024-01-15',\n",
    "    'total_alerts': len(low_stock_products),\n",
    "    'products': sorted(low_stock_products, key=lambda x: x['shortage'], reverse=True)\n",
    "}\n",
    "\n",
    "# Write to file\n",
    "output = open('assets/datasets/inventory_alerts.json', 'w')\n",
    "json.dump(alerts, output, indent=2)\n",
    "output.close()\n",
    "\n",
    "# Display\n",
    "print(\"Inventory Alerts:\")\n",
    "file = open('assets/datasets/inventory_alerts.json', 'r')\n",
    "print(file.read())\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 5: Context Managers (with statement)\n",
    "---\n",
    "\n",
    "## The Problem with Manual File Handling\n",
    "\n",
    "So far, we've been manually opening and closing files. This approach has problems:\n",
    "\n",
    "```python\n",
    "file = open('data.txt', 'r')\n",
    "content = file.read()\n",
    "# What if an error occurs here?\n",
    "file.close()  # This might never run!\n",
    "```\n",
    "\n",
    "If an error occurs before `close()`, the file remains open, which can:\n",
    "- Lock the file\n",
    "- Consume system resources\n",
    "- Cause data corruption when writing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `with` Statement\n",
    "\n",
    "Context managers solve this problem elegantly:\n",
    "\n",
    "```python\n",
    "with open('data.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "    # Process content\n",
    "# File is automatically closed here, even if errors occur!\n",
    "```\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Using `with` is the **recommended** way to handle files in Python:\n",
    "- **Automatic cleanup** - Files are always closed properly\n",
    "- **Cleaner code** - No need to remember `close()`\n",
    "- **Error handling** - Works correctly even when exceptions occur\n",
    "- **Professional standard** - Expected in production code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Reading with context manager\n",
    "\n",
    "with open('assets/datasets/meeting_notes.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "    print(f\"Read {len(content)} characters\")\n",
    "    print(f\"File is open inside with block: {not file.closed}\")\n",
    "\n",
    "# File is automatically closed after the with block\n",
    "print(f\"File is closed after with block: {file.closed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Writing with context manager\n",
    "\n",
    "data_to_write = [\n",
    "    \"Monthly Sales Report\",\n",
    "    \"=\" * 20,\n",
    "    \"\",\n",
    "    \"Total Sales: $125,430\",\n",
    "    \"Top Product: Laptop Pro 15\",\n",
    "    \"Best Region: Central\"\n",
    "]\n",
    "\n",
    "with open('assets/datasets/monthly_report.txt', 'w') as file:\n",
    "    for line in data_to_write:\n",
    "        file.write(line + '\\n')\n",
    "\n",
    "# Verify by reading back\n",
    "with open('assets/datasets/monthly_report.txt', 'r') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Error handling with context managers\n",
    "# The file is still closed even when an error occurs\n",
    "\n",
    "try:\n",
    "    with open('assets/datasets/server_log.txt', 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        # Simulate an error during processing\n",
    "        result = 10 / 0  # This will raise ZeroDivisionError\n",
    "except ZeroDivisionError:\n",
    "    print(\"An error occurred during processing!\")\n",
    "\n",
    "# Even though an error occurred, the file was properly closed\n",
    "print(f\"File is closed: {file.closed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Working with multiple files simultaneously\n",
    "\n",
    "with open('assets/datasets/server_log.txt', 'r') as source:\n",
    "    with open('assets/datasets/errors_log.txt', 'w') as errors:\n",
    "        with open('assets/datasets/warnings_log.txt', 'w') as warnings:\n",
    "            for line in source:\n",
    "                if 'ERROR' in line:\n",
    "                    errors.write(line)\n",
    "                elif 'WARNING' in line:\n",
    "                    warnings.write(line)\n",
    "\n",
    "# Verify\n",
    "print(\"Errors file:\")\n",
    "with open('assets/datasets/errors_log.txt', 'r') as f:\n",
    "    print(f.read())\n",
    "\n",
    "print(\"Warnings file:\")\n",
    "with open('assets/datasets/warnings_log.txt', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Modern Python syntax for multiple files (Python 3.9+)\n",
    "# Cleaner way to handle multiple files\n",
    "\n",
    "with (\n",
    "    open('assets/datasets/sales_data.csv', 'r') as sales_file,\n",
    "    open('assets/datasets/sales_analysis.txt', 'w') as report_file\n",
    "):\n",
    "    import csv\n",
    "    reader = csv.DictReader(sales_file)\n",
    "    \n",
    "    total_sales = 0\n",
    "    count = 0\n",
    "    \n",
    "    for row in reader:\n",
    "        total_sales += float(row['total_amount'])\n",
    "        count += 1\n",
    "    \n",
    "    report_file.write(\"Sales Analysis\\n\")\n",
    "    report_file.write(\"=\" * 20 + \"\\n\")\n",
    "    report_file.write(f\"Total Transactions: {count}\\n\")\n",
    "    report_file.write(f\"Total Revenue: ${total_sales:,.2f}\\n\")\n",
    "    report_file.write(f\"Average Transaction: ${total_sales/count:,.2f}\\n\")\n",
    "\n",
    "# Verify\n",
    "with open('assets/datasets/sales_analysis.txt', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using context managers with CSV module\n",
    "\n",
    "import csv\n",
    "\n",
    "# Read and process CSV properly\n",
    "with open('assets/datasets/employees.csv', 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    \n",
    "    engineering_employees = []\n",
    "    for row in reader:\n",
    "        if row['department'] == 'Engineering':\n",
    "            engineering_employees.append({\n",
    "                'name': f\"{row['first_name']} {row['last_name']}\",\n",
    "                'title': row['title'],\n",
    "                'salary': int(row['salary'])\n",
    "            })\n",
    "\n",
    "print(f\"Found {len(engineering_employees)} engineers\")\n",
    "print(\"\\nTop 5 by salary:\")\n",
    "for emp in sorted(engineering_employees, key=lambda x: x['salary'], reverse=True)[:5]:\n",
    "    print(f\"  {emp['name']:25} {emp['title']:25} ${emp['salary']:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using context managers with JSON module\n",
    "\n",
    "import json\n",
    "\n",
    "# Read JSON properly\n",
    "with open('assets/datasets/products.json', 'r') as file:\n",
    "    catalog = json.load(file)\n",
    "\n",
    "# Process and create new JSON\n",
    "electronics = catalog['categories']['Electronics']['products']\n",
    "top_electronics = sorted(electronics, key=lambda x: x['ratings']['average'], reverse=True)[:3]\n",
    "\n",
    "# Write new JSON file\n",
    "with open('assets/datasets/top_electronics.json', 'w') as file:\n",
    "    json.dump({\n",
    "        'report': 'Top Rated Electronics',\n",
    "        'products': top_electronics\n",
    "    }, file, indent=2)\n",
    "\n",
    "# Verify\n",
    "with open('assets/datasets/top_electronics.json', 'r') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercise 5.1\n",
    "\n",
    "**Task:** Rewrite this code to use context managers properly:\n",
    "\n",
    "```python\n",
    "file = open('assets/datasets/sales_data.csv', 'r')\n",
    "reader = csv.DictReader(file)\n",
    "regions = {}\n",
    "for row in reader:\n",
    "    region = row['region']\n",
    "    if region not in regions:\n",
    "        regions[region] = 0\n",
    "    regions[region] += 1\n",
    "file.close()\n",
    "\n",
    "output = open('assets/datasets/region_counts.txt', 'w')\n",
    "for region, count in regions.items():\n",
    "    output.write(f\"{region}: {count}\\n\")\n",
    "output.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 5.1\n",
    "\n",
    "import csv\n",
    "\n",
    "# Read with context manager\n",
    "with open('assets/datasets/sales_data.csv', 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    regions = {}\n",
    "    for row in reader:\n",
    "        region = row['region']\n",
    "        if region not in regions:\n",
    "            regions[region] = 0\n",
    "        regions[region] += 1\n",
    "\n",
    "# Write with context manager\n",
    "with open('assets/datasets/region_counts.txt', 'w') as output:\n",
    "    for region, count in sorted(regions.items()):\n",
    "        output.write(f\"{region}: {count}\\n\")\n",
    "\n",
    "# Verify\n",
    "with open('assets/datasets/region_counts.txt', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 6: File Paths and OS Module\n",
    "---\n",
    "\n",
    "## Why File Paths Matter\n",
    "\n",
    "When working with files, you need to correctly specify their locations. Different operating systems use different path formats:\n",
    "\n",
    "- **Windows:** `C:\\Users\\data\\sales.csv`\n",
    "- **Mac/Linux:** `/home/user/data/sales.csv`\n",
    "\n",
    "Python's `os` and `os.path` modules help you work with paths in a platform-independent way.\n",
    "\n",
    "### Why This Matters in Data Science\n",
    "\n",
    "Data pipelines must work across different systems. Hardcoding paths breaks when:\n",
    "- Moving code to a server\n",
    "- Sharing with team members\n",
    "- Deploying to different environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Example: Basic os module functions\n",
    "\n",
    "# Get current working directory\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "\n",
    "# List files in a directory\n",
    "print(f\"\\nFiles in datasets folder:\")\n",
    "for item in os.listdir('assets/datasets'):\n",
    "    print(f\"  {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: os.path for path manipulation\n",
    "\n",
    "import os.path\n",
    "\n",
    "file_path = 'assets/datasets/sales_data.csv'\n",
    "\n",
    "# Get different parts of the path\n",
    "print(f\"Full path: {file_path}\")\n",
    "print(f\"Directory: {os.path.dirname(file_path)}\")\n",
    "print(f\"Filename: {os.path.basename(file_path)}\")\n",
    "print(f\"File extension: {os.path.splitext(file_path)[1]}\")\n",
    "print(f\"Name without extension: {os.path.splitext(os.path.basename(file_path))[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Checking if files/directories exist\n",
    "\n",
    "import os.path\n",
    "\n",
    "paths_to_check = [\n",
    "    'assets/datasets/sales_data.csv',\n",
    "    'assets/datasets/nonexistent.csv',\n",
    "    'assets/datasets',\n",
    "    'assets/images'\n",
    "]\n",
    "\n",
    "for path in paths_to_check:\n",
    "    exists = os.path.exists(path)\n",
    "    is_file = os.path.isfile(path)\n",
    "    is_dir = os.path.isdir(path)\n",
    "    \n",
    "    if exists:\n",
    "        path_type = \"file\" if is_file else \"directory\"\n",
    "        print(f\"{path}: exists ({path_type})\")\n",
    "    else:\n",
    "        print(f\"{path}: does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Building paths safely with os.path.join()\n",
    "\n",
    "import os.path\n",
    "\n",
    "# WRONG way - may break on different operating systems\n",
    "bad_path = 'assets' + '/' + 'datasets' + '/' + 'sales_data.csv'\n",
    "\n",
    "# RIGHT way - works on any operating system\n",
    "good_path = os.path.join('assets', 'datasets', 'sales_data.csv')\n",
    "\n",
    "print(f\"Manual path: {bad_path}\")\n",
    "print(f\"os.path.join: {good_path}\")\n",
    "\n",
    "# Using variables\n",
    "base_dir = 'assets'\n",
    "data_dir = 'datasets'\n",
    "filename = 'employees.csv'\n",
    "\n",
    "full_path = os.path.join(base_dir, data_dir, filename)\n",
    "print(f\"\\nConstructed path: {full_path}\")\n",
    "print(f\"File exists: {os.path.exists(full_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Getting file information\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "\n",
    "file_path = 'assets/datasets/sales_data.csv'\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    # Get file size\n",
    "    size_bytes = os.path.getsize(file_path)\n",
    "    size_kb = size_bytes / 1024\n",
    "    \n",
    "    # Get modification time\n",
    "    mod_time = os.path.getmtime(file_path)\n",
    "    mod_datetime = datetime.fromtimestamp(mod_time)\n",
    "    \n",
    "    print(f\"File: {file_path}\")\n",
    "    print(f\"Size: {size_bytes:,} bytes ({size_kb:.2f} KB)\")\n",
    "    print(f\"Last modified: {mod_datetime}\")\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Creating directories\n",
    "\n",
    "import os\n",
    "\n",
    "# Create a single directory\n",
    "new_dir = 'assets/datasets/reports'\n",
    "\n",
    "if not os.path.exists(new_dir):\n",
    "    os.mkdir(new_dir)\n",
    "    print(f\"Created directory: {new_dir}\")\n",
    "else:\n",
    "    print(f\"Directory already exists: {new_dir}\")\n",
    "\n",
    "# Create nested directories (including any missing parent directories)\n",
    "nested_dir = 'assets/datasets/archive/2024/january'\n",
    "\n",
    "if not os.path.exists(nested_dir):\n",
    "    os.makedirs(nested_dir)\n",
    "    print(f\"Created nested directories: {nested_dir}\")\n",
    "else:\n",
    "    print(f\"Directory already exists: {nested_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Listing files with filtering\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "data_dir = 'assets/datasets'\n",
    "\n",
    "# List only CSV files\n",
    "print(\"CSV files:\")\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith('.csv'):\n",
    "        full_path = os.path.join(data_dir, filename)\n",
    "        size = os.path.getsize(full_path)\n",
    "        print(f\"  {filename}: {size:,} bytes\")\n",
    "\n",
    "print(\"\\nJSON files:\")\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith('.json'):\n",
    "        full_path = os.path.join(data_dir, filename)\n",
    "        size = os.path.getsize(full_path)\n",
    "        print(f\"  {filename}: {size:,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Walking through directories recursively\n",
    "\n",
    "import os\n",
    "\n",
    "print(\"All files in assets folder:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for root, dirs, files in os.walk('assets'):\n",
    "    # Calculate depth for indentation\n",
    "    level = root.replace('assets', '').count(os.sep)\n",
    "    indent = '  ' * level\n",
    "    \n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    \n",
    "    # Print files\n",
    "    subindent = '  ' * (level + 1)\n",
    "    for file in files:\n",
    "        print(f\"{subindent}{file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Practical use case - processing all CSV files in a directory\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import csv\n",
    "\n",
    "data_dir = 'assets/datasets'\n",
    "\n",
    "print(\"CSV File Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for filename in os.listdir(data_dir):\n",
    "    if not filename.endswith('.csv'):\n",
    "        continue\n",
    "    \n",
    "    full_path = os.path.join(data_dir, filename)\n",
    "    \n",
    "    with open(full_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)\n",
    "        row_count = sum(1 for row in reader)\n",
    "    \n",
    "    file_size = os.path.getsize(full_path)\n",
    "    \n",
    "    print(f\"\\n{filename}\")\n",
    "    print(f\"  Size: {file_size:,} bytes\")\n",
    "    print(f\"  Columns: {len(header)}\")\n",
    "    print(f\"  Rows: {row_count}\")\n",
    "    print(f\"  Headers: {', '.join(header[:5])}{'...' if len(header) > 5 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pathlib Module (Modern Alternative)\n",
    "\n",
    "Python 3.4+ introduced `pathlib`, which provides an object-oriented approach to file paths. It's often more readable than `os.path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Example: Basic pathlib usage\n",
    "\n",
    "# Create a Path object\n",
    "data_path = Path('assets') / 'datasets' / 'sales_data.csv'\n",
    "\n",
    "print(f\"Path: {data_path}\")\n",
    "print(f\"Exists: {data_path.exists()}\")\n",
    "print(f\"Is file: {data_path.is_file()}\")\n",
    "print(f\"Parent: {data_path.parent}\")\n",
    "print(f\"Name: {data_path.name}\")\n",
    "print(f\"Stem: {data_path.stem}\")\n",
    "print(f\"Suffix: {data_path.suffix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Example: Finding files with glob patterns\n",
    "\n",
    "data_dir = Path('assets/datasets')\n",
    "\n",
    "# Find all CSV files\n",
    "print(\"CSV files:\")\n",
    "for csv_file in data_dir.glob('*.csv'):\n",
    "    print(f\"  {csv_file.name}\")\n",
    "\n",
    "# Find all JSON files\n",
    "print(\"\\nJSON files:\")\n",
    "for json_file in data_dir.glob('*.json'):\n",
    "    print(f\"  {json_file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Example: Reading and writing with pathlib\n",
    "\n",
    "# Read a file\n",
    "notes_path = Path('assets/datasets/meeting_notes.txt')\n",
    "content = notes_path.read_text()\n",
    "print(f\"Read {len(content)} characters from {notes_path.name}\")\n",
    "\n",
    "# Write a file\n",
    "output_path = Path('assets/datasets/pathlib_example.txt')\n",
    "output_path.write_text(\"This file was created using pathlib!\\n\")\n",
    "print(f\"Wrote to {output_path.name}\")\n",
    "\n",
    "# Verify\n",
    "print(f\"Content: {output_path.read_text()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercise 6.1\n",
    "\n",
    "**Task:** Write a function called `summarize_directory` that:\n",
    "1. Takes a directory path as input\n",
    "2. Counts files by extension\n",
    "3. Calculates total size by extension\n",
    "4. Returns a dictionary with the summary\n",
    "\n",
    "Test it on the `assets/datasets` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 6.1\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "def summarize_directory(dir_path):\n",
    "    \"\"\"Summarize files in a directory by extension.\"\"\"\n",
    "    summary = {}\n",
    "    \n",
    "    for filename in os.listdir(dir_path):\n",
    "        full_path = os.path.join(dir_path, filename)\n",
    "        \n",
    "        # Skip directories\n",
    "        if not os.path.isfile(full_path):\n",
    "            continue\n",
    "        \n",
    "        # Get extension\n",
    "        ext = os.path.splitext(filename)[1]\n",
    "        if ext == '':\n",
    "            ext = '(no extension)'\n",
    "        \n",
    "        # Get file size\n",
    "        size = os.path.getsize(full_path)\n",
    "        \n",
    "        # Add to summary\n",
    "        if ext not in summary:\n",
    "            summary[ext] = {'count': 0, 'total_size': 0}\n",
    "        \n",
    "        summary[ext]['count'] += 1\n",
    "        summary[ext]['total_size'] += size\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Test the function\n",
    "result = summarize_directory('assets/datasets')\n",
    "\n",
    "print(\"Directory Summary: assets/datasets\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Extension':<15} {'Count':>8} {'Total Size':>15}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for ext, stats in sorted(result.items()):\n",
    "    size_str = f\"{stats['total_size']:,} bytes\"\n",
    "    print(f\"{ext:<15} {stats['count']:>8} {size_str:>15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercise 6.2\n",
    "\n",
    "**Task:** Create a data processing pipeline that:\n",
    "1. Reads all CSV files in `assets/datasets/`\n",
    "2. Creates a summary JSON file in `assets/datasets/reports/data_inventory.json`\n",
    "3. The JSON should contain info about each CSV: filename, columns, row_count, file_size\n",
    "\n",
    "Use `os.makedirs` to create the reports directory if it doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 6.2\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import csv\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Define paths\n",
    "data_dir = 'assets/datasets'\n",
    "reports_dir = os.path.join(data_dir, 'reports')\n",
    "output_file = os.path.join(reports_dir, 'data_inventory.json')\n",
    "\n",
    "# Create reports directory if needed\n",
    "if not os.path.exists(reports_dir):\n",
    "    os.makedirs(reports_dir)\n",
    "    print(f\"Created directory: {reports_dir}\")\n",
    "\n",
    "# Process all CSV files\n",
    "inventory = {\n",
    "    'generated': datetime.now().isoformat(),\n",
    "    'source_directory': data_dir,\n",
    "    'files': []\n",
    "}\n",
    "\n",
    "for filename in sorted(os.listdir(data_dir)):\n",
    "    if not filename.endswith('.csv'):\n",
    "        continue\n",
    "    \n",
    "    full_path = os.path.join(data_dir, filename)\n",
    "    \n",
    "    # Get file info\n",
    "    file_size = os.path.getsize(full_path)\n",
    "    \n",
    "    # Read CSV to get column and row info\n",
    "    with open(full_path, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        columns = next(reader)\n",
    "        row_count = sum(1 for row in reader)\n",
    "    \n",
    "    # Add to inventory\n",
    "    inventory['files'].append({\n",
    "        'filename': filename,\n",
    "        'columns': columns,\n",
    "        'column_count': len(columns),\n",
    "        'row_count': row_count,\n",
    "        'file_size_bytes': file_size\n",
    "    })\n",
    "\n",
    "# Add summary statistics\n",
    "inventory['summary'] = {\n",
    "    'total_files': len(inventory['files']),\n",
    "    'total_rows': sum(f['row_count'] for f in inventory['files']),\n",
    "    'total_size_bytes': sum(f['file_size_bytes'] for f in inventory['files'])\n",
    "}\n",
    "\n",
    "# Write to JSON\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(inventory, f, indent=2)\n",
    "\n",
    "print(f\"Created: {output_file}\")\n",
    "print(f\"\\nInventory Summary:\")\n",
    "print(f\"  Files: {inventory['summary']['total_files']}\")\n",
    "print(f\"  Total Rows: {inventory['summary']['total_rows']:,}\")\n",
    "print(f\"  Total Size: {inventory['summary']['total_size_bytes']:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Module Summary\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Text Files**: Use `open()` with modes 'r', 'w', 'a' for reading, writing, and appending\n",
    "2. **CSV Files**: The `csv` module handles comma-separated data properly, including quoted fields\n",
    "3. **JSON Files**: The `json` module converts between Python objects and JSON format\n",
    "4. **Context Managers**: Always use `with` statements for automatic and safe file handling\n",
    "5. **File Paths**: Use `os.path.join()` or `pathlib.Path` for cross-platform compatibility\n",
    "6. **Directory Operations**: Use `os.listdir()`, `os.makedirs()`, and `os.walk()` for file system navigation\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "- **Always use context managers** (`with` statement) for file operations\n",
    "- **Handle missing values** when reading data files\n",
    "- **Use os.path.join()** to build file paths instead of hardcoded slashes\n",
    "- **Check if files exist** before attempting to read them\n",
    "- **Use appropriate modes** - 'w' overwrites, 'a' appends\n",
    "\n",
    "## Common Patterns\n",
    "\n",
    "```python\n",
    "# Reading CSV\n",
    "with open('data.csv', 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        process(row)\n",
    "\n",
    "# Writing JSON\n",
    "with open('output.json', 'w') as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "\n",
    "# Processing files in directory\n",
    "for filename in os.listdir(directory):\n",
    "    full_path = os.path.join(directory, filename)\n",
    "    if filename.endswith('.csv'):\n",
    "        process_csv(full_path)\n",
    "```\n",
    "\n",
    "## Next Module\n",
    "\n",
    "In the next module, we'll cover **NumPy Fundamentals** - the foundation for numerical computing in Python. You'll learn to work with arrays, perform mathematical operations, and prepare for data analysis with pandas.\n",
    "\n",
    "## Additional Practice\n",
    "\n",
    "For extra practice, try these challenges:\n",
    "\n",
    "1. **Log Analyzer**: Read a log file, parse timestamps, and create an hourly activity report as JSON\n",
    "2. **CSV Merger**: Write a function that combines multiple CSV files with the same structure into one\n",
    "3. **Data Validator**: Create a script that reads a CSV, checks for missing values and data type issues, and writes a validation report\n",
    "4. **Backup Tool**: Write a function that copies all files of a specific type from one directory to another, adding timestamps to filenames"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
